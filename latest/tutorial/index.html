
<!DOCTYPE html>
<html class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Nonlinear Programming Models">
      
      
      
        <meta name="author" content="JuliaSmoothOptimizers">
      
      
        <link rel="shortcut icon" href="../assets/images/favicon.ico">
      
      <meta name="generator" content="mkdocs+mkdocs-material#1.0.3">
    
    
      
        <title>Tutorial - NLPModels.jl</title>
      
    
    
      <script src="../assets/javascripts/modernizr-facb31f4a3.js"></script>
    
    
      
        
        
        
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application-f3ab63f78a.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-02ce7adcc2.palette.css">
      
      
        <link rel="stylesheet" href="../assets/Documenter.css">
      
        <link rel="stylesheet" href="../assets/style.css">
      
    
    
  </head>
  
  
  
  
    <body data-md-color-primary="deep-orange" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <header class="md-header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="NLPModels.jl" class=" md-icon md-icon--home  md-header-nav__button">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <span class="md-flex__ellipsis md-header-nav__title">
          
            
              
            
            Tutorial
          
        </span>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
          
<div class="md-search" data-md-component="search">
  <div class="md-search__overlay"></div>
  <div class="md-search__inner">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" accesskey="s" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false">
      <label class="md-icon md-search__icon" for="search"></label>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result"></div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            
              


  


  <a href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      GitHub
    </div>
  </a>

            
          </div>
      </div>
    </div>
  </nav>
</header>
    
    <div class="md-container">
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    <i class=" md-icon md-icon--home  md-nav__button">
      
    </i>
    NLPModels.jl
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      GitHub
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  <li class="md-nav__item">
    
      <a href=".." title="Home" class="md-nav__link">
        Home
      </a>
    
  </li>

    
      
      
  <li class="md-nav__item">
    
      <a href="../models/" title="Models" class="md-nav__link">
        Models
      </a>
    
  </li>

    
      
      
  <li class="md-nav__item">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="toc">
        Tutorial
      </label>
    
    <a href="./" title="Tutorial" class="md-nav__link md-nav__link--active">
      Tutorial
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#adnlpmodel-tutorial" title="ADNLPModel Tutorial" class="md-nav__link">
    ADNLPModel Tutorial
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simplenlpmodel-tutorial" title="SimpleNLPModel Tutorial" class="md-nav__link">
    SimpleNLPModel Tutorial
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

    
      
      
  <li class="md-nav__item">
    
      <a href="../api/" title="API" class="md-nav__link">
        API
      </a>
    
  </li>

    
      
      
  <li class="md-nav__item">
    
      <a href="../reference/" title="Reference" class="md-nav__link">
        Reference
      </a>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#adnlpmodel-tutorial" title="ADNLPModel Tutorial" class="md-nav__link">
    ADNLPModel Tutorial
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simplenlpmodel-tutorial" title="SimpleNLPModel Tutorial" class="md-nav__link">
    SimpleNLPModel Tutorial
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
               
                 <a href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/edit/master/docs/tutorial.md" title="Edit this page" class="md-icon md-content__edit">edit</a>
               
              
                
                <p><a id='Tutorial-1'></a></p>
<h1 id="tutorial">Tutorial</h1>
<p>NLPModels.jl was created for two purposes:</p>
<ul>
<li>Allow users to access problem databases in an unified way.</li>
</ul>
<p>Mainly, this means  <a href="https://github.com/JuliaSmoothOptimizers/CUTEst.jl">CUTEst.jl</a>,  but it also gives access to <a href="https://github.com/JuliaSmoothOptimizers/AmplNLReader.jl">AMPL  problems</a>,  as well as JuMP defined problems (e.g. as in  <a href="https://github.com/JuliaSmoothOptimizers/OptimizationProblems.jl">OptimizationProblems.jl</a>).</p>
<ul>
<li>Allow users to create their own problems in the same way.</li>
</ul>
<p>As a consequence, optimization methods designed according to the NLPModels API  will accept NLPModels of any provenance.  See, for instance,  <a href="https://github.com/JuliaSmoothOptimizers/Optimize.jl">Optimize.jl</a>.</p>
<p>The main interfaces for user defined problems are</p>
<ul>
<li><a href="../models/#adnlpmodel">ADNLPModel</a>, which defines a model easily, using automatic differentiation.</li>
<li><a href="../models/#simplenlpmodel">SimpleNLPModel</a>, which allows users to handle all functions themselves, giving</li>
</ul>
<p><a id='ADNLPModel-Tutorial-1'></a></p>
<h2 id="adnlpmodel-tutorial">ADNLPModel Tutorial</h2>
<p>ADNLPModel is simple to use and is useful for classrooms. It only needs the objective function $f$ and a starting point $x^0$ to be well-defined. For constrained problems, you'll also need the constraints function $c$, and the constraints vectors $c_L$ and $c_U$, such that $c_L \leq c(x) \leq c_U$. Equality constraints will be automatically identified as those indices $i$ for which $c_{L_i} = c_{U_i}$.</p>
<p>Let's define the famous Rosenbrock function <script type="math/tex; mode=display">\begin{align*} f(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2, \end{align*}</script> with starting point $x^0 = (-1.2,1.0)$.</p>
<div class="code"><pre><span></span><span class="k">using</span> <span class="n">NLPModels</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">ADNLPModel</span><span class="p">(</span><span class="n">x</span><span class="o">-&gt;</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.2</span><span class="p">;</span> <span class="mf">1.0</span><span class="p">])</span>
</pre></div>


<div class="code"><pre><span></span>NLPModels.ADNLPModel(Minimization problem Generic
nvar = 2, ncon = 0 (0 linear)
,NLPModels.Counters(0,0,0,0,0,0,0,0,0,0,0),ex-adnlp.#1,NLPModels.#32)
</pre></div>


<p>This is enough to define the model. Let's get the objective function value at $x^0$, using only <code>nlp</code>.</p>
<div class="code"><pre><span></span><span class="n">fx</span> <span class="o">=</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;fx = </span><span class="si">$fx</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>fx = 24.199999999999996
</pre></div>


<p>Done. Let's try the gradient and Hessian.</p>
<div class="code"><pre><span></span><span class="n">gx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
<span class="n">Hx</span> <span class="o">=</span> <span class="n">hess</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;gx = </span><span class="si">$gx</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Hx = </span><span class="si">$Hx</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>gx = [-215.6,-88.0]
Hx = [1330.0 0.0; 480.0 200.0]
</pre></div>


<p>Notice how only the lower triangle of the Hessian is stored. Also notice that it is <em>dense</em>. This is a current limitation of this model. It doesn't return sparse matrices, so use it with care.</p>
<p>Let's do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method</p>
<ol>
<li>Given $x^0$, $\varepsilon &gt; 0$, and $\eta \in (0,1)$. Set $k = 0$;</li>
<li>If $\Vert \nabla f(x^k) \Vert &lt; \varepsilon$ STOP with $x^* = x^k$;</li>
<li>Compute $d^k = -\nabla f(x^k)$;</li>
<li>Compute $\alpha_k \in (0,1]$ such that</li>
</ol>
<p>$ f(x^k + \alpha_kd^k) &lt; f(x^k) + \alpha_k\eta \nabla f(x^k)^Td^k $</p>
<ol>
<li>Define $x^{k+1} = x^k + \alpha_kx^k$</li>
<li>Update $k = k + 1$ and go to step 2.</li>
</ol>
<div class="code"><pre><span></span><span class="k">function</span> <span class="n">steepest</span><span class="p">(</span><span class="n">nlp</span><span class="p">;</span> <span class="n">itmax</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.66</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span>
  <span class="n">fx</span> <span class="o">=</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="n">∇fx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="n">slope</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">∇fx</span><span class="p">,</span> <span class="n">∇fx</span><span class="p">)</span>
  <span class="n">∇f_norm</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">slope</span><span class="p">)</span>
  <span class="n">iter</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="n">∇f_norm</span> <span class="o">&gt;</span> <span class="n">eps</span> <span class="o">&amp;&amp;</span> <span class="n">iter</span> <span class="o">&lt;</span> <span class="n">itmax</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">x_trial</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">t</span> <span class="o">*</span> <span class="n">∇fx</span>
    <span class="n">f_trial</span> <span class="o">=</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x_trial</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">f_trial</span> <span class="o">&gt;</span> <span class="n">fx</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">t</span> <span class="o">*</span> <span class="n">slope</span>
      <span class="n">t</span> <span class="o">*=</span> <span class="n">sigma</span>
      <span class="n">x_trial</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">t</span> <span class="o">*</span> <span class="n">∇fx</span>
      <span class="n">f_trial</span> <span class="o">=</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x_trial</span><span class="p">)</span>
    <span class="k">end</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_trial</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">f_trial</span>
    <span class="n">∇fx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">∇fx</span><span class="p">,</span> <span class="n">∇fx</span><span class="p">)</span>
    <span class="n">∇f_norm</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">slope</span><span class="p">)</span>
    <span class="n">iter</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">end</span>
  <span class="n">optimal</span> <span class="o">=</span> <span class="n">∇f_norm</span> <span class="o">&lt;=</span> <span class="n">eps</span>
  <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="n">∇f_norm</span><span class="p">,</span> <span class="n">optimal</span><span class="p">,</span> <span class="n">iter</span>
<span class="k">end</span>

<span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="n">ngx</span><span class="p">,</span> <span class="n">optimal</span><span class="p">,</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">steepest</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;x = </span><span class="si">$x</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;fx = </span><span class="si">$fx</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;ngx = </span><span class="si">$ngx</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;optimal = </span><span class="si">$optimal</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;iter = </span><span class="si">$iter</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>x = [1.0,1.0]
fx = 4.2438440239813445e-13
ngx = 9.984661274466946e-7
optimal = true
</pre></div>


<p>Maybe this code is too complicated? If you're in a class you just want to show a Newton step.</p>
<div class="code"><pre><span></span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">H</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">hess</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">triu</span><span class="p">(</span><span class="n">hess</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span>
<span class="n">d</span> <span class="o">=</span> <span class="o">-</span><span class="n">H</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">\</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>2-element Array{Float64,1}:
 0.0247191
 0.380674
</pre></div>


<p>or a few</p>
<div class="code"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="mi">5</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">H</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">\</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">println</span><span class="p">(</span><span class="s">&quot;x = </span><span class="si">$x</span><span class="s">&quot;</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>


<div class="code"><pre><span></span>x = [-1.17528,1.38067]
x = [0.763115,-3.17503]
x = [0.76343,0.582825]
x = [0.999995,0.944027]
x = [0.999996,0.999991]
</pre></div>


<p>Also, notice how we can reuse the method.</p>
<div class="code"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">;</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">ADNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="n">ngx</span><span class="p">,</span> <span class="n">optimal</span><span class="p">,</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">steepest</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>([1.93185,0.517638],1.2842480268876117e-14,9.402834701778815e-7,true,120)
</pre></div>


<p>Even using a different model.</p>
<div class="code"><pre><span></span><span class="k">using</span> <span class="n">OptimizationProblems</span> <span class="c"># Defines a lot of JuMP models</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">MathProgNLPModel</span><span class="p">(</span><span class="n">woods</span><span class="p">())</span>
<span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="n">ngx</span><span class="p">,</span> <span class="n">optimal</span><span class="p">,</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">steepest</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;fx = </span><span class="si">$fx</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;ngx = </span><span class="si">$ngx</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;optimal = </span><span class="si">$optimal</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;iter = </span><span class="si">$iter</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>WARNING: The curly syntax (sum{},prod{},norm2{}) is deprecated in favor of the new generator syntax (sum(),prod(),norm()).
WARNING: Replace sum{((x[i] ^ 2 + x[n] ^ 2) ^ 2 - 4 * x[i]) + 3,i = 1:n - 1} with sum(((x[i] ^ 2 + x[n] ^ 2) ^ 2 - 4 * x[i]) + 3 for i = 1:n - 1).
WARNING: Replace sum{(3 - 4 * x[i]) ^ 2 + (x[i] ^ 2 + 2 * x[i + 1] ^ 2 + 3 * x[i + 2] ^ 2 + 4 * x[i + 3] ^ 2 + 5 * x[n] ^ 2) ^ 2,i = 1:n - 4} with sum((3 - 4 * x[i]) ^ 2 + (x[i] ^ 2 + 2 * x[i + 1] ^ 2 + 3 * x[i + 2] ^ 2 + 4 * x[i + 3] ^ 2 + 5 * x[n] ^ 2) ^ 2 for i = 1:n - 4).
WARNING: Replace sum{abs(((1 - x[i - 1]) - 2 * x[i + 1]) + (3 - x[i] / 2) * x[i]) ^ p,i = 2:n - 1} with sum(abs(((1 - x[i - 1]) - 2 * x[i + 1]) + (3 - x[i] / 2) * x[i]) ^ p for i = 2:n - 1).
WARNING: Replace sum{abs(x[i] + x[i + n2]) ^ p,i = 1:n2} with sum(abs(x[i] + x[i + n2]) ^ p for i = 1:n2).
WARNING: Replace sum{((x[i] * (2 + 5 * x[i] ^ 2) + 1) - sum{x[j] * (1 + x[j]),j = max(1,i - ml):min(n,i + mu); j != i}) ^ 2,i = 1:n} with sum(((x[i] * (2 + 5 * x[i] ^ 2) + 1) - sum{x[j] * (1 + x[j]),j = max(1,i - ml):min(n,i + mu); j != i}) ^ 2 for i = 1:n).
WARNING: Replace sum{x[j] * (1 + x[j]),j = max(1,i - ml):min(n,i + mu); j != i} with sum(x[j] * (1 + x[j]) for j = max(1,i - ml):min(n,i + mu) if j != i).
WARNING: Replace sum{100 * (x[2i] - x[2i - 1] ^ 2) ^ 2 + (1 - x[2i - 1]) ^ 2 + 90 * (x[2i + 2] - x[2i + 1] ^ 2) ^ 2 + (1 - x[2i + 1]) ^ 2 + 10 * ((x[2i] + x[2i + 2]) - 2) ^ 2 + 0.1 * (x[2i] - x[2i + 2]) ^ 2,i = 1:div(n,2) - 1} with sum(100 * (x[2i] - x[2i - 1] ^ 2) ^ 2 + (1 - x[2i - 1]) ^ 2 + 90 * (x[2i + 2] - x[2i + 1] ^ 2) ^ 2 + (1 - x[2i + 1]) ^ 2 + 10 * ((x[2i] + x[2i + 2]) - 2) ^ 2 + 0.1 * (x[2i] - x[2i + 2]) ^ 2 for i = 1:div(n,2) - 1).
WARNING: Replace sum{cos(x[i] ^ 2 - 0.5 * x[i + 1]),i = 1:n - 1} with sum(cos(x[i] ^ 2 - 0.5 * x[i + 1]) for i = 1:n - 1).
WARNING: Replace sum{(exp(x[2i - 1]) - x[2i]) ^ 4 + 100 * (x[2i] - x[2i + 1]) ^ 6 + ((tan(x[2i + 1] - x[2i + 2]) + x[2i + 1]) - x[2i + 2]) ^ 4 + x[2i - 1] ^ 8 + (x[2i + 2] - 1) ^ 2,i = 1:div(n,2) - 1} with sum((exp(x[2i - 1]) - x[2i]) ^ 4 + 100 * (x[2i] - x[2i + 1]) ^ 6 + ((tan(x[2i + 1] - x[2i + 2]) + x[2i + 1]) - x[2i + 2]) ^ 4 + x[2i - 1] ^ 8 + (x[2i + 2] - 1) ^ 2 for i = 1:div(n,2) - 1).
WARNING: Replace sum{x[j],j = i:min(i + b,n)} with sum(x[j] for j = i:min(i + b,n)).
WARNING: Replace sum{f[i] * (f[i] * (f[i] ^ 2 - 20) - 0.1),i = 1:n} with sum(f[i] * (f[i] * (f[i] ^ 2 - 20) - 0.1) for i = 1:n).
WARNING: Replace sum{(i / n) * α * x[i] ^ 2,i = 1:n} with sum((i / n) * α * x[i] ^ 2 for i = 1:n).
WARNING: Replace sum{β * x[i] ^ 2 * (x[i + 1] + x[i + 1] ^ 2) ^ 2,i = 1:n - 1} with sum(β * x[i] ^ 2 * (x[i + 1] + x[i + 1] ^ 2) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{γ * x[i] ^ 2 * x[i + m] ^ 4,i = 1:2m} with sum(γ * x[i] ^ 2 * x[i + m] ^ 4 for i = 1:2m).
WARNING: Replace sum{(i / n) * δ * x[i] * x[i + 2m],i = 1:m} with sum((i / n) * δ * x[i] * x[i + 2m] for i = 1:m).
WARNING: Replace sum{(i / n) ^ 2 * α * x[i] ^ 2,i = 1:n} with sum((i / n) ^ 2 * α * x[i] ^ 2 for i = 1:n).
WARNING: Replace sum{(i / n) ^ 2 * δ * x[i] * x[i + 2m],i = 1:m} with sum((i / n) ^ 2 * δ * x[i] * x[i + 2m] for i = 1:m).
WARNING: Replace sum{(i / n) * β * x[i] ^ 2 * (x[i + 1] + x[i + 1] ^ 2) ^ 2,i = 1:n - 1} with sum((i / n) * β * x[i] ^ 2 * (x[i + 1] + x[i + 1] ^ 2) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{(i / n) * γ * x[i] ^ 2 * x[i + m] ^ 4,i = 1:2m} with sum((i / n) * γ * x[i] ^ 2 * x[i + m] ^ 4 for i = 1:2m).
WARNING: Replace sum{(x[i] - i) ^ 4,i = 1:n} with sum((x[i] - i) ^ 4 for i = 1:n).
WARNING: Replace sum{(x[i] - 2) ^ 4 + (x[i] * x[i + 1] - 2 * x[i + 1]) ^ 2 + (x[i + 1] + 1) ^ 2,i = 1:n - 1} with sum((x[i] - 2) ^ 4 + (x[i] * x[i + 1] - 2 * x[i + 1]) ^ 2 + (x[i + 1] + 1) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{sin((x[1] + x[i] ^ 2) - 1),i = 1:n - 1} with sum(sin((x[1] + x[i] ^ 2) - 1) for i = 1:n - 1).
WARNING: Replace sum{((x[i] ^ 2 + x[i + 1] ^ 2) ^ 2 - 4 * x[i]) + 3,i = 1:n - 1} with sum(((x[i] ^ 2 + x[i + 1] ^ 2) ^ 2 - 4 * x[i]) + 3 for i = 1:n - 1).
WARNING: Replace sum{(x[i + 1] - x[i] ^ 2) ^ 2,i = 1:n - 1} with sum((x[i + 1] - x[i] ^ 2) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{(x[i] - 1.0) ^ 2,i = 1:n - 1} with sum((x[i] - 1.0) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{(x[i] - x[i - 1] ^ 2) ^ 2,i = 2:n} with sum((x[i] - x[i - 1] ^ 2) ^ 2 for i = 2:n).
WARNING: Replace sum{(1.0 - x[i]) ^ 2,i = 2:n} with sum((1.0 - x[i]) ^ 2 for i = 2:n).
WARNING: Replace sum{(x[i - 1] - x[i] ^ 2) ^ 2 * (1.5 + sin(i)) ^ 2,i = 2:n} with sum((x[i - 1] - x[i] ^ 2) ^ 2 * (1.5 + sin(i)) ^ 2 for i = 2:n).
WARNING: Replace sum{(x[i - 1] - 16.0 * x[i] ^ 2 * (1.5 + sin(i)) ^ 2) ^ 2,i = 2:n} with sum((x[i - 1] - 16.0 * x[i] ^ 2 * (1.5 + sin(i)) ^ 2) ^ 2 for i = 2:n).
WARNING: Replace sum{(x[i] - x[i + 1]) ^ 2,i = 1:n - 1} with sum((x[i] - x[i + 1]) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{100.0 * (1 + 2.0 / h ^ 2) * sin(x[i] / 100.0) + (1 / h ^ 2) * cos(x[i]),i = 1:n} with sum(100.0 * (1 + 2.0 / h ^ 2) * sin(x[i] / 100.0) + (1 / h ^ 2) * cos(x[i]) for i = 1:n).
WARNING: Replace sum{2 * x[i] + cos(x[i]),i = 1:n} with sum(2 * x[i] + cos(x[i]) for i = 1:n).
WARNING: Replace sum{(((x[i + 1] - x[i]) + 1) - x[i] ^ 2) ^ 2,i = 1:n - 1} with sum((((x[i + 1] - x[i]) + 1) - x[i] ^ 2) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{sum{(100.0 * sqrt(0.5 * (p - 1) ^ 2 * ((x[i,j] - x[i + 1,j + 1]) ^ 2 + (x[i + 1,j] - x[i,j + 1]) ^ 2) + 1.0)) / scale,i = 1:p - 1},j = 1:p - 1} with sum(sum{(100.0 * sqrt(0.5 * (p - 1) ^ 2 * ((x[i,j] - x[i + 1,j + 1]) ^ 2 + (x[i + 1,j] - x[i,j + 1]) ^ 2) + 1.0)) / scale,i = 1:p - 1} for j = 1:p - 1).
WARNING: Replace sum{(100.0 * sqrt(0.5 * (p - 1) ^ 2 * ((x[i,j] - x[i + 1,j + 1]) ^ 2 + (x[i + 1,j] - x[i,j + 1]) ^ 2) + 1.0)) / scale,i = 1:p - 1} with sum((100.0 * sqrt(0.5 * (p - 1) ^ 2 * ((x[i,j] - x[i + 1,j + 1]) ^ 2 + (x[i + 1,j] - x[i,j + 1]) ^ 2) + 1.0)) / scale for i = 1:p - 1).
WARNING: Replace sum{((((5.0 - x[i + 1]) * x[i + 1] ^ 2 + x[i]) - 2 * x[i + 1]) - 13.0) ^ 2,i = 1:ngs} with sum(((((5.0 - x[i + 1]) * x[i + 1] ^ 2 + x[i]) - 2 * x[i + 1]) - 13.0) ^ 2 for i = 1:ngs).
WARNING: Replace sum{((((1.0 + x[i + 1]) * x[i + 1] ^ 2 + x[i]) - 14 * x[i + 1]) - 29.0) ^ 2,i = 1:ngs} with sum(((((1.0 + x[i + 1]) * x[i + 1] ^ 2 + x[i]) - 14 * x[i + 1]) - 29.0) ^ 2 for i = 1:ngs).
WARNING: Replace sum{sin(ζ * x[i]) ^ 2 * sin(ζ * x[i + 1]) ^ 2 + 0.05 * (x[i] ^ 2 + x[i + 1] ^ 2),i = 1:n - 1} with sum(sin(ζ * x[i]) ^ 2 * sin(ζ * x[i + 1]) ^ 2 + 0.05 * (x[i] ^ 2 + x[i + 1] ^ 2) for i = 1:n - 1).
WARNING: Replace sum{sin(x[i] / 100.0),i = 1:n} with sum(sin(x[i] / 100.0) for i = 1:n).
WARNING: Replace sum{cos((2.0 * x[i] - x[n]) - x[1]),i = 2:n - 1} with sum(cos((2.0 * x[i] - x[n]) - x[1]) for i = 2:n - 1).
WARNING: Replace sum{4.0 * (x[i] ^ 2 - x[1]) ^ 2 + (x[i] - 1) ^ 2,i = 1:n} with sum(4.0 * (x[i] ^ 2 - x[1]) ^ 2 + (x[i] - 1) ^ 2 for i = 1:n).
WARNING: Replace sum{(((2.0 * x[i] - x[i - 1]) - x[i + 1]) + (h ^ 2 / 2.0) * (x[i] + (i - 1) * h + 1) ^ 3) ^ 2,i = 2:n - 1} with sum((((2.0 * x[i] - x[i - 1]) - x[i + 1]) + (h ^ 2 / 2.0) * (x[i] + (i - 1) * h + 1) ^ 3) ^ 2 for i = 2:n - 1).
WARNING: Replace sum{(10.0 / i) * sum{x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2),j = 1:20} ^ 2 - 0.2 * sum{x[(i + j) - 1],j = 1:20},i = 1:n - 30} with sum((10.0 / i) * sum{x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2),j = 1:20} ^ 2 - 0.2 * sum{x[(i + j) - 1],j = 1:20} for i = 1:n - 30).
WARNING: Replace sum{x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2),j = 1:20} with sum(x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2) for j = 1:20).
WARNING: Replace sum{x[(i + j) - 1],j = 1:20} with sum(x[(i + j) - 1] for j = 1:20).
WARNING: Replace sum{x[i] ^ 4 + 2,i = 1:n - 10} with sum(x[i] ^ 4 + 2 for i = 1:n - 10).
WARNING: Replace sum{x[i] * x[i + 10] * x[(i + n) - 10] + 2.0 * x[(i + n) - 10] ^ 2,i = 1:10} with sum(x[i] * x[i + 10] * x[(i + n) - 10] + 2.0 * x[(i + n) - 10] ^ 2 for i = 1:10).
WARNING: Replace sum{(10.0 / i) * sum{x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2),j = 1:20} ^ 2 - 0.2 * sum{x[(i + j) - 1],j = 1:20},i = 1:n - 19} with sum((10.0 / i) * sum{x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2),j = 1:20} ^ 2 - 0.2 * sum{x[(i + j) - 1],j = 1:20} for i = 1:n - 19).
WARNING: Replace sum{100.0 * x[i] ^ 4 + 2.0,i = 1:n} with sum(100.0 * x[i] ^ 4 + 2.0 for i = 1:n).
WARNING: Replace sum{(x[i] + x[mod(3i - 2,n) + 1] + x[mod(7i - 3,n) + 1]) ^ 2 + 4.0 * cos(x[i] + x[mod(3i - 2,n) + 1] + x[mod(7i - 3,n) + 1]),i = 1:n} with sum((x[i] + x[mod(3i - 2,n) + 1] + x[mod(7i - 3,n) + 1]) ^ 2 + 4.0 * cos(x[i] + x[mod(3i - 2,n) + 1] + x[mod(7i - 3,n) + 1]) for i = 1:n).
WARNING: Replace sum{(x[i] + x[mod(2i - 1,n) + 1] + x[mod(3i - 1,n) + 1]) ^ 2 + 4.0 * cos(x[i] + x[mod(2i - 1,n) + 1] + x[mod(3i - 1,n) + 1]),i = 1:n} with sum((x[i] + x[mod(2i - 1,n) + 1] + x[mod(3i - 1,n) + 1]) ^ 2 + 4.0 * cos(x[i] + x[mod(2i - 1,n) + 1] + x[mod(3i - 1,n) + 1]) for i = 1:n).
WARNING: Replace sum{(100.0 * x[1] - x[i - 1] ^ 2) ^ 2,i = 2:n} with sum((100.0 * x[1] - x[i - 1] ^ 2) ^ 2 for i = 2:n).
WARNING: Replace sum{(x[i] + x[i + 1] + x[n]) ^ 4,i = 1:n - 2} with sum((x[i] + x[i + 1] + x[n]) ^ 4 for i = 1:n - 2).
WARNING: Replace sum{a * ((exp(x[i] / 10.0) + exp(x[i - 1] / 10.0)) - y[i]) ^ 2,i = 2:n} with sum(a * ((exp(x[i] / 10.0) + exp(x[i - 1] / 10.0)) - y[i]) ^ 2 for i = 2:n).
WARNING: Replace sum{a * (exp(x[(i - n) + 1] / 10.0) - exp(-1 / 10)) ^ 2,i = n + 1:2n - 1} with sum(a * (exp(x[(i - n) + 1] / 10.0) - exp(-1 / 10)) ^ 2 for i = n + 1:2n - 1).
WARNING: Replace sum{((n - j) + 1) * x[j] ^ 2,j = 1:n} with sum(((n - j) + 1) * x[j] ^ 2 for j = 1:n).
WARNING: Replace sum{(x[i] - 1.0) ^ 2,i = 1:div(n,2)} with sum((x[i] - 1.0) ^ 2 for i = 1:div(n,2)).
WARNING: Replace sum{((x[i] + 2.0 * x[i + 1] + 10.0 * x[i + 2]) - 1.0) ^ 2,i = 1:n - 2} with sum(((x[i] + 2.0 * x[i + 1] + 10.0 * x[i + 2]) - 1.0) ^ 2 for i = 1:n - 2).
WARNING: Replace sum{((2.0 * x[i] + x[i + 1]) - 3.0) ^ 2,i = 1:n - 2} with sum(((2.0 * x[i] + x[i + 1]) - 3.0) ^ 2 for i = 1:n - 2).
WARNING: Replace sum{x[i] ^ 2 - n,i = 1:n} with sum(x[i] ^ 2 - n for i = 1:n).
WARNING: Replace sum{(x[j] + 10.0 * x[j + 1]) ^ 2 + 5.0 * (x[j + 2] - x[j + 3]) ^ 2 + (x[j + 1] - 2.0 * x[j + 2]) ^ 4 + 10.0 * (x[j] - x[j + 3]) ^ 4,j = 1:div(n,4)} with sum((x[j] + 10.0 * x[j + 1]) ^ 2 + 5.0 * (x[j + 2] - x[j + 3]) ^ 2 + (x[j + 1] - 2.0 * x[j + 2]) ^ 4 + 10.0 * (x[j] - x[j + 3]) ^ 4 for j = 1:div(n,4)).
WARNING: Replace sum{(((2.0 + 5.0 * p[i] ^ 2 * x[i] ^ 2) * p[i] * x[i] + 1.0) - sum{p[j] * x[j] * (1.0 + p[j] * x[j]),j = J[i]}) ^ 2,i = 1:n} with sum((((2.0 + 5.0 * p[i] ^ 2 * x[i] ^ 2) * p[i] * x[i] + 1.0) - sum{p[j] * x[j] * (1.0 + p[j] * x[j]),j = J[i]}) ^ 2 for i = 1:n).
WARNING: Replace sum{p[j] * x[j] * (1.0 + p[j] * x[j]),j = J[i]} with sum(p[j] * x[j] * (1.0 + p[j] * x[j]) for j = J[i]).
WARNING: Replace sum{cos(p[i] ^ 2 * x[i] ^ 2 - (p[i + 1] * x[i + 1]) / 2.0),i = 1:n - 1} with sum(cos(p[i] ^ 2 * x[i] ^ 2 - (p[i + 1] * x[i + 1]) / 2.0) for i = 1:n - 1).
WARNING: Replace sum{(-(1.0 / (1.0 + (x[i] - x[i + 1]) ^ 2)) - sin((pi * x[i + 1] + x[i + 2]) / 2.0)) - exp(-(((x[i] + x[i + 2]) / x[i + 1] - 2.0) ^ 2)),i = 1:n - 2} with sum((-(1.0 / (1.0 + (x[i] - x[i + 1]) ^ 2)) - sin((pi * x[i + 1] + x[i + 2]) / 2.0)) - exp(-(((x[i] + x[i + 2]) / x[i + 1] - 2.0) ^ 2)) for i = 1:n - 2).
WARNING: Replace sum{((sin(x[i] - x[n]) - x[1] ^ 2) + x[i] ^ 2) ^ 2,i = 2:n - 1} with sum(((sin(x[i] - x[n]) - x[1] ^ 2) + x[i] ^ 2) ^ 2 for i = 2:n - 1).
WARNING: Replace sum{i * (sin(x[i]) + sin(x[mod(2i - 1,n) + 1]) + sin(x[mod(3i - 1,n) + 1]) + sin(x[mod(5i - 1,n) + 1]) + sin(x[mod(7i - 1,n) + 1]) + sin(x[mod(11i - 1,n) + 1])) ^ 2,i = 1:n} with sum(i * (sin(x[i]) + sin(x[mod(2i - 1,n) + 1]) + sin(x[mod(3i - 1,n) + 1]) + sin(x[mod(5i - 1,n) + 1]) + sin(x[mod(7i - 1,n) + 1]) + sin(x[mod(11i - 1,n) + 1])) ^ 2 for i = 1:n).
WARNING: Replace sum{i * (x[i] ^ 2 + x[mod(2i - 1,n) + 1] ^ 2 + x[mod(3i - 1,n) + 1] ^ 2 + x[mod(5i - 1,n) + 1] ^ 2 + x[mod(7i - 1,n) + 1] ^ 2 + x[mod(11i - 1,n) + 1] ^ 2) ^ 2,i = 1:n} with sum(i * (x[i] ^ 2 + x[mod(2i - 1,n) + 1] ^ 2 + x[mod(3i - 1,n) + 1] ^ 2 + x[mod(5i - 1,n) + 1] ^ 2 + x[mod(7i - 1,n) + 1] ^ 2 + x[mod(11i - 1,n) + 1] ^ 2) ^ 2 for i = 1:n).
WARNING: Replace sum{100.0 * (x[2i] - x[2i - 1] ^ 2) ^ 2 + (x[2i - 1] - 1.0) ^ 2,i = 1:div(n,2)} with sum(100.0 * (x[2i] - x[2i - 1] ^ 2) ^ 2 + (x[2i - 1] - 1.0) ^ 2 for i = 1:div(n,2)).
WARNING: Replace sum{(10.0 / (n + 2) + x[i + 2] ^ 2) * (2.0 - exp(-((x[i] - x[i + 1]) ^ 2) / (0.1 + x[i + 2] ^ 2))),i = 1:n - 2} with sum((10.0 / (n + 2) + x[i + 2] ^ 2) * (2.0 - exp(-((x[i] - x[i + 1]) ^ 2) / (0.1 + x[i + 2] ^ 2))) for i = 1:n - 2).
WARNING: Replace sum{(x[1] ^ 2 - x[i + 1] ^ 2) ^ 2,i = 1:n - 2} with sum((x[1] ^ 2 - x[i + 1] ^ 2) ^ 2 for i = 1:n - 2).
WARNING: Replace sum{100 * (x[4i - 2] - x[4i - 3] ^ 2) ^ 2 + (1 - x[4i - 3]) ^ 2 + 90 * (x[4i] - x[4i - 1] ^ 2) ^ 2 + (1 - x[4i - 1]) ^ 2 + 10 * ((x[4i - 2] + x[4i]) - 2) ^ 2 + 0.1 * (x[4i - 2] - x[4i]) ^ 2,i = 1:div(n,4)} with sum(100 * (x[4i - 2] - x[4i - 3] ^ 2) ^ 2 + (1 - x[4i - 3]) ^ 2 + 90 * (x[4i] - x[4i - 1] ^ 2) ^ 2 + (1 - x[4i - 1]) ^ 2 + 10 * ((x[4i - 2] + x[4i]) - 2) ^ 2 + 0.1 * (x[4i - 2] - x[4i]) ^ 2 for i = 1:div(n,4)).
WARNING: Replace sum{exp(x[j]) * ((c[j] + x[j]) - log(sum{exp(x[k]),k = 1:10})),j = 1:10} with sum(exp(x[j]) * ((c[j] + x[j]) - log(sum{exp(x[k]),k = 1:10})) for j = 1:10).
WARNING: Replace sum{exp(x[k]),k = 1:10} with sum(exp(x[k]) for k = 1:10).
WARNING: Replace sum{x[j] * (c[j] + log(x[j] / sum{x[k],k = 1:10})),j = 1:10} with sum(x[j] * (c[j] + log(x[j] / sum{x[k],k = 1:10})) for j = 1:10).
WARNING: Replace sum{x[k],k = 1:10} with sum(x[k] for k = 1:10).
fx = 1.0000000000002167
ngx = 9.893253859340887e-7
optimal = true
iter = 12016
</pre></div>


<p>For constrained minimization, you need the constraints vector and bounds too. Bounds on the variables can be passed through a new vector.</p>
<div class="code"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.2</span><span class="p">;</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">lvar</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="nb">Inf</span><span class="p">;</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="n">uvar</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">;</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">c</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">;</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">]</span>
<span class="n">lcon</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">;</span> <span class="o">-</span><span class="nb">Inf</span><span class="p">]</span>
<span class="n">ucon</span> <span class="o">=</span> <span class="p">[</span><span class="nb">Inf</span><span class="p">;</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">ADNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">lvar</span><span class="o">=</span><span class="n">lvar</span><span class="p">,</span> <span class="n">uvar</span><span class="o">=</span><span class="n">uvar</span><span class="p">,</span> <span class="n">lcon</span><span class="o">=</span><span class="n">lcon</span><span class="p">,</span> <span class="n">ucon</span><span class="o">=</span><span class="n">ucon</span><span class="p">)</span>

<span class="n">println</span><span class="p">(</span><span class="s">&quot;cx = </span><span class="si">$</span><span class="p">(</span><span class="n">cons</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">))</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Jx = </span><span class="si">$</span><span class="p">(</span><span class="n">jac</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">))</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>cx = [-2.2,2.44]
Jx = [1.0 1.0; -2.4 2.0]
</pre></div>


<p><a id='SimpleNLPModel-Tutorial-1'></a></p>
<h2 id="simplenlpmodel-tutorial">SimpleNLPModel Tutorial</h2>
<p>SimpleNLPModel allows you to pass every single function of the model. On the other hand, it doesn't handle anything else. Calling an undefined function will throw a <code>NotImplementedError</code>. Only the objective function is mandatory (if you don't need it, pass <code>x-&gt;0</code>).</p>
<div class="code"><pre><span></span><span class="k">using</span> <span class="n">NLPModels</span>

<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">SimpleNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>

<span class="n">fx</span> <span class="o">=</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;fx = </span><span class="si">$fx</span><span class="s">&quot;</span><span class="p">)</span>

<span class="c"># grad(nlp, nlp.meta.x0) # This is undefined</span>
</pre></div>


<div class="code"><pre><span></span>fx = 5.0
</pre></div>


<div class="code"><pre><span></span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">);</span> <span class="mi">8</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)]</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">SimpleNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>

<span class="n">grad</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>2-element Array{Float64,1}:
 -2.0
 -8.0
</pre></div>


<p>"But what's to stop me from defining <code>g</code> however I want?" Nothing. So you have to be careful on how you're defining it. You should probably check your derivatives. If the function is simply defined, you can try using automatic differentiation. Alternatively, you can use the <a href="../dercheck">derivative checker</a>.</p>
<div class="code"><pre><span></span><span class="n">gradient_check</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>Dict{Int64,Float64} with 0 entries
</pre></div>


<div class="code"><pre><span></span><span class="n">gwrong</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">);</span> <span class="mi">8</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">]</span> <span class="c"># Find the error</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">SimpleNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">gwrong</span><span class="p">)</span>
<span class="n">gradient_check</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>Dict{Int64,Float64} with 1 entry:
  2 =&gt; 7.0
</pre></div>


<p>For constrained problems, we still need the constraints function, <code>lcon</code> and <code>ucon</code>. Also, let's pass the Jacobian-vector product.</p>
<div class="code"><pre><span></span><span class="n">c</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">;</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">lcon</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">;</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="n">ucon</span> <span class="o">=</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">;</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="n">Jacprod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">SimpleNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">lcon</span><span class="o">=</span><span class="n">lcon</span><span class="p">,</span> <span class="n">ucon</span><span class="o">=</span><span class="n">ucon</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">Jp</span><span class="o">=</span><span class="n">Jacprod</span><span class="p">)</span>
<span class="n">jprod</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>


<div class="code"><pre><span></span>2-element Array{Float64,1}:
 4.0
 2.0
</pre></div>


<p>Furthermore, NLPModels also works with inplace operations. Since some models do not take full advantage of this (like ADNLPModel), a user might want to define his/her own functions that do.</p>
<div class="code"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">g!</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gx</span><span class="p">)</span> <span class="o">=</span> <span class="k">begin</span>
  <span class="n">gx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span>
  <span class="n">gx</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gx</span>
<span class="k">end</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">SimpleNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">g!</span> <span class="o">=</span><span class="n">g!</span><span class="p">)</span> <span class="c"># Watchout, g!=g! is interpreted as g != g!</span>
<span class="n">gx</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">grad!</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">,</span> <span class="n">gx</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>2-element Array{Float64,1}:
 -2.0
  8.0
</pre></div>
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../models/" title="Models" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Models
              </span>
            </div>
          </a>
        
        
          <a href="../api/" title="API" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                API
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="http://www.mkdocs.org" title="MkDocs">MkDocs</a>
        and
        <a href="http://squidfunk.github.io/mkdocs-material/" title="Material for MkDocs">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application-2afe21e0b2.js"></script>
      <script>var config={url:{base:".."}},app=new Application(config);app.initialize()</script>
      
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
      
        <script src="../assets/mathjaxhelper.js"></script>
      
    
    
      
    
  </body>
</html>