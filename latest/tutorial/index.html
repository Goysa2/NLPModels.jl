<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="no-js ie6"><![endif]-->
<!--[if IE 7 ]><html class="no-js ie7"><![endif]-->
<!--[if IE 8 ]><html class="no-js ie8"><![endif]-->
<!--[if IE 9 ]><html class="no-js ie9"><![endif]-->
<!--[if (gt IE 9)|!(IE)]><!--> <html class="no-js"> <!--<![endif]-->
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    
      
        <title>Tutorial - NLPModels.jl</title>
      
      
      
      
        <meta name="author" content="JuliaSmoothOptimizers">
      
    
    <meta property="og:url" content="None">
    <meta property="og:title" content="NLPModels.jl">
    <meta property="og:image" content="None/../">
    <meta name="apple-mobile-web-app-title" content="NLPModels.jl">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    
    
    <link rel="shortcut icon" type="image/x-icon" href="../assets/images/favicon-e565ddfa3b.ico">
    <link rel="icon" type="image/x-icon" href="../assets/images/favicon-e565ddfa3b.ico">
    <style>
      @font-face {
      	font-family: 'Icon';
      	src: url('../assets/fonts/icon.eot?52m981');
      	src: url('../assets/fonts/icon.eot?#iefix52m981')
               format('embedded-opentype'),
      		   url('../assets/fonts/icon.woff?52m981')
               format('woff'),
      		   url('../assets/fonts/icon.ttf?52m981')
               format('truetype'),
      		   url('../assets/fonts/icon.svg?52m981#icon')
               format('svg');
      	font-weight: normal;
      	font-style: normal;
      }
    </style>
    <link rel="stylesheet" href="../assets/stylesheets/application-a422ff04cc.css">
    
      <link rel="stylesheet" href="../assets/stylesheets/palettes-05ab2406df.css">
    
    
      
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu+Mono">
      <style>
        body, input {
          font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
        }
        pre, code {
          font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
        }
      </style>
    
    
      <link rel="stylesheet" href="../assets/Documenter.css">
    
      <link rel="stylesheet" href="../assets/style.css">
    
    <script src="../assets/javascripts/modernizr-4ab42b99fd.js"></script>
    
  </head>
  
  
  
  <body class="palette-primary-deep-orange palette-accent-indigo">
    
      
      
    
    <div class="backdrop">
      <div class="backdrop-paper"></div>
    </div>
    <input class="toggle" type="checkbox" id="toggle-drawer">
    <input class="toggle" type="checkbox" id="toggle-search">
    <label class="toggle-button overlay" for="toggle-drawer"></label>
    <header class="header">
      <nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        
          <span class="path">
            
          </span>
        
        Tutorial
      </div>
    </div>
    
    
      
      <div class="button button-github" role="button" aria-label="GitHub">
        <a href="https://github.com/JuliaSmoothOptimizers" title="@JuliaSmoothOptimizers on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
      </div>
    
    <div class="button button-search" role="button" aria-label="Search">
      <label class="toggle-button icon icon-search" title="Search" for="toggle-search"></label>
    </div>
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
    </header>
    <main class="main">
      
      <div class="drawer">
        <nav aria-label="Navigation">
  
  <a href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl" class="project">
    <div class="banner">
      
      <div class="name">
        <strong>
          NLPModels.jl
          <span class="version">
            
          </span>
        </strong>
        
          <br>
          JuliaSmoothOptimizers/NLPModels.jl
        
      </div>
    </div>
  </a>
  <div class="scrollable">
    <div class="wrapper">
      
        <ul class="repo">
          <li class="repo-download">
            
            <a href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/archive/master.zip" target="_blank" title="Download" data-action="download">
              <i class="icon icon-download"></i> Download
            </a>
          </li>
          <li class="repo-stars">
            <a href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/stargazers" target="_blank" title="Stargazers" data-action="star">
              <i class="icon icon-star"></i> Stars
              <span class="count">&ndash;</span>
            </a>
          </li>
        </ul>
        <hr>
      
      <div class="toc">
        <ul>
          
            
  <li>
    <a class="" title="Home" href="..">
      Home
    </a>
    
  </li>

          
            
  <li>
    <a class="" title="Models" href="../models/">
      Models
    </a>
    
  </li>

          
            
  <li>
    <a class="current" title="Tutorial" href="./">
      Tutorial
    </a>
    
      
        
      
      
        <ul>
          
            <li class="anchor">
              <a title="ADNLPModel Tutorial" href="#adnlpmodel-tutorial">
                ADNLPModel Tutorial
              </a>
            </li>
          
            <li class="anchor">
              <a title="SimpleNLPModel Tutorial" href="#simplenlpmodel-tutorial">
                SimpleNLPModel Tutorial
              </a>
            </li>
          
        </ul>
      
    
  </li>

          
            
  <li>
    <a class="" title="API" href="../api/">
      API
    </a>
    
  </li>

          
            
  <li>
    <a class="" title="Reference" href="../reference/">
      Reference
    </a>
    
  </li>

          
        </ul>
        
          <hr>
          <span class="section">The author</span>
          <ul>
            
            
              
              <li>
                <a href="https://github.com/JuliaSmoothOptimizers" target="_blank" title="@JuliaSmoothOptimizers on GitHub">
                  @JuliaSmoothOptimizers on GitHub
                </a>
              </li>
            
          </ul>
        
      </div>
    </div>
  </div>
</nav>
      </div>
      <article class="article">
        <div class="wrapper">
          
          <p><a id='Tutorial-1'></a></p>
<h1 id="tutorial">Tutorial</h1>
<p>NLPModels.jl was created for two purposes:</p>
<ul>
<li>Allow users to access problem databases in an unified way.  Mainly, this means  <a href="https://github.com/JuliaSmoothOptimizers/CUTEst.jl">CUTEst.jl</a>,  but it also gives access to <a href="https://github.com/JuliaSmoothOptimizers/AmplNLReader.jl">AMPL  problems</a>,  as well as JuMP defined problems (e.g. as in  <a href="https://github.com/JuliaSmoothOptimizers/OptimizationProblems.jl">OptimizationProblems.jl</a>).</li>
<li>Allow users to create their own problems in the same way.  As a consequence, optimization methods designed according to the NLPModels API  will accept NLPModels of any provenance.  See, for instance,  <a href="https://github.com/JuliaSmoothOptimizers/Optimize.jl">Optimize.jl</a>.</li>
</ul>
<p>The main interfaces for user defined problems are</p>
<ul>
<li><a href="../models/#adnlpmodel">ADNLPModel</a>, which defines a model easily, using automatic   differentiation.</li>
<li><a href="../models/#simplenlpmodel">SimpleNLPModel</a>, which allows users to handle all functions themselves,   giving</li>
</ul>
<p><a id='ADNLPModel-Tutorial-1'></a></p>
<h2 id="adnlpmodel-tutorial">ADNLPModel Tutorial</h2>
<p>ADNLPModel is simple to use and is useful for classrooms. It only needs the objective function $f$ and a starting point $x^0$ to be well-defined. For constrained problems, you'll also need the constraints function $c$, and the constraints vectors $c_L$ and $c_U$, such that $c_L \leq c(x) \leq c_U$. Equality constraints will be automatically identified as those indices $i$ for which $c_{L_i} = c_{U_i}$.</p>
<p>Let's define the famous Rosenbrock function <script type="math/tex; mode=display">\begin{align*} f(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2, \end{align*}</script> with starting point $x^0 = (-1.2,1.0)$.</p>
<div class="code"><pre><span></span><span class="k">using</span> <span class="n">NLPModels</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">ADNLPModel</span><span class="p">(</span><span class="n">x</span><span class="o">-&gt;</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.2</span><span class="p">;</span> <span class="mf">1.0</span><span class="p">])</span>
</pre></div>


<div class="code"><pre><span></span>NLPModels.ADNLPModel(Minimization problem Generic
nvar = 2, ncon = 0 (0 linear)
,NLPModels.Counters(0,0,0,0,0,0,0,0,0,0,0),(anonymous function),(anonymous function))
</pre></div>


<p>This is enough to define the model. Let's get the objective function value at $x^0$, using only <code>nlp</code>.</p>
<div class="code"><pre><span></span><span class="n">fx</span> <span class="o">=</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;fx = </span><span class="si">$fx</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>fx = 24.199999999999996
</pre></div>


<p>Done. Let's try the gradient and Hessian.</p>
<div class="code"><pre><span></span><span class="n">gx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
<span class="n">Hx</span> <span class="o">=</span> <span class="n">hess</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;gx = </span><span class="si">$gx</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Hx = </span><span class="si">$Hx</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>gx = [-215.59999999999997,-87.99999999999999]
Hx = [1330.0 0.0
 480.0 200.0]
</pre></div>


<p>Notice how only the lower triangle of the Hessian is stored. Also notice that it is <em>dense</em>. This is a current limitation of this model. It doesn't return sparse matrices, so use it with care.</p>
<p>Let's do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method</p>
<ol>
<li>Given $x^0$, $\varepsilon &gt; 0$, and $\eta \in (0,1)$. Set $k = 0$;</li>
<li>If $\Vert \nabla f(x^k) \Vert &lt; \varepsilon$ STOP with $x^* = x^k$;</li>
<li>Compute $d^k = -\nabla f(x^k)$;</li>
<li>Compute $\alpha_k \in (0,1]$ such that $ f(x^k + \alpha_kd^k) &lt; f(x^k) + \alpha_k\eta \nabla f(x^k)^Td^k $</li>
<li>Define $x^{k+1} = x^k + \alpha_kx^k$</li>
<li>Update $k = k + 1$ and go to step 2.</li>
</ol>
<div class="code"><pre><span></span><span class="k">function</span><span class="nf"> steepest</span><span class="p">(</span><span class="n">nlp</span><span class="p">;</span> <span class="n">itmax</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="nb">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.66</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span>
  <span class="n">fx</span> <span class="o">=</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="n">∇fx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="n">slope</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">∇fx</span><span class="p">,</span> <span class="n">∇fx</span><span class="p">)</span>
  <span class="n">∇f_norm</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">slope</span><span class="p">)</span>
  <span class="n">iter</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="n">∇f_norm</span> <span class="o">&gt;</span> <span class="nb">eps</span> <span class="o">&amp;&amp;</span> <span class="n">iter</span> <span class="o">&lt;</span> <span class="n">itmax</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">x_trial</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">t</span> <span class="o">*</span> <span class="n">∇fx</span>
    <span class="n">f_trial</span> <span class="o">=</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x_trial</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="n">t</span><span class="o">*</span><span class="n">∇fx</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">fx</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">t</span> <span class="o">*</span> <span class="n">slope</span>
      <span class="n">t</span> <span class="o">*=</span> <span class="n">sigma</span>
      <span class="n">x_trial</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">t</span> <span class="o">*</span> <span class="n">∇fx</span>
      <span class="n">f_trial</span> <span class="o">=</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x_trial</span><span class="p">)</span>
    <span class="k">end</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_trial</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">f_trial</span>
    <span class="n">∇fx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">∇fx</span><span class="p">,</span> <span class="n">∇fx</span><span class="p">)</span>
    <span class="n">∇f_norm</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">slope</span><span class="p">)</span>
    <span class="n">iter</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">end</span>
  <span class="n">optimal</span> <span class="o">=</span> <span class="n">∇f_norm</span> <span class="o">&lt;=</span> <span class="nb">eps</span>
  <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="n">∇f_norm</span><span class="p">,</span> <span class="n">optimal</span><span class="p">,</span> <span class="n">iter</span>
<span class="k">end</span>

<span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="n">ngx</span><span class="p">,</span> <span class="n">optimal</span><span class="p">,</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">steepest</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;x = </span><span class="si">$x</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;fx = </span><span class="si">$fx</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;ngx = </span><span class="si">$ngx</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;optimal = </span><span class="si">$optimal</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;iter = </span><span class="si">$iter</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>x = [1.0000006499501406,1.0000013043156974]
fx = 4.2438440239813445e-13
ngx = 9.984661274466946e-7
optimal = true
</pre></div>


<p>Maybe this code is too complicated? If you're in a class you just want to show a Newton step.</p>
<div class="code"><pre><span></span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">H</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">hess</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">triu</span><span class="p">(</span><span class="n">hess</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span>
<span class="n">d</span> <span class="o">=</span> <span class="o">-</span><span class="n">H</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>\<span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>2-element Array{Float64,1}:
 0.0247191
 0.380674
</pre></div>


<p>or a few</p>
<div class="code"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="mi">5</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">H</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>\<span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">println</span><span class="p">(</span><span class="s">&quot;x = </span><span class="si">$x</span><span class="s">&quot;</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>


<div class="code"><pre><span></span>x = [-1.1752808988764043,1.3806741573033705]
x = [0.763114871176745,-3.1750338547488415]
x = [0.7634296788843487,0.5828247754975662]
x = [0.9999953110849514,0.9440273238534179]
x = [0.9999956956536327,0.999991391325645]
</pre></div>


<p>Also, notice how we can reuse the method.</p>
<div class="code"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">;</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">ADNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="n">ngx</span><span class="p">,</span> <span class="n">optimal</span><span class="p">,</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">steepest</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>([1.9318516779143746,0.5176381004152917],1.2842480268876117e-14,9.402834701778815e-7,true,120)
</pre></div>


<p>Even using a different model.</p>
<div class="code"><pre><span></span><span class="k">using</span> <span class="n">OptimizationProblems</span> <span class="c"># Defines a lot of JuMP models</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">MathProgNLPModel</span><span class="p">(</span><span class="n">woods</span><span class="p">())</span>
<span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="n">ngx</span><span class="p">,</span> <span class="n">optimal</span><span class="p">,</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">steepest</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;fx = </span><span class="si">$fx</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;ngx = </span><span class="si">$ngx</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;optimal = </span><span class="si">$optimal</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;iter = </span><span class="si">$iter</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>fx = 1.0000000000002167
ngx = 9.893253859340887e-7
optimal = true
iter = 12016
</pre></div>


<p>For constrained minimization, you need the constraints vector and bounds too. Bounds on the variables can be passed through a new vector.</p>
<div class="code"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.2</span><span class="p">;</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">lvar</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="nb">Inf</span><span class="p">;</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="n">uvar</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">;</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">c</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">;</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">]</span>
<span class="n">lcon</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">;</span> <span class="o">-</span><span class="nb">Inf</span><span class="p">]</span>
<span class="n">ucon</span> <span class="o">=</span> <span class="p">[</span><span class="nb">Inf</span><span class="p">;</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">ADNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">lvar</span><span class="o">=</span><span class="n">lvar</span><span class="p">,</span> <span class="n">uvar</span><span class="o">=</span><span class="n">uvar</span><span class="p">,</span> <span class="n">lcon</span><span class="o">=</span><span class="n">lcon</span><span class="p">,</span> <span class="n">ucon</span><span class="o">=</span><span class="n">ucon</span><span class="p">)</span>

<span class="n">println</span><span class="p">(</span><span class="s">&quot;cx = </span><span class="si">$(cons(nlp, nlp.meta.x0))</span><span class="s">&quot;</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Jx = </span><span class="si">$(jac(nlp, nlp.meta.x0))</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>cx = [-2.2,2.44]
Jx = [1.0 1.0
 -2.4 2.0]
</pre></div>


<p><a id='SimpleNLPModel-Tutorial-1'></a></p>
<h2 id="simplenlpmodel-tutorial">SimpleNLPModel Tutorial</h2>
<p>SimpleNLPModel allows you to pass every single function of the model. On the other hand, it doesn't handle anything else. Calling an undefined function will throw a <code>NotImplementedError</code>. Only the objective function is mandatory (if you don't need it, pass <code>x-&gt;0</code>).</p>
<div class="code"><pre><span></span><span class="k">using</span> <span class="n">NLPModels</span>

<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">SimpleNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>

<span class="n">fx</span> <span class="o">=</span> <span class="n">obj</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;fx = </span><span class="si">$fx</span><span class="s">&quot;</span><span class="p">)</span>

<span class="c"># grad(nlp, nlp.meta.x0) # This is undefined</span>
</pre></div>


<div class="code"><pre><span></span>fx = 5.0
</pre></div>


<div class="code"><pre><span></span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">);</span> <span class="mi">8</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)]</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">SimpleNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>

<span class="n">grad</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>2-element Array{Float64,1}:
 -2.0
 -8.0
</pre></div>


<p>"But what's to stop me from defining <code>g</code> however I want?" Nothing. So you have to be careful on how you're defining it. You should probably check your derivatives. If the function is simply defined, you can try using automatic differentiation. Alternatively, you can use the <a href="../dercheck">derivative checker</a>.</p>
<div class="code"><pre><span></span><span class="n">gradient_check</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>Dict{Int64,Float64} with 0 entries
</pre></div>


<div class="code"><pre><span></span><span class="n">gwrong</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">);</span> <span class="mi">8</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">]</span> <span class="c"># Find the error</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">SimpleNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">gwrong</span><span class="p">)</span>
<span class="n">gradient_check</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>Dict{Int64,Float64} with 1 entry:
  2 =&gt; 6.999999999951384
</pre></div>


<p>For constrained problems, we still need the constraints function, <code>lcon</code> and <code>ucon</code>. Also, let's pass the Jacobian-vector product.</p>
<div class="code"><pre><span></span><span class="n">c</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">;</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">lcon</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">;</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="n">ucon</span> <span class="o">=</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">;</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="n">Jacprod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">SimpleNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">lcon</span><span class="o">=</span><span class="n">lcon</span><span class="p">,</span> <span class="n">ucon</span><span class="o">=</span><span class="n">ucon</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">Jp</span><span class="o">=</span><span class="n">Jacprod</span><span class="p">)</span>
<span class="n">jprod</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>


<div class="code"><pre><span></span>2-element Array{Float64,1}:
 4.0
 2.0
</pre></div>


<p>Furthermore, NLPModels also works with inplace operations. Since some models do not take full advantage of this (like ADNLPModel), a user might want to define his/her own functions that do.</p>
<div class="code"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">g!</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gx</span><span class="p">)</span> <span class="o">=</span> <span class="k">begin</span>
  <span class="n">gx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span>
  <span class="n">gx</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gx</span>
<span class="k">end</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">SimpleNLPModel</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">g!</span> <span class="o">=</span><span class="n">g!</span><span class="p">)</span> <span class="c"># Watchout, g!=g! is interpreted as g != g!</span>
<span class="n">gx</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">grad!</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="n">nlp</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">x0</span><span class="p">,</span> <span class="n">gx</span><span class="p">)</span>
</pre></div>


<div class="code"><pre><span></span>2-element Array{Float64,1}:
 -2.0
  8.0
</pre></div>
          <aside class="copyright" role="note">
            
            Documentation built with
            <a href="http://www.mkdocs.org" target="_blank">MkDocs</a>
            using the
            <a href="http://squidfunk.github.io/mkdocs-material/" target="_blank">
              Material
            </a>
            theme.
          </aside>
          
            <footer class="footer">
              
  <nav class="pagination" aria-label="Footer">
    <div class="previous">
      
        <a href="../models/" title="Models">
          <span class="direction">
            Previous
          </span>
          <div class="page">
            <div class="button button-previous" role="button" aria-label="Previous">
              <i class="icon icon-back"></i>
            </div>
            <div class="stretch">
              <div class="title">
                Models
              </div>
            </div>
          </div>
        </a>
      
    </div>
    <div class="next">
      
        <a href="../api/" title="API">
          <span class="direction">
            Next
          </span>
          <div class="page">
            <div class="stretch">
              <div class="title">
                API
              </div>
            </div>
            <div class="button button-next" role="button" aria-label="Next">
              <i class="icon icon-forward"></i>
            </div>
          </div>
        </a>
      
    </div>
  </nav>

            </footer>
          
        </div>
      </article>
      <div class="results" role="status" aria-live="polite">
        <div class="scrollable">
          <div class="wrapper">
            <div class="meta"></div>
            <div class="list"></div>
          </div>
        </div>
      </div>
    </main>
    <script>
      var base_url = '..';
      var repo_id  = 'JuliaSmoothOptimizers/NLPModels.jl';
    </script>
    <script src="../assets/javascripts/application-997097ee0c.js"></script>
    
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
    
      <script src="../assets/mathjaxhelper.js"></script>
    
    
  </body>
</html>