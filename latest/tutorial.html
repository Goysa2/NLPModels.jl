<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · NLPModels.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/default.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Ubuntu+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="assets/documenter.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="../versions.js"></script><link href="assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="index.html"><img class="logo" src="assets/logo.png" alt="NLPModels.jl logo"/></a><h1>NLPModels.jl</h1><form class="search" action="search.html"><select id="version-selector" onChange="window.location.href=this.value"><option value="#" selected="selected" disabled="disabled">Version</option></select><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><a class="toctext" href="models.html">Models</a></li><li class="current"><a class="toctext" href="tutorial.html">Tutorial</a><ul class="internal"><li><a class="toctext" href="#ADNLPModel-Tutorial-1">ADNLPModel Tutorial</a></li><li><a class="toctext" href="#SimpleNLPModel-Tutorial-1">SimpleNLPModel Tutorial</a></li></ul></li><li><a class="toctext" href="api.html">API</a></li><li><a class="toctext" href="reference.html">Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="tutorial.html">Tutorial</a></li></ul><a class="edit-page" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/tree/19c37869b88f17b4daefe59a55304e2366b643c6/docs/src/tutorial.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/></header><h1><a class="nav-anchor" id="Tutorial-1" href="#Tutorial-1">Tutorial</a></h1><p>NLPModels.jl was created for two purposes:</p><ul><li><p>Allow users to access problem databases in an unified way.</p></li></ul><p>Mainly, this means  <a href="https://github.com/JuliaSmoothOptimizers/CUTEst.jl">CUTEst.jl</a>,  but it also gives access to <a href="https://github.com/JuliaSmoothOptimizers/AmplNLReader.jl">AMPL  problems</a>,  as well as JuMP defined problems (e.g. as in  <a href="https://github.com/JuliaSmoothOptimizers/OptimizationProblems.jl">OptimizationProblems.jl</a>).</p><ul><li><p>Allow users to create their own problems in the same way.</p></li></ul><p>As a consequence, optimization methods designed according to the NLPModels API  will accept NLPModels of any provenance.  See, for instance,  <a href="https://github.com/JuliaSmoothOptimizers/Optimize.jl">Optimize.jl</a>.</p><p>The main interfaces for user defined problems are</p><ul><li><p><a href="models/#adnlpmodel">ADNLPModel</a>, which defines a model easily, using automatic differentiation.</p></li><li><p><a href="models/#simplenlpmodel">SimpleNLPModel</a>, which allows users to handle all functions themselves, giving</p></li></ul><h2><a class="nav-anchor" id="ADNLPModel-Tutorial-1" href="#ADNLPModel-Tutorial-1">ADNLPModel Tutorial</a></h2><p>ADNLPModel is simple to use and is useful for classrooms. It only needs the objective function <span>$f$</span> and a starting point <span>$x^0$</span> to be well-defined. For constrained problems, you&#39;ll also need the constraints function <span>$c$</span>, and the constraints vectors <span>$c_L$</span> and <span>$c_U$</span>, such that <span>$c_L \leq c(x) \leq c_U$</span>. Equality constraints will be automatically identified as those indices <span>$i$</span> for which <span>$c_{L_i} = c_{U_i}$</span>.</p><p>Let&#39;s define the famous Rosenbrock function</p><div>\[f(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2,\]</div><p>with starting point <span>$x^0 = (-1.2,1.0)$</span>.</p><div><pre><code class="language-julia">using NLPModels

nlp = ADNLPModel(x-&gt;(x[1] - 1.0)^2 + 100*(x[2] - x[1]^2)^2 , [-1.2; 1.0])</code></pre><pre><code class="language-none">NLPModels.ADNLPModel(Minimization problem Generic
nvar = 2, ncon = 0 (0 linear)
,NLPModels.Counters(0,0,0,0,0,0,0,0,0,0,0),ex-adnlp.#1,NLPModels.#32)</code></pre></div><p>This is enough to define the model. Let&#39;s get the objective function value at <span>$x^0$</span>, using only <code>nlp</code>.</p><div><pre><code class="language-julia">fx = obj(nlp, nlp.meta.x0)
println(&quot;fx = $fx&quot;)</code></pre><pre><code class="language-none">fx = 24.199999999999996</code></pre></div><p>Done. Let&#39;s try the gradient and Hessian.</p><div><pre><code class="language-julia">gx = grad(nlp, nlp.meta.x0)
Hx = hess(nlp, nlp.meta.x0)
println(&quot;gx = $gx&quot;)
println(&quot;Hx = $Hx&quot;)</code></pre><pre><code class="language-none">gx = [-215.6,-88.0]
Hx = [1330.0 0.0; 480.0 200.0]</code></pre></div><p>Notice how only the lower triangle of the Hessian is stored. Also notice that it is <em>dense</em>. This is a current limitation of this model. It doesn&#39;t return sparse matrices, so use it with care.</p><p>Let&#39;s do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method</p><ol><li><p>Given <span>$x^0$</span>, <span>$\varepsilon &gt; 0$</span>, and <span>$\eta \in (0,1)$</span>. Set <span>$k = 0$</span>;</p></li><li><p>If <span>$\Vert \nabla f(x^k) \Vert &lt; \varepsilon$</span> STOP with <span>$x^* = x^k$</span>;</p></li><li><p>Compute <span>$d^k = -\nabla f(x^k)$</span>;</p></li><li><p>Compute <span>$\alpha_k \in (0,1]$</span> such that <span>$f(x^k + \alpha_kd^k) &lt; f(x^k) + \alpha_k\eta \nabla f(x^k)^Td^k$</span></p></li><li><p>Define <span>$x^{k+1} = x^k + \alpha_kx^k$</span></p></li><li><p>Update <span>$k = k + 1$</span> and go to step 2.</p></li></ol><div><pre><code class="language-julia">function steepest(nlp; itmax=100000, eta=1e-4, eps=1e-6, sigma=0.66)
  x = nlp.meta.x0
  fx = obj(nlp, x)
  ∇fx = grad(nlp, x)
  slope = dot(∇fx, ∇fx)
  ∇f_norm = sqrt(slope)
  iter = 0
  while ∇f_norm &gt; eps &amp;&amp; iter &lt; itmax
    t = 1.0
    x_trial = x - t * ∇fx
    f_trial = obj(nlp, x_trial)
    while f_trial &gt; fx - eta * t * slope
      t *= sigma
      x_trial = x - t * ∇fx
      f_trial = obj(nlp, x_trial)
    end
    x = x_trial
    fx = f_trial
    ∇fx = grad(nlp, x)
    slope = dot(∇fx, ∇fx)
    ∇f_norm = sqrt(slope)
    iter += 1
  end
  optimal = ∇f_norm &lt;= eps
  return x, fx, ∇f_norm, optimal, iter
end

x, fx, ngx, optimal, iter = steepest(nlp)
println(&quot;x = $x&quot;)
println(&quot;fx = $fx&quot;)
println(&quot;ngx = $ngx&quot;)
println(&quot;optimal = $optimal&quot;)
println(&quot;iter = $iter&quot;)</code></pre><pre><code class="language-none">x = [1.0,1.0]
fx = 4.2438440239813445e-13
ngx = 9.984661274466946e-7
optimal = true</code></pre></div><p>Maybe this code is too complicated? If you&#39;re in a class you just want to show a Newton step.</p><div><pre><code class="language-julia">g(x) = grad(nlp, x)
H(x) = hess(nlp, x) + triu(hess(nlp, x)&#39;, 1)
x = nlp.meta.x0
d = -H(x)\g(x)</code></pre><pre><code class="language-none">2-element Array{Float64,1}:
 0.0247191
 0.380674</code></pre></div><p>or a few</p><div><pre><code class="language-julia">for i = 1:5
  x = x - H(x)\g(x)
  println(&quot;x = $x&quot;)
end</code></pre><pre><code class="language-none">x = [-1.17528,1.38067]
x = [0.763115,-3.17503]
x = [0.76343,0.582825]
x = [0.999995,0.944027]
x = [0.999996,0.999991]</code></pre></div><p>Also, notice how we can reuse the method.</p><div><pre><code class="language-julia">f(x) = (x[1]^2 + x[2]^2 - 4)^2 + (x[1]*x[2] - 1)^2
x0 = [2.0; 1.0]
nlp = ADNLPModel(f, x0)

x, fx, ngx, optimal, iter = steepest(nlp)</code></pre><pre><code class="language-none">([1.93185,0.517638],1.2842480268876117e-14,9.402834701778815e-7,true,120)</code></pre></div><p>Even using a different model.</p><div><pre><code class="language-julia">using OptimizationProblems # Defines a lot of JuMP models

nlp = MathProgNLPModel(woods())
x, fx, ngx, optimal, iter = steepest(nlp)
println(&quot;fx = $fx&quot;)
println(&quot;ngx = $ngx&quot;)
println(&quot;optimal = $optimal&quot;)
println(&quot;iter = $iter&quot;)</code></pre><pre><code class="language-none">WARNING: The curly syntax (sum{},prod{},norm2{}) is deprecated in favor of the new generator syntax (sum(),prod(),norm()).
WARNING: Replace sum{((x[i] ^ 2 + x[n] ^ 2) ^ 2 - 4 * x[i]) + 3,i = 1:n - 1} with sum(((x[i] ^ 2 + x[n] ^ 2) ^ 2 - 4 * x[i]) + 3 for i = 1:n - 1).
WARNING: Replace sum{(3 - 4 * x[i]) ^ 2 + (x[i] ^ 2 + 2 * x[i + 1] ^ 2 + 3 * x[i + 2] ^ 2 + 4 * x[i + 3] ^ 2 + 5 * x[n] ^ 2) ^ 2,i = 1:n - 4} with sum((3 - 4 * x[i]) ^ 2 + (x[i] ^ 2 + 2 * x[i + 1] ^ 2 + 3 * x[i + 2] ^ 2 + 4 * x[i + 3] ^ 2 + 5 * x[n] ^ 2) ^ 2 for i = 1:n - 4).
WARNING: Replace sum{abs(((1 - x[i - 1]) - 2 * x[i + 1]) + (3 - x[i] / 2) * x[i]) ^ p,i = 2:n - 1} with sum(abs(((1 - x[i - 1]) - 2 * x[i + 1]) + (3 - x[i] / 2) * x[i]) ^ p for i = 2:n - 1).
WARNING: Replace sum{abs(x[i] + x[i + n2]) ^ p,i = 1:n2} with sum(abs(x[i] + x[i + n2]) ^ p for i = 1:n2).
WARNING: Replace sum{((x[i] * (2 + 5 * x[i] ^ 2) + 1) - sum{x[j] * (1 + x[j]),j = max(1,i - ml):min(n,i + mu); j != i}) ^ 2,i = 1:n} with sum(((x[i] * (2 + 5 * x[i] ^ 2) + 1) - sum{x[j] * (1 + x[j]),j = max(1,i - ml):min(n,i + mu); j != i}) ^ 2 for i = 1:n).
WARNING: Replace sum{x[j] * (1 + x[j]),j = max(1,i - ml):min(n,i + mu); j != i} with sum(x[j] * (1 + x[j]) for j = max(1,i - ml):min(n,i + mu) if j != i).
WARNING: Replace sum{100 * (x[2i] - x[2i - 1] ^ 2) ^ 2 + (1 - x[2i - 1]) ^ 2 + 90 * (x[2i + 2] - x[2i + 1] ^ 2) ^ 2 + (1 - x[2i + 1]) ^ 2 + 10 * ((x[2i] + x[2i + 2]) - 2) ^ 2 + 0.1 * (x[2i] - x[2i + 2]) ^ 2,i = 1:div(n,2) - 1} with sum(100 * (x[2i] - x[2i - 1] ^ 2) ^ 2 + (1 - x[2i - 1]) ^ 2 + 90 * (x[2i + 2] - x[2i + 1] ^ 2) ^ 2 + (1 - x[2i + 1]) ^ 2 + 10 * ((x[2i] + x[2i + 2]) - 2) ^ 2 + 0.1 * (x[2i] - x[2i + 2]) ^ 2 for i = 1:div(n,2) - 1).
WARNING: Replace sum{cos(x[i] ^ 2 - 0.5 * x[i + 1]),i = 1:n - 1} with sum(cos(x[i] ^ 2 - 0.5 * x[i + 1]) for i = 1:n - 1).
WARNING: Replace sum{(exp(x[2i - 1]) - x[2i]) ^ 4 + 100 * (x[2i] - x[2i + 1]) ^ 6 + ((tan(x[2i + 1] - x[2i + 2]) + x[2i + 1]) - x[2i + 2]) ^ 4 + x[2i - 1] ^ 8 + (x[2i + 2] - 1) ^ 2,i = 1:div(n,2) - 1} with sum((exp(x[2i - 1]) - x[2i]) ^ 4 + 100 * (x[2i] - x[2i + 1]) ^ 6 + ((tan(x[2i + 1] - x[2i + 2]) + x[2i + 1]) - x[2i + 2]) ^ 4 + x[2i - 1] ^ 8 + (x[2i + 2] - 1) ^ 2 for i = 1:div(n,2) - 1).
WARNING: Replace sum{x[j],j = i:min(i + b,n)} with sum(x[j] for j = i:min(i + b,n)).
WARNING: Replace sum{f[i] * (f[i] * (f[i] ^ 2 - 20) - 0.1),i = 1:n} with sum(f[i] * (f[i] * (f[i] ^ 2 - 20) - 0.1) for i = 1:n).
WARNING: Replace sum{(i / n) * α * x[i] ^ 2,i = 1:n} with sum((i / n) * α * x[i] ^ 2 for i = 1:n).
WARNING: Replace sum{β * x[i] ^ 2 * (x[i + 1] + x[i + 1] ^ 2) ^ 2,i = 1:n - 1} with sum(β * x[i] ^ 2 * (x[i + 1] + x[i + 1] ^ 2) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{γ * x[i] ^ 2 * x[i + m] ^ 4,i = 1:2m} with sum(γ * x[i] ^ 2 * x[i + m] ^ 4 for i = 1:2m).
WARNING: Replace sum{(i / n) * δ * x[i] * x[i + 2m],i = 1:m} with sum((i / n) * δ * x[i] * x[i + 2m] for i = 1:m).
WARNING: Replace sum{(i / n) ^ 2 * α * x[i] ^ 2,i = 1:n} with sum((i / n) ^ 2 * α * x[i] ^ 2 for i = 1:n).
WARNING: Replace sum{(i / n) ^ 2 * δ * x[i] * x[i + 2m],i = 1:m} with sum((i / n) ^ 2 * δ * x[i] * x[i + 2m] for i = 1:m).
WARNING: Replace sum{(i / n) * β * x[i] ^ 2 * (x[i + 1] + x[i + 1] ^ 2) ^ 2,i = 1:n - 1} with sum((i / n) * β * x[i] ^ 2 * (x[i + 1] + x[i + 1] ^ 2) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{(i / n) * γ * x[i] ^ 2 * x[i + m] ^ 4,i = 1:2m} with sum((i / n) * γ * x[i] ^ 2 * x[i + m] ^ 4 for i = 1:2m).
WARNING: Replace sum{(x[i] - i) ^ 4,i = 1:n} with sum((x[i] - i) ^ 4 for i = 1:n).
WARNING: Replace sum{(x[i] - 2) ^ 4 + (x[i] * x[i + 1] - 2 * x[i + 1]) ^ 2 + (x[i + 1] + 1) ^ 2,i = 1:n - 1} with sum((x[i] - 2) ^ 4 + (x[i] * x[i + 1] - 2 * x[i + 1]) ^ 2 + (x[i + 1] + 1) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{sin((x[1] + x[i] ^ 2) - 1),i = 1:n - 1} with sum(sin((x[1] + x[i] ^ 2) - 1) for i = 1:n - 1).
WARNING: Replace sum{((x[i] ^ 2 + x[i + 1] ^ 2) ^ 2 - 4 * x[i]) + 3,i = 1:n - 1} with sum(((x[i] ^ 2 + x[i + 1] ^ 2) ^ 2 - 4 * x[i]) + 3 for i = 1:n - 1).
WARNING: Replace sum{(x[i + 1] - x[i] ^ 2) ^ 2,i = 1:n - 1} with sum((x[i + 1] - x[i] ^ 2) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{(x[i] - 1.0) ^ 2,i = 1:n - 1} with sum((x[i] - 1.0) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{(x[i] - x[i - 1] ^ 2) ^ 2,i = 2:n} with sum((x[i] - x[i - 1] ^ 2) ^ 2 for i = 2:n).
WARNING: Replace sum{(1.0 - x[i]) ^ 2,i = 2:n} with sum((1.0 - x[i]) ^ 2 for i = 2:n).
WARNING: Replace sum{(x[i - 1] - x[i] ^ 2) ^ 2 * (1.5 + sin(i)) ^ 2,i = 2:n} with sum((x[i - 1] - x[i] ^ 2) ^ 2 * (1.5 + sin(i)) ^ 2 for i = 2:n).
WARNING: Replace sum{(x[i - 1] - 16.0 * x[i] ^ 2 * (1.5 + sin(i)) ^ 2) ^ 2,i = 2:n} with sum((x[i - 1] - 16.0 * x[i] ^ 2 * (1.5 + sin(i)) ^ 2) ^ 2 for i = 2:n).
WARNING: Replace sum{(x[i] - x[i + 1]) ^ 2,i = 1:n - 1} with sum((x[i] - x[i + 1]) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{100.0 * (1 + 2.0 / h ^ 2) * sin(x[i] / 100.0) + (1 / h ^ 2) * cos(x[i]),i = 1:n} with sum(100.0 * (1 + 2.0 / h ^ 2) * sin(x[i] / 100.0) + (1 / h ^ 2) * cos(x[i]) for i = 1:n).
WARNING: Replace sum{2 * x[i] + cos(x[i]),i = 1:n} with sum(2 * x[i] + cos(x[i]) for i = 1:n).
WARNING: Replace sum{(((x[i + 1] - x[i]) + 1) - x[i] ^ 2) ^ 2,i = 1:n - 1} with sum((((x[i + 1] - x[i]) + 1) - x[i] ^ 2) ^ 2 for i = 1:n - 1).
WARNING: Replace sum{sum{(100.0 * sqrt(0.5 * (p - 1) ^ 2 * ((x[i,j] - x[i + 1,j + 1]) ^ 2 + (x[i + 1,j] - x[i,j + 1]) ^ 2) + 1.0)) / scale,i = 1:p - 1},j = 1:p - 1} with sum(sum{(100.0 * sqrt(0.5 * (p - 1) ^ 2 * ((x[i,j] - x[i + 1,j + 1]) ^ 2 + (x[i + 1,j] - x[i,j + 1]) ^ 2) + 1.0)) / scale,i = 1:p - 1} for j = 1:p - 1).
WARNING: Replace sum{(100.0 * sqrt(0.5 * (p - 1) ^ 2 * ((x[i,j] - x[i + 1,j + 1]) ^ 2 + (x[i + 1,j] - x[i,j + 1]) ^ 2) + 1.0)) / scale,i = 1:p - 1} with sum((100.0 * sqrt(0.5 * (p - 1) ^ 2 * ((x[i,j] - x[i + 1,j + 1]) ^ 2 + (x[i + 1,j] - x[i,j + 1]) ^ 2) + 1.0)) / scale for i = 1:p - 1).
WARNING: Replace sum{((((5.0 - x[i + 1]) * x[i + 1] ^ 2 + x[i]) - 2 * x[i + 1]) - 13.0) ^ 2,i = 1:ngs} with sum(((((5.0 - x[i + 1]) * x[i + 1] ^ 2 + x[i]) - 2 * x[i + 1]) - 13.0) ^ 2 for i = 1:ngs).
WARNING: Replace sum{((((1.0 + x[i + 1]) * x[i + 1] ^ 2 + x[i]) - 14 * x[i + 1]) - 29.0) ^ 2,i = 1:ngs} with sum(((((1.0 + x[i + 1]) * x[i + 1] ^ 2 + x[i]) - 14 * x[i + 1]) - 29.0) ^ 2 for i = 1:ngs).
WARNING: Replace sum{sin(ζ * x[i]) ^ 2 * sin(ζ * x[i + 1]) ^ 2 + 0.05 * (x[i] ^ 2 + x[i + 1] ^ 2),i = 1:n - 1} with sum(sin(ζ * x[i]) ^ 2 * sin(ζ * x[i + 1]) ^ 2 + 0.05 * (x[i] ^ 2 + x[i + 1] ^ 2) for i = 1:n - 1).
WARNING: Replace sum{sin(x[i] / 100.0),i = 1:n} with sum(sin(x[i] / 100.0) for i = 1:n).
WARNING: Replace sum{cos((2.0 * x[i] - x[n]) - x[1]),i = 2:n - 1} with sum(cos((2.0 * x[i] - x[n]) - x[1]) for i = 2:n - 1).
WARNING: Replace sum{4.0 * (x[i] ^ 2 - x[1]) ^ 2 + (x[i] - 1) ^ 2,i = 1:n} with sum(4.0 * (x[i] ^ 2 - x[1]) ^ 2 + (x[i] - 1) ^ 2 for i = 1:n).
WARNING: Replace sum{(((2.0 * x[i] - x[i - 1]) - x[i + 1]) + (h ^ 2 / 2.0) * (x[i] + (i - 1) * h + 1) ^ 3) ^ 2,i = 2:n - 1} with sum((((2.0 * x[i] - x[i - 1]) - x[i + 1]) + (h ^ 2 / 2.0) * (x[i] + (i - 1) * h + 1) ^ 3) ^ 2 for i = 2:n - 1).
WARNING: Replace sum{(10.0 / i) * sum{x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2),j = 1:20} ^ 2 - 0.2 * sum{x[(i + j) - 1],j = 1:20},i = 1:n - 30} with sum((10.0 / i) * sum{x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2),j = 1:20} ^ 2 - 0.2 * sum{x[(i + j) - 1],j = 1:20} for i = 1:n - 30).
WARNING: Replace sum{x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2),j = 1:20} with sum(x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2) for j = 1:20).
WARNING: Replace sum{x[(i + j) - 1],j = 1:20} with sum(x[(i + j) - 1] for j = 1:20).
WARNING: Replace sum{x[i] ^ 4 + 2,i = 1:n - 10} with sum(x[i] ^ 4 + 2 for i = 1:n - 10).
WARNING: Replace sum{x[i] * x[i + 10] * x[(i + n) - 10] + 2.0 * x[(i + n) - 10] ^ 2,i = 1:10} with sum(x[i] * x[i + 10] * x[(i + n) - 10] + 2.0 * x[(i + n) - 10] ^ 2 for i = 1:10).
WARNING: Replace sum{(10.0 / i) * sum{x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2),j = 1:20} ^ 2 - 0.2 * sum{x[(i + j) - 1],j = 1:20},i = 1:n - 19} with sum((10.0 / i) * sum{x[(i + j) - 1] / (1 + x[(i + j) - 1] ^ 2),j = 1:20} ^ 2 - 0.2 * sum{x[(i + j) - 1],j = 1:20} for i = 1:n - 19).
WARNING: Replace sum{100.0 * x[i] ^ 4 + 2.0,i = 1:n} with sum(100.0 * x[i] ^ 4 + 2.0 for i = 1:n).
WARNING: Replace sum{(x[i] + x[mod(3i - 2,n) + 1] + x[mod(7i - 3,n) + 1]) ^ 2 + 4.0 * cos(x[i] + x[mod(3i - 2,n) + 1] + x[mod(7i - 3,n) + 1]),i = 1:n} with sum((x[i] + x[mod(3i - 2,n) + 1] + x[mod(7i - 3,n) + 1]) ^ 2 + 4.0 * cos(x[i] + x[mod(3i - 2,n) + 1] + x[mod(7i - 3,n) + 1]) for i = 1:n).
WARNING: Replace sum{(x[i] + x[mod(2i - 1,n) + 1] + x[mod(3i - 1,n) + 1]) ^ 2 + 4.0 * cos(x[i] + x[mod(2i - 1,n) + 1] + x[mod(3i - 1,n) + 1]),i = 1:n} with sum((x[i] + x[mod(2i - 1,n) + 1] + x[mod(3i - 1,n) + 1]) ^ 2 + 4.0 * cos(x[i] + x[mod(2i - 1,n) + 1] + x[mod(3i - 1,n) + 1]) for i = 1:n).
WARNING: Replace sum{(100.0 * x[1] - x[i - 1] ^ 2) ^ 2,i = 2:n} with sum((100.0 * x[1] - x[i - 1] ^ 2) ^ 2 for i = 2:n).
WARNING: Replace sum{(x[i] + x[i + 1] + x[n]) ^ 4,i = 1:n - 2} with sum((x[i] + x[i + 1] + x[n]) ^ 4 for i = 1:n - 2).
WARNING: Replace sum{a * ((exp(x[i] / 10.0) + exp(x[i - 1] / 10.0)) - y[i]) ^ 2,i = 2:n} with sum(a * ((exp(x[i] / 10.0) + exp(x[i - 1] / 10.0)) - y[i]) ^ 2 for i = 2:n).
WARNING: Replace sum{a * (exp(x[(i - n) + 1] / 10.0) - exp(-1 / 10)) ^ 2,i = n + 1:2n - 1} with sum(a * (exp(x[(i - n) + 1] / 10.0) - exp(-1 / 10)) ^ 2 for i = n + 1:2n - 1).
WARNING: Replace sum{((n - j) + 1) * x[j] ^ 2,j = 1:n} with sum(((n - j) + 1) * x[j] ^ 2 for j = 1:n).
WARNING: Replace sum{(x[i] - 1.0) ^ 2,i = 1:div(n,2)} with sum((x[i] - 1.0) ^ 2 for i = 1:div(n,2)).
WARNING: Replace sum{((x[i] + 2.0 * x[i + 1] + 10.0 * x[i + 2]) - 1.0) ^ 2,i = 1:n - 2} with sum(((x[i] + 2.0 * x[i + 1] + 10.0 * x[i + 2]) - 1.0) ^ 2 for i = 1:n - 2).
WARNING: Replace sum{((2.0 * x[i] + x[i + 1]) - 3.0) ^ 2,i = 1:n - 2} with sum(((2.0 * x[i] + x[i + 1]) - 3.0) ^ 2 for i = 1:n - 2).
WARNING: Replace sum{x[i] ^ 2 - n,i = 1:n} with sum(x[i] ^ 2 - n for i = 1:n).
WARNING: Replace sum{(x[j] + 10.0 * x[j + 1]) ^ 2 + 5.0 * (x[j + 2] - x[j + 3]) ^ 2 + (x[j + 1] - 2.0 * x[j + 2]) ^ 4 + 10.0 * (x[j] - x[j + 3]) ^ 4,j = 1:div(n,4)} with sum((x[j] + 10.0 * x[j + 1]) ^ 2 + 5.0 * (x[j + 2] - x[j + 3]) ^ 2 + (x[j + 1] - 2.0 * x[j + 2]) ^ 4 + 10.0 * (x[j] - x[j + 3]) ^ 4 for j = 1:div(n,4)).
WARNING: Replace sum{(((2.0 + 5.0 * p[i] ^ 2 * x[i] ^ 2) * p[i] * x[i] + 1.0) - sum{p[j] * x[j] * (1.0 + p[j] * x[j]),j = J[i]}) ^ 2,i = 1:n} with sum((((2.0 + 5.0 * p[i] ^ 2 * x[i] ^ 2) * p[i] * x[i] + 1.0) - sum{p[j] * x[j] * (1.0 + p[j] * x[j]),j = J[i]}) ^ 2 for i = 1:n).
WARNING: Replace sum{p[j] * x[j] * (1.0 + p[j] * x[j]),j = J[i]} with sum(p[j] * x[j] * (1.0 + p[j] * x[j]) for j = J[i]).
WARNING: Replace sum{cos(p[i] ^ 2 * x[i] ^ 2 - (p[i + 1] * x[i + 1]) / 2.0),i = 1:n - 1} with sum(cos(p[i] ^ 2 * x[i] ^ 2 - (p[i + 1] * x[i + 1]) / 2.0) for i = 1:n - 1).
WARNING: Replace sum{(-(1.0 / (1.0 + (x[i] - x[i + 1]) ^ 2)) - sin((pi * x[i + 1] + x[i + 2]) / 2.0)) - exp(-(((x[i] + x[i + 2]) / x[i + 1] - 2.0) ^ 2)),i = 1:n - 2} with sum((-(1.0 / (1.0 + (x[i] - x[i + 1]) ^ 2)) - sin((pi * x[i + 1] + x[i + 2]) / 2.0)) - exp(-(((x[i] + x[i + 2]) / x[i + 1] - 2.0) ^ 2)) for i = 1:n - 2).
WARNING: Replace sum{((sin(x[i] - x[n]) - x[1] ^ 2) + x[i] ^ 2) ^ 2,i = 2:n - 1} with sum(((sin(x[i] - x[n]) - x[1] ^ 2) + x[i] ^ 2) ^ 2 for i = 2:n - 1).
WARNING: Replace sum{i * (sin(x[i]) + sin(x[mod(2i - 1,n) + 1]) + sin(x[mod(3i - 1,n) + 1]) + sin(x[mod(5i - 1,n) + 1]) + sin(x[mod(7i - 1,n) + 1]) + sin(x[mod(11i - 1,n) + 1])) ^ 2,i = 1:n} with sum(i * (sin(x[i]) + sin(x[mod(2i - 1,n) + 1]) + sin(x[mod(3i - 1,n) + 1]) + sin(x[mod(5i - 1,n) + 1]) + sin(x[mod(7i - 1,n) + 1]) + sin(x[mod(11i - 1,n) + 1])) ^ 2 for i = 1:n).
WARNING: Replace sum{i * (x[i] ^ 2 + x[mod(2i - 1,n) + 1] ^ 2 + x[mod(3i - 1,n) + 1] ^ 2 + x[mod(5i - 1,n) + 1] ^ 2 + x[mod(7i - 1,n) + 1] ^ 2 + x[mod(11i - 1,n) + 1] ^ 2) ^ 2,i = 1:n} with sum(i * (x[i] ^ 2 + x[mod(2i - 1,n) + 1] ^ 2 + x[mod(3i - 1,n) + 1] ^ 2 + x[mod(5i - 1,n) + 1] ^ 2 + x[mod(7i - 1,n) + 1] ^ 2 + x[mod(11i - 1,n) + 1] ^ 2) ^ 2 for i = 1:n).
WARNING: Replace sum{100.0 * (x[2i] - x[2i - 1] ^ 2) ^ 2 + (x[2i - 1] - 1.0) ^ 2,i = 1:div(n,2)} with sum(100.0 * (x[2i] - x[2i - 1] ^ 2) ^ 2 + (x[2i - 1] - 1.0) ^ 2 for i = 1:div(n,2)).
WARNING: Replace sum{(10.0 / (n + 2) + x[i + 2] ^ 2) * (2.0 - exp(-((x[i] - x[i + 1]) ^ 2) / (0.1 + x[i + 2] ^ 2))),i = 1:n - 2} with sum((10.0 / (n + 2) + x[i + 2] ^ 2) * (2.0 - exp(-((x[i] - x[i + 1]) ^ 2) / (0.1 + x[i + 2] ^ 2))) for i = 1:n - 2).
WARNING: Replace sum{(x[1] ^ 2 - x[i + 1] ^ 2) ^ 2,i = 1:n - 2} with sum((x[1] ^ 2 - x[i + 1] ^ 2) ^ 2 for i = 1:n - 2).
WARNING: Replace sum{100 * (x[4i - 2] - x[4i - 3] ^ 2) ^ 2 + (1 - x[4i - 3]) ^ 2 + 90 * (x[4i] - x[4i - 1] ^ 2) ^ 2 + (1 - x[4i - 1]) ^ 2 + 10 * ((x[4i - 2] + x[4i]) - 2) ^ 2 + 0.1 * (x[4i - 2] - x[4i]) ^ 2,i = 1:div(n,4)} with sum(100 * (x[4i - 2] - x[4i - 3] ^ 2) ^ 2 + (1 - x[4i - 3]) ^ 2 + 90 * (x[4i] - x[4i - 1] ^ 2) ^ 2 + (1 - x[4i - 1]) ^ 2 + 10 * ((x[4i - 2] + x[4i]) - 2) ^ 2 + 0.1 * (x[4i - 2] - x[4i]) ^ 2 for i = 1:div(n,4)).
WARNING: Replace sum{exp(x[j]) * ((c[j] + x[j]) - log(sum{exp(x[k]),k = 1:10})),j = 1:10} with sum(exp(x[j]) * ((c[j] + x[j]) - log(sum{exp(x[k]),k = 1:10})) for j = 1:10).
WARNING: Replace sum{exp(x[k]),k = 1:10} with sum(exp(x[k]) for k = 1:10).
WARNING: Replace sum{x[j] * (c[j] + log(x[j] / sum{x[k],k = 1:10})),j = 1:10} with sum(x[j] * (c[j] + log(x[j] / sum{x[k],k = 1:10})) for j = 1:10).
WARNING: Replace sum{x[k],k = 1:10} with sum(x[k] for k = 1:10).
fx = 1.0000000000002167
ngx = 9.893253859340887e-7
optimal = true
iter = 12016</code></pre></div><p>For constrained minimization, you need the constraints vector and bounds too. Bounds on the variables can be passed through a new vector.</p><div><pre><code class="language-julia">f(x) = (x[1] - 1.0)^2 + 100*(x[2] - x[1]^2)^2
x0 = [-1.2; 1.0]
lvar = [-Inf; 0.1]
uvar = [0.5; 0.5]
c(x) = [x[1] + x[2] - 2; x[1]^2 + x[2]^2]
lcon = [0.0; -Inf]
ucon = [Inf; 1.0]
nlp = ADNLPModel(f, x0, c=c, lvar=lvar, uvar=uvar, lcon=lcon, ucon=ucon)

println(&quot;cx = $(cons(nlp, nlp.meta.x0))&quot;)
println(&quot;Jx = $(jac(nlp, nlp.meta.x0))&quot;)</code></pre><pre><code class="language-none">cx = [-2.2,2.44]
Jx = [1.0 1.0; -2.4 2.0]</code></pre></div><h2><a class="nav-anchor" id="SimpleNLPModel-Tutorial-1" href="#SimpleNLPModel-Tutorial-1">SimpleNLPModel Tutorial</a></h2><p>SimpleNLPModel allows you to pass every single function of the model. On the other hand, it doesn&#39;t handle anything else. Calling an undefined function will throw a <code>NotImplementedError</code>. Only the objective function is mandatory (if you don&#39;t need it, pass <code>x-&gt;0</code>).</p><div><pre><code class="language-julia">using NLPModels

f(x) = (x[1] - 1.0)^2 + 4*(x[2] - 1.0)^2
x0 = zeros(2)
nlp = SimpleNLPModel(f, x0)

fx = obj(nlp, nlp.meta.x0)
println(&quot;fx = $fx&quot;)

# grad(nlp, nlp.meta.x0) # This is undefined</code></pre><pre><code class="language-none">fx = 5.0</code></pre></div><div><pre><code class="language-julia">g(x) = [2*(x[1] - 1.0); 8*(x[2] - 1.0)]
nlp = SimpleNLPModel(f, x0, g=g)

grad(nlp, nlp.meta.x0)</code></pre><pre><code class="language-none">2-element Array{Float64,1}:
 -2.0
 -8.0</code></pre></div><p>&quot;But what&#39;s to stop me from defining <code>g</code> however I want?&quot; Nothing. So you have to be careful on how you&#39;re defining it. You should probably check your derivatives. If the function is simply defined, you can try using automatic differentiation. Alternatively, you can use the <a href="dercheck">derivative checker</a>.</p><div><pre><code class="language-julia">gradient_check(nlp)</code></pre><pre><code class="language-none">Dict{Int64,Float64} with 0 entries</code></pre></div><div><pre><code class="language-julia">gwrong(x) = [2*(x[1] - 1.0); 8*x[2] - 1.0] # Find the error
nlp = SimpleNLPModel(f, x0, g=gwrong)
gradient_check(nlp)</code></pre><pre><code class="language-none">Dict{Int64,Float64} with 1 entry:
  2 =&gt; 7.0</code></pre></div><p>For constrained problems, we still need the constraints function, <code>lcon</code> and <code>ucon</code>. Also, let&#39;s pass the Jacobian-vector product.</p><div><pre><code class="language-julia">c(x) = [x[1]^2 + x[2]^2; x[1]*x[2] - 1]
lcon = [1.0; 0.0]
ucon = [4.0; 0.0]
Jacprod(x, v) = [2*x[1]*v[1] + 2*x[2]*v[2]; x[2]*v[1] + x[1]*v[2]]
nlp = SimpleNLPModel(f, x0, c=c, lcon=lcon, ucon=ucon, g=g, Jp=Jacprod)
jprod(nlp, ones(2), ones(2))</code></pre><pre><code class="language-none">2-element Array{Float64,1}:
 4.0
 2.0</code></pre></div><p>Furthermore, NLPModels also works with inplace operations. Since some models do not take full advantage of this (like ADNLPModel), a user might want to define his/her own functions that do.</p><div><pre><code class="language-julia">f(x) = (x[1] - 1.0)^2 + 4*(x[2] - 1.0)^2
x0 = zeros(2)
g!(x, gx) = begin
  gx[1] = 2*(x[1] - 1.0)
  gx[2] = 8*(x[2] = 1.0)
  return gx
end
nlp = SimpleNLPModel(f, x0, g! =g!) # Watchout, g!=g! is interpreted as g != g!
gx = zeros(2)
grad!(nlp, nlp.meta.x0, gx)</code></pre><pre><code class="language-none">2-element Array{Float64,1}:
 -2.0
  8.0</code></pre></div><footer><hr/><a class="previous" href="models.html"><span class="direction">Previous</span><span class="title">Models</span></a><a class="next" href="api.html"><span class="direction">Next</span><span class="title">API</span></a></footer></article></body></html>
