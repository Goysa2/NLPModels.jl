{
    "docs": [
        {
            "location": "/", 
            "text": "NLPModels.jl documentation\n\n\nThis package provides general guidelines to represent optimization problems in Julia and a standardized API to evaluate the functions and their derivatives. The main objective is to be able to rely on that API when designing optimization solvers in Julia.\n\n\nThe general form of the optimization problem is \n\\begin{align*} \\min \\quad & f(x) \\\\\n& c_i(x) = 0, \\quad i \\in E, \\\\\n& c_{L_i} \\leq c_i(x) \\leq c_{U_i}, \\quad i \\in I, \\\\\n& \\ell \\leq x \\leq u, \\end{align*}\n where $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$, $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$, $E\\cup I = \\{1,2,\\dots,m\\}$, $E\\cap I = \\emptyset$, and $c_{L_i}, c_{U_i}, \\ell_j, u_j \\in \\mathbb{R}\\cup\\{\\pm\\infty\\}$ for $i = 1,\\dots,m$ and $j = 1,\\dots,n$.\n\n\nFor computational reasons, we write \n\\begin{align*} \\min \\quad & f(x) \\\\\n& c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u, \\end{align*}\n defining $c_{L_i} = c_{U_i}$ for all $i \\in E$. The Lagrangian of this problem is defined as \n\\begin{align*} L(x,\\lambda,z^L,z^U;\\sigma) = \\sigma f(x) + c(x)^T\\lambda  + \\sum_{i=1}^n z_i^L(x_i-l_i) + \\sum_{i=1}^nz_i^U(u_i-x_i), \\end{align*}\n where $\\sigma$ is a scaling parameter included for computational reasons.\n\n\nOptimization problems are represented by an instance/subtype of \nAbstractNLPModel\n. Such instances are composed of\n\n\n\n\nan instance of \nNLPModelMeta\n, which provides information about the problem,   including the number of variables, constraints, bounds on the variables, etc.\n\n\nother data specific to the provenance of the problem.\n\n\n\n\n\n\nInternal Interfaces\n\n\n\n\nSimpleNLPModel\n: Uses    \nForwardDiff\n to compute the    derivatives. It has a very simple interface.\n\n\nJuMPNLPModel\n: Uses a \nJuMP\n model.\n\n\nSlackModel\n: Creates an equality constrained problem with bounds     on the variables using an existing NLPModel.\n\n\n\n\n\n\nExternal Interfaces\n\n\n\n\nAmplModel\n: Defined in    \nAmplNLReader.jl\n    for problems modeled using \nAMPL\n\n\nCUTEstModel\n: Defined in    \nCUTEst.jl\n for    problems from \nCUTEst\n.\n\n\n\n\nIf you want your interface here, open a PR.", 
            "title": "Home"
        }, 
        {
            "location": "/#nlpmodelsjl-documentation", 
            "text": "This package provides general guidelines to represent optimization problems in Julia and a standardized API to evaluate the functions and their derivatives. The main objective is to be able to rely on that API when designing optimization solvers in Julia.  The general form of the optimization problem is  \\begin{align*} \\min \\quad & f(x) \\\\\n& c_i(x) = 0, \\quad i \\in E, \\\\\n& c_{L_i} \\leq c_i(x) \\leq c_{U_i}, \\quad i \\in I, \\\\\n& \\ell \\leq x \\leq u, \\end{align*}  where $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$, $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$, $E\\cup I = \\{1,2,\\dots,m\\}$, $E\\cap I = \\emptyset$, and $c_{L_i}, c_{U_i}, \\ell_j, u_j \\in \\mathbb{R}\\cup\\{\\pm\\infty\\}$ for $i = 1,\\dots,m$ and $j = 1,\\dots,n$.  For computational reasons, we write  \\begin{align*} \\min \\quad & f(x) \\\\\n& c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u, \\end{align*}  defining $c_{L_i} = c_{U_i}$ for all $i \\in E$. The Lagrangian of this problem is defined as  \\begin{align*} L(x,\\lambda,z^L,z^U;\\sigma) = \\sigma f(x) + c(x)^T\\lambda  + \\sum_{i=1}^n z_i^L(x_i-l_i) + \\sum_{i=1}^nz_i^U(u_i-x_i), \\end{align*}  where $\\sigma$ is a scaling parameter included for computational reasons.  Optimization problems are represented by an instance/subtype of  AbstractNLPModel . Such instances are composed of   an instance of  NLPModelMeta , which provides information about the problem,   including the number of variables, constraints, bounds on the variables, etc.  other data specific to the provenance of the problem.", 
            "title": "NLPModels.jl documentation"
        }, 
        {
            "location": "/#internal-interfaces", 
            "text": "SimpleNLPModel : Uses     ForwardDiff  to compute the    derivatives. It has a very simple interface.  JuMPNLPModel : Uses a  JuMP  model.  SlackModel : Creates an equality constrained problem with bounds     on the variables using an existing NLPModel.", 
            "title": "Internal Interfaces"
        }, 
        {
            "location": "/#external-interfaces", 
            "text": "AmplModel : Defined in     AmplNLReader.jl     for problems modeled using  AMPL  CUTEstModel : Defined in     CUTEst.jl  for    problems from  CUTEst .   If you want your interface here, open a PR.", 
            "title": "External Interfaces"
        }, 
        {
            "location": "/models/", 
            "text": "Models\n\n\nThere are currently three models implemented in this package, besides the external ones.\n\n\n\n\nSimpleNLPModel\n\n\n#\n\n\nNLPModels.SimpleNLPModel\n \n \nType\n.\n\n\nSimpleNLPModel is an AbstractNLPModel using ForwardDiff to computer the derivatives. In this interface, the objective function $f$ and an initial estimate are required. If there are constraints, the function $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$  and the vectors $c_L$ and $c_U$ also need to be passed. Bounds on the variables and an inital estimate to the Lagrangian multipliers can also be provided.\n\n\nSimpleNLPModel(f, x0; lvar = [-\u221e,\u2026,-\u221e], uvar = [\u221e,\u2026,\u221e], y0=zeros,\n  c = NotImplemented, lcon = [-\u221e,\u2026,-\u221e], ucon = [\u221e,\u2026,\u221e], name = \nGeneric\n)\n\n\n\n\n\n\nf :: Function\n - The objective function $f$;\n\n\nx0 :: Vector\n - The initial point of the problem;\n\n\nlvar :: Vector\n - $\\ell$, the lower bound of the variables;\n\n\nuvar :: Vector\n - $u$, the upper bound of the variables;\n\n\nc :: Function\n - The constraints function $c$;\n\n\ny0 :: Vector\n - The initial value of the Lagrangian estimates;\n\n\nlcon :: Vector\n - $c_L$, the lower bounds of the constraints function;\n\n\nucon :: Vector\n - $c_U$, the upper bounds of the constraints function;\n\n\nname :: AbstractString\n - A name for the model.\n\n\n\n\nThe functions follow the same restrictions of ForwardDiff functions, summarised here:\n\n\n\n\nThe function can only be composed of generic Julia functions;\n\n\nThe function must accept only one argument;\n\n\nThe function's argument must accept a subtype of Vector;\n\n\nThe function should be type-stable.\n\n\n\n\nFor contrained problems, the function $c$ is required, and it must return an array even when m = 1, and $c_L$ and $c_U$ should be passed, otherwise the problem is ill-formed. For equality constraints, the corresponding index of $c_L$ and $c_U$ should be the same.\n\n\n\n\nExample\n\n\nusing NLPModels\nf(x) = sum(x.^4)\nx = [1.0; 0.5; 0.25; 0.125]\nnlp = SimpleNLPModel(f, x)\ngrad(nlp, x)\n\n\n\n\n4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125\n\n\n\n\n\n\nList of implemented functions\n\n\ncons\n, \ncons!\n, \ngrad\n, \ngrad!\n, \nhess\n, \nhess_coord\n, \nhprod\n, \nhprod!\n, \njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n, \nobj\n\n\n\n\nJuMPNLPModel\n\n\n#\n\n\nNLPModels.JuMPNLPModel\n \n \nType\n.\n\n\nConstruct a \nJuMPNLPModel\n from a JuMP \nModel\n.\n\n\n\n\nExample\n\n\nusing NLPModels, JuMP\nm = Model()\n@variable(m, x[1:4])\n@NLobjective(m, Min, sum{x[i]^4, i=1:4})\nnlp = JuMPNLPModel(m)\nx0 = [1.0; 0.5; 0.25; 0.125]\ngrad(nlp, x0)\n\n\n\n\n4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125\n\n\n\n\n\n\nList of implemented functions\n\n\nNLPtoMPB\n, \ncons\n, \ncons!\n, \ngrad\n, \ngrad!\n, \nhess\n, \nhess_coord\n, \nhprod\n, \nhprod!\n, \njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n, \nobj\n\n\n\n\nSlackModel\n\n\n#\n\n\nNLPModels.SlackModel\n \n \nType\n.\n\n\nA model whose only inequality constraints are bounds.\n\n\nGiven a model, this type represents a second model in which slack variables are introduced so as to convert linear and nonlinear inequality constraints to equality constraints and bounds. More precisely, if the original model has the form\n\n\n\n\n \\min f(x)  \\mbox{ s. t. }  c_L \\leq c(x) \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u, \n\n\n\n\nthe new model appears to the user as\n\n\n\n\n \\min f(X)  \\mbox{ s. t. }  g(X) = 0 \\mbox{ and } L \\leq X \\leq U. \n\n\n\n\nThe unknowns $X = (x, s)$ contain the original variables and slack variables $s$. The latter are such that the new model has the general form\n\n\n\n\n \\min f(x)  \\mbox{ s. t. }  c(x) - s = 0, c_L \\leq s \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u, \n\n\n\n\nalthough no slack variables are introduced for equality constraints.\n\n\nThe slack variables are implicitly ordered as [s(low), s(upp), s(rng)], where \nlow\n, \nupp\n and \nrng\n represent the indices of the constraints of the form $c_L \\leq c(x) \n \\infty$, $-\\infty \n c(x) \\leq c_U$ and $c_L \\leq c(x) \\leq c_U$, respectively.\n\n\n\n\nExample\n\n\nusing NLPModels\nf(x) = x[1]^2 + 4x[2]^2\nc(x) = [x[1]*x[2] - 1]\nx = [2.0; 2.0]\nnlp = SimpleNLPModel(f, x, c=c, lcon=[0.0])\nnlp_slack = SlackModel(nlp)\nnlp_slack.meta.lvar\n\n\n\n\n3-element Array{Float64,1}:\n -Inf\n -Inf\n    0.0\n\n\n\n\n\n\nList of implemented functions\n\n\ncons\n, \ncons!\n, \ngrad\n, \ngrad!\n, \nhess\n, \nhess_coord\n, \nhprod\n, \nhprod!\n, \njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n, \nobj\n, \nreset!", 
            "title": "Models"
        }, 
        {
            "location": "/models/#models", 
            "text": "There are currently three models implemented in this package, besides the external ones.", 
            "title": "Models"
        }, 
        {
            "location": "/models/#simplenlpmodel", 
            "text": "#  NLPModels.SimpleNLPModel     Type .  SimpleNLPModel is an AbstractNLPModel using ForwardDiff to computer the derivatives. In this interface, the objective function $f$ and an initial estimate are required. If there are constraints, the function $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$  and the vectors $c_L$ and $c_U$ also need to be passed. Bounds on the variables and an inital estimate to the Lagrangian multipliers can also be provided.  SimpleNLPModel(f, x0; lvar = [-\u221e,\u2026,-\u221e], uvar = [\u221e,\u2026,\u221e], y0=zeros,\n  c = NotImplemented, lcon = [-\u221e,\u2026,-\u221e], ucon = [\u221e,\u2026,\u221e], name =  Generic )   f :: Function  - The objective function $f$;  x0 :: Vector  - The initial point of the problem;  lvar :: Vector  - $\\ell$, the lower bound of the variables;  uvar :: Vector  - $u$, the upper bound of the variables;  c :: Function  - The constraints function $c$;  y0 :: Vector  - The initial value of the Lagrangian estimates;  lcon :: Vector  - $c_L$, the lower bounds of the constraints function;  ucon :: Vector  - $c_U$, the upper bounds of the constraints function;  name :: AbstractString  - A name for the model.   The functions follow the same restrictions of ForwardDiff functions, summarised here:   The function can only be composed of generic Julia functions;  The function must accept only one argument;  The function's argument must accept a subtype of Vector;  The function should be type-stable.   For contrained problems, the function $c$ is required, and it must return an array even when m = 1, and $c_L$ and $c_U$ should be passed, otherwise the problem is ill-formed. For equality constraints, the corresponding index of $c_L$ and $c_U$ should be the same.", 
            "title": "SimpleNLPModel"
        }, 
        {
            "location": "/models/#example", 
            "text": "using NLPModels\nf(x) = sum(x.^4)\nx = [1.0; 0.5; 0.25; 0.125]\nnlp = SimpleNLPModel(f, x)\ngrad(nlp, x)  4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125", 
            "title": "Example"
        }, 
        {
            "location": "/models/#list-of-implemented-functions", 
            "text": "cons ,  cons! ,  grad ,  grad! ,  hess ,  hess_coord ,  hprod ,  hprod! ,  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod! ,  obj", 
            "title": "List of implemented functions"
        }, 
        {
            "location": "/models/#jumpnlpmodel", 
            "text": "#  NLPModels.JuMPNLPModel     Type .  Construct a  JuMPNLPModel  from a JuMP  Model .", 
            "title": "JuMPNLPModel"
        }, 
        {
            "location": "/models/#example_1", 
            "text": "using NLPModels, JuMP\nm = Model()\n@variable(m, x[1:4])\n@NLobjective(m, Min, sum{x[i]^4, i=1:4})\nnlp = JuMPNLPModel(m)\nx0 = [1.0; 0.5; 0.25; 0.125]\ngrad(nlp, x0)  4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125", 
            "title": "Example"
        }, 
        {
            "location": "/models/#list-of-implemented-functions_1", 
            "text": "NLPtoMPB ,  cons ,  cons! ,  grad ,  grad! ,  hess ,  hess_coord ,  hprod ,  hprod! ,  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod! ,  obj", 
            "title": "List of implemented functions"
        }, 
        {
            "location": "/models/#slackmodel", 
            "text": "#  NLPModels.SlackModel     Type .  A model whose only inequality constraints are bounds.  Given a model, this type represents a second model in which slack variables are introduced so as to convert linear and nonlinear inequality constraints to equality constraints and bounds. More precisely, if the original model has the form    \\min f(x)  \\mbox{ s. t. }  c_L \\leq c(x) \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u,    the new model appears to the user as    \\min f(X)  \\mbox{ s. t. }  g(X) = 0 \\mbox{ and } L \\leq X \\leq U.    The unknowns $X = (x, s)$ contain the original variables and slack variables $s$. The latter are such that the new model has the general form    \\min f(x)  \\mbox{ s. t. }  c(x) - s = 0, c_L \\leq s \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u,    although no slack variables are introduced for equality constraints.  The slack variables are implicitly ordered as [s(low), s(upp), s(rng)], where  low ,  upp  and  rng  represent the indices of the constraints of the form $c_L \\leq c(x)   \\infty$, $-\\infty   c(x) \\leq c_U$ and $c_L \\leq c(x) \\leq c_U$, respectively.", 
            "title": "SlackModel"
        }, 
        {
            "location": "/models/#example_2", 
            "text": "using NLPModels\nf(x) = x[1]^2 + 4x[2]^2\nc(x) = [x[1]*x[2] - 1]\nx = [2.0; 2.0]\nnlp = SimpleNLPModel(f, x, c=c, lcon=[0.0])\nnlp_slack = SlackModel(nlp)\nnlp_slack.meta.lvar  3-element Array{Float64,1}:\n -Inf\n -Inf\n    0.0", 
            "title": "Example"
        }, 
        {
            "location": "/models/#list-of-implemented-functions_2", 
            "text": "cons ,  cons! ,  grad ,  grad! ,  hess ,  hess_coord ,  hprod ,  hprod! ,  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod! ,  obj ,  reset!", 
            "title": "List of implemented functions"
        }, 
        {
            "location": "/api/", 
            "text": "API\n\n\nobj\n\n\nobj(nlp, x)\n\n\nEvaluate $f(x)$, the objective function of \nnlp\n at \nx\n.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\ngrad\n\n\ngrad(nlp, x)\n\n\nEvaluate $\\nabla f(x)$, the gradient of the objective function at \nx\n.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\ngrad!\n\n\ngrad!(nlp, x, g)\n\n\nEvaluate $\\nabla f(x)$, the gradient of the objective function at \nx\n in place.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\ncons\n\n\ncons(nlp, x)\n\n\nEvaluate $c(x)$, the constraints at \nx\n.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\ncons!\n\n\ncons!(nlp, x, c)\n\n\nEvaluate $c(x)$, the constraints at \nx\n in place.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\njac_coord\n\n\n(rows,cols,vals) = jac_coord(nlp, x)\n\n\nEvaluate $\\nabla c(x)$, the constraint's Jacobian at \nx\n in sparse coordinate format.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\njac\n\n\nJx = jac(nlp, x)\n\n\nEvaluate $\\nabla c(x)$, the constraint's Jacobian at \nx\n as a sparse matrix.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\njprod\n\n\nJv = jprod(nlp, x, v)\n\n\nEvaluate $\\nabla c(x)v$, the Jacobian-vector product at \nx\n.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\njprod!\n\n\nJv = jprod!(nlp, x, v, Jv)\n\n\nEvaluate $\\nabla c(x)v$, the Jacobian-vector product at \nx\n in place.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\njtprod\n\n\nJtv = jtprod(nlp, x, v, Jtv)\n\n\nEvaluate $\\nabla c(x)^Tv$, the transposed-Jacobian-vector product at \nx\n.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\njtprod!\n\n\nJtv = jtprod!(nlp, x, v, Jtv)\n\n\nEvaluate $\\nabla c(x)^Tv$, the transposed-Jacobian-vector product at \nx\n in place.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\nhess_coord\n\n\n(rows,cols,vals) = hess_coord(nlp, x; obj_weight=1.0, y=zeros)\n\n\nEvaluate the Lagrangian Hessian at \n(x,y)\n in sparse coordinate format, with objective function scaled by \nobj_weight\n, i.e.,\n\n\n\n\n \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x), \n\n\n\n\nwith \u03c3 = obj_weight. Only the lower triangle is returned.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\nhess\n\n\nHx = hess(nlp, x; obj_weight=1.0, y=zeros)\n\n\nEvaluate the Lagrangian Hessian at \n(x,y)\n as a sparse matrix, with objective function scaled by \nobj_weight\n, i.e.,\n\n\n\n\n \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x), \n\n\n\n\nwith \u03c3 = obj_weight. Only the lower triangle is returned.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\nhprod\n\n\nHv = hprod(nlp, x, v; obj_weight=1.0, y=zeros)\n\n\nEvaluate the product of the Lagrangian Hessian at \n(x,y)\n with the vector \nv\n, with objective function scaled by \nobj_weight\n, i.e.,\n\n\n\n\n \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x), \n\n\n\n\nwith \u03c3 = obj_weight.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\nhprod!\n\n\nHv = hprod!(nlp, x, v, Hv; obj_weight=1.0, y=zeros)\n\n\nEvaluate the product of the Lagrangian Hessian at \n(x,y)\n with the vector \nv\n in place, with objective function scaled by \nobj_weight\n, i.e.,\n\n\n\n\n \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x), \n\n\n\n\nwith \u03c3 = obj_weight.\n\n\nImplemented by \nSimpleNLPModel\n, \nJuMPNLPModel\n, \nSlackModel\n\n\nNLPtoMPB\n\n\nmp = NLPtoMPB(nlp, solver)\n\n\nReturn a \nMathProgBase\n model corresponding to this model. \nsolver\n should be a solver instance, e.g., \nIpoptSolver()\n. Currently, all models are treated as nonlinear models.\n\n\nImplemented by \nJuMPNLPModel\n\n\nreset!\n\n\n`reset!(nlp)\n\n\nReset evaluation count in \nnlp\n\n\nreset!(counters)\n\n\nReset evaluation counters\n\n\nImplemented by \nSlackModel", 
            "title": "API"
        }, 
        {
            "location": "/api/#api", 
            "text": "", 
            "title": "API"
        }, 
        {
            "location": "/api/#obj", 
            "text": "obj(nlp, x)  Evaluate $f(x)$, the objective function of  nlp  at  x .  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "obj"
        }, 
        {
            "location": "/api/#grad", 
            "text": "grad(nlp, x)  Evaluate $\\nabla f(x)$, the gradient of the objective function at  x .  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "grad"
        }, 
        {
            "location": "/api/#grad_1", 
            "text": "grad!(nlp, x, g)  Evaluate $\\nabla f(x)$, the gradient of the objective function at  x  in place.  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "grad!"
        }, 
        {
            "location": "/api/#cons", 
            "text": "cons(nlp, x)  Evaluate $c(x)$, the constraints at  x .  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "cons"
        }, 
        {
            "location": "/api/#cons_1", 
            "text": "cons!(nlp, x, c)  Evaluate $c(x)$, the constraints at  x  in place.  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "cons!"
        }, 
        {
            "location": "/api/#jac_coord", 
            "text": "(rows,cols,vals) = jac_coord(nlp, x)  Evaluate $\\nabla c(x)$, the constraint's Jacobian at  x  in sparse coordinate format.  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "jac_coord"
        }, 
        {
            "location": "/api/#jac", 
            "text": "Jx = jac(nlp, x)  Evaluate $\\nabla c(x)$, the constraint's Jacobian at  x  as a sparse matrix.  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "jac"
        }, 
        {
            "location": "/api/#jprod", 
            "text": "Jv = jprod(nlp, x, v)  Evaluate $\\nabla c(x)v$, the Jacobian-vector product at  x .  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "jprod"
        }, 
        {
            "location": "/api/#jprod_1", 
            "text": "Jv = jprod!(nlp, x, v, Jv)  Evaluate $\\nabla c(x)v$, the Jacobian-vector product at  x  in place.  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "jprod!"
        }, 
        {
            "location": "/api/#jtprod", 
            "text": "Jtv = jtprod(nlp, x, v, Jtv)  Evaluate $\\nabla c(x)^Tv$, the transposed-Jacobian-vector product at  x .  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "jtprod"
        }, 
        {
            "location": "/api/#jtprod_1", 
            "text": "Jtv = jtprod!(nlp, x, v, Jtv)  Evaluate $\\nabla c(x)^Tv$, the transposed-Jacobian-vector product at  x  in place.  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "jtprod!"
        }, 
        {
            "location": "/api/#hess_coord", 
            "text": "(rows,cols,vals) = hess_coord(nlp, x; obj_weight=1.0, y=zeros)  Evaluate the Lagrangian Hessian at  (x,y)  in sparse coordinate format, with objective function scaled by  obj_weight , i.e.,    \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x),    with \u03c3 = obj_weight. Only the lower triangle is returned.  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "hess_coord"
        }, 
        {
            "location": "/api/#hess", 
            "text": "Hx = hess(nlp, x; obj_weight=1.0, y=zeros)  Evaluate the Lagrangian Hessian at  (x,y)  as a sparse matrix, with objective function scaled by  obj_weight , i.e.,    \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x),    with \u03c3 = obj_weight. Only the lower triangle is returned.  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "hess"
        }, 
        {
            "location": "/api/#hprod", 
            "text": "Hv = hprod(nlp, x, v; obj_weight=1.0, y=zeros)  Evaluate the product of the Lagrangian Hessian at  (x,y)  with the vector  v , with objective function scaled by  obj_weight , i.e.,    \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x),    with \u03c3 = obj_weight.  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "hprod"
        }, 
        {
            "location": "/api/#hprod_1", 
            "text": "Hv = hprod!(nlp, x, v, Hv; obj_weight=1.0, y=zeros)  Evaluate the product of the Lagrangian Hessian at  (x,y)  with the vector  v  in place, with objective function scaled by  obj_weight , i.e.,    \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x),    with \u03c3 = obj_weight.  Implemented by  SimpleNLPModel ,  JuMPNLPModel ,  SlackModel", 
            "title": "hprod!"
        }, 
        {
            "location": "/api/#nlptompb", 
            "text": "mp = NLPtoMPB(nlp, solver)  Return a  MathProgBase  model corresponding to this model.  solver  should be a solver instance, e.g.,  IpoptSolver() . Currently, all models are treated as nonlinear models.  Implemented by  JuMPNLPModel", 
            "title": "NLPtoMPB"
        }, 
        {
            "location": "/api/#reset", 
            "text": "`reset!(nlp)  Reset evaluation count in  nlp  reset!(counters)  Reset evaluation counters  Implemented by  SlackModel", 
            "title": "reset!"
        }
    ]
}