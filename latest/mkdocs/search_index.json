{
    "docs": [
        {
            "location": "/", 
            "text": "NLPModels.jl documentation\n\n\nThis package provides general guidelines to represent optimization problems in Julia and a standardized API to evaluate the functions and their derivatives. The main objective is to be able to rely on that \nAPI\n when designing optimization solvers in Julia.\n\n\n\n\nIntroduction\n\n\nThe general form of the optimization problem is \n\\begin{align*} \\min \\quad & f(x) \\\\\n& c_i(x) = 0, \\quad i \\in E, \\\\\n& c_{L_i} \\leq c_i(x) \\leq c_{U_i}, \\quad i \\in I, \\\\\n& \\ell \\leq x \\leq u, \\end{align*}\n where $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$, $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$, $E\\cup I = \\{1,2,\\dots,m\\}$, $E\\cap I = \\emptyset$, and $c_{L_i}, c_{U_i}, \\ell_j, u_j \\in \\mathbb{R}\\cup\\{\\pm\\infty\\}$ for $i = 1,\\dots,m$ and $j = 1,\\dots,n$.\n\n\nFor computational reasons, we write \n\\begin{align*} \\min \\quad & f(x) \\\\\n& c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u, \\end{align*}\n defining $c_{L_i} = c_{U_i}$ for all $i \\in E$. The Lagrangian of this problem is defined as \n\\begin{align*} L(x,\\lambda,z^L,z^U;\\sigma) = \\sigma f(x) + c(x)^T\\lambda  + \\sum_{i=1}^n z_i^L(x_i-l_i) + \\sum_{i=1}^nz_i^U(u_i-x_i), \\end{align*}\n where $\\sigma$ is a scaling parameter included for computational reasons. Notice that, for the Hessian, the variables $z^L$ and $z^U$ are not used.\n\n\nOptimization problems are represented by an instance/subtype of \nAbstractNLPModel\n. Such instances are composed of\n\n\n\n\nan instance of \nNLPModelMeta\n, which provides information about the problem,   including the number of variables, constraints, bounds on the variables, etc.\n\n\nother data specific to the provenance of the problem.\n\n\n\n\n\n\nInstall\n\n\nThe current usable version of NLPModels.jl is in the development branch. Install with the following commands.\n\n\nPkg\n.\nclone\n(\nhttps://github.com/JuliaSmoothOptimizers/NLPModels.jl\n)\n\n\nPkg\n.\nbuild\n(\nNLPModels\n)\n\n\n\n\n\n\nIf you want the \nADNLPModel\n or the \nMathProgNLPModel\n, you also need the\n\n\nPkg\n.\nadd\n(\nForwardDiff\n)\n\n\nPkg\n.\nadd\n(\nMathProgBase\n)\n\n\n\n\n\n\nrespectively. In addition, if you want to create a \nMathProgNLPModel\n from a \nJuMP\n model, you'll need\n\n\nPkg\n.\nadd\n(\nJuMP\n)\n\n\n\n\n\n\n\n\nUsage\n\n\nSee the \nModels\n, or the \nTutorial\n, or the \nAPI\n.\n\n\n\n\nInternal Interfaces\n\n\n\n\nADNLPModel\n: Uses    \nForwardDiff\n to compute the    derivatives. It has a very simple interface, though it isn't very efficient    for larger problems.\n\n\nMathProgNLPModel\n: Uses a \nMathProgModel\n, derived from a    \nAbstractMathProgModel\n model.    For instance, \nJuMP.jl\n models can be    used.\n\n\nSimpleNLPModel\n: Only uses user defined functions.\n\n\nSlackModel\n: Creates an equality constrained problem with bounds     on the variables using an existing NLPModel.\n\n\n\n\n\n\nExternal Interfaces\n\n\n\n\nAmplModel\n: Defined in    \nAmplNLReader.jl\n    for problems modeled using \nAMPL\n\n\nCUTEstModel\n: Defined in    \nCUTEst.jl\n for    problems from \nCUTEst\n.\n\n\n\n\nIf you want your interface here, open a PR.\n\n\n\n\nContents\n\n\n\n\nAPI\n\n\nReference guide\n\n\nAbstractNLPModel functions\n\n\nDerivative check\n\n\n\n\n\n\nIndex\n\n\nModels\n\n\nADNLPModel\n\n\nMathProgNLPModel\n\n\nSimpleNLPModel\n\n\nSlackModel\n\n\n\n\n\n\nNLPModels.jl documentation\n\n\nIntroduction\n\n\nInstall\n\n\nUsage\n\n\nInternal Interfaces\n\n\nExternal Interfaces\n\n\nContents\n\n\n\n\n\n\nTutorial\n\n\nADNLPModel Tutorial\n\n\nSimpleNLPModel Tutorial", 
            "title": "Home"
        }, 
        {
            "location": "/#nlpmodelsjl-documentation", 
            "text": "This package provides general guidelines to represent optimization problems in Julia and a standardized API to evaluate the functions and their derivatives. The main objective is to be able to rely on that  API  when designing optimization solvers in Julia.", 
            "title": "NLPModels.jl documentation"
        }, 
        {
            "location": "/#introduction", 
            "text": "The general form of the optimization problem is  \\begin{align*} \\min \\quad & f(x) \\\\\n& c_i(x) = 0, \\quad i \\in E, \\\\\n& c_{L_i} \\leq c_i(x) \\leq c_{U_i}, \\quad i \\in I, \\\\\n& \\ell \\leq x \\leq u, \\end{align*}  where $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$, $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$, $E\\cup I = \\{1,2,\\dots,m\\}$, $E\\cap I = \\emptyset$, and $c_{L_i}, c_{U_i}, \\ell_j, u_j \\in \\mathbb{R}\\cup\\{\\pm\\infty\\}$ for $i = 1,\\dots,m$ and $j = 1,\\dots,n$.  For computational reasons, we write  \\begin{align*} \\min \\quad & f(x) \\\\\n& c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u, \\end{align*}  defining $c_{L_i} = c_{U_i}$ for all $i \\in E$. The Lagrangian of this problem is defined as  \\begin{align*} L(x,\\lambda,z^L,z^U;\\sigma) = \\sigma f(x) + c(x)^T\\lambda  + \\sum_{i=1}^n z_i^L(x_i-l_i) + \\sum_{i=1}^nz_i^U(u_i-x_i), \\end{align*}  where $\\sigma$ is a scaling parameter included for computational reasons. Notice that, for the Hessian, the variables $z^L$ and $z^U$ are not used.  Optimization problems are represented by an instance/subtype of  AbstractNLPModel . Such instances are composed of   an instance of  NLPModelMeta , which provides information about the problem,   including the number of variables, constraints, bounds on the variables, etc.  other data specific to the provenance of the problem.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#install", 
            "text": "The current usable version of NLPModels.jl is in the development branch. Install with the following commands.  Pkg . clone ( https://github.com/JuliaSmoothOptimizers/NLPModels.jl )  Pkg . build ( NLPModels )   If you want the  ADNLPModel  or the  MathProgNLPModel , you also need the  Pkg . add ( ForwardDiff )  Pkg . add ( MathProgBase )   respectively. In addition, if you want to create a  MathProgNLPModel  from a  JuMP  model, you'll need  Pkg . add ( JuMP )", 
            "title": "Install"
        }, 
        {
            "location": "/#usage", 
            "text": "See the  Models , or the  Tutorial , or the  API .", 
            "title": "Usage"
        }, 
        {
            "location": "/#internal-interfaces", 
            "text": "ADNLPModel : Uses     ForwardDiff  to compute the    derivatives. It has a very simple interface, though it isn't very efficient    for larger problems.  MathProgNLPModel : Uses a  MathProgModel , derived from a     AbstractMathProgModel  model.    For instance,  JuMP.jl  models can be    used.  SimpleNLPModel : Only uses user defined functions.  SlackModel : Creates an equality constrained problem with bounds     on the variables using an existing NLPModel.", 
            "title": "Internal Interfaces"
        }, 
        {
            "location": "/#external-interfaces", 
            "text": "AmplModel : Defined in     AmplNLReader.jl     for problems modeled using  AMPL  CUTEstModel : Defined in     CUTEst.jl  for    problems from  CUTEst .   If you want your interface here, open a PR.", 
            "title": "External Interfaces"
        }, 
        {
            "location": "/#contents", 
            "text": "API  Reference guide  AbstractNLPModel functions  Derivative check    Index  Models  ADNLPModel  MathProgNLPModel  SimpleNLPModel  SlackModel    NLPModels.jl documentation  Introduction  Install  Usage  Internal Interfaces  External Interfaces  Contents    Tutorial  ADNLPModel Tutorial  SimpleNLPModel Tutorial", 
            "title": "Contents"
        }, 
        {
            "location": "/models/", 
            "text": "Models\n\n\nThere are currently three models implemented in this package, besides the external ones.\n\n\n\n\nADNLPModel\n\n\nNLPModels.ADNLPModel\n\n\n\n\n\n\n\nExample\n\n\n```@example\nusing NLPModels\nf(x) = sum(x.^4)\nx = [1.0; 0.5; 0.25; 0.125]\nnlp = ADNLPModel(f, x)\ngrad(nlp, x)\n\n\na id=\nList-of-implemented-functions-1\n/a\n\n\n### List of implemented functions\n\n[cons](/api/#NLPModels.cons), [cons!](/api/#NLPModels.cons!), [grad](/api/#NLPModels.grad), [grad!](/api/#NLPModels.grad!), [hess](/api/#NLPModels.hess), [hess_coord](/api/#NLPModels.hess_coord), [hprod](/api/#NLPModels.hprod), [hprod!](/api/#NLPModels.hprod!), [jac](/api/#NLPModels.jac), [jac_coord](/api/#NLPModels.jac_coord), [jprod](/api/#NLPModels.jprod), [jprod!](/api/#NLPModels.jprod!), [jtprod](/api/#NLPModels.jtprod), [jtprod!](/api/#NLPModels.jtprod!), [obj](/api/#NLPModels.obj)\n\na id=\nMathProgNLPModel-1\n/a\n\n\n## MathProgNLPModel\n\n\n\n\n\nNLPModels.MathProgNLPModel\n\n\na id=\nExample-2\n/a\n\n\n### Example\n\n\n```@example\nusing NLPModels, MathProgBase, JuMP\nm = Model()\n@variable(m, x[1:4])\n@NLobjective(m, Min, sum{x[i]^4, i=1:4})\nnlp = MathProgNLPModel(m)\nx0 = [1.0; 0.5; 0.25; 0.125]\ngrad(nlp, x0)\n\n\n\n\n\n\n\nList of implemented functions\n\n\ncons\n, \ncons!\n, \ngrad\n, \ngrad!\n, \nhess\n, \nhess_coord\n, \nhprod\n, \nhprod!\n, \njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n, \nobj\n\n\n\n\nSimpleNLPModel\n\n\n#\n\n\nNLPModels.SimpleNLPModel\n \n \nType\n.\n\n\nSimpleNLPModel is an AbstractNLPModel that uses only user-defined functions. In this interface, the objective function $f$ and an initial estimate are required. If the user wants to use derivatives, they need to be passed. The same goes for the Hessian and Hessian-Vector product. For constraints, $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$  and the vectors $c_L$ and $c_U$ also need to be passed. Bounds on the variables and an inital estimate to the Lagrangian multipliers can also be provided. The user can also pass the Jacobian and the Lagrangian Hessian and Hessian-Vector product.\n\n\nSimpleNLPModel(f, x0; lvar = [-\u221e,\u2026,-\u221e], uvar = [\u221e,\u2026,\u221e], y0=zeros,\n  lcon = [-\u221e,\u2026,-\u221e], ucon = [\u221e,\u2026,\u221e], name = \nGeneric\n,\n  [list of functions])\n\n\n\n\n\n\n\nf :: Function\n - The objective function $f$;\n\n\nx0 :: Vector\n - The initial point of the problem;\n\n\nlvar :: Vector\n - $\\ell$, the lower bound of the variables;\n\n\nuvar :: Vector\n - $u$, the upper bound of the variables;\n\n\ny0 :: Vector\n - The initial value of the Lagrangian estimates;\n\n\nlcon :: Vector\n - $c_L$, the lower bounds of the constraints function;\n\n\nucon :: Vector\n - $c_U$, the upper bounds of the constraints function;\n\n\nname :: AbstractString\n - A name for the model.\n\n\n\n\nAll functions passed have a direct correlation with a NLP function. You don't have to define any more than you need, but calling an undefined function will throw a \nNotImplementedError\n. The list is\n\n\n\n\ng\n and \ng!\n: $\\nabla f(x)$, the gradient of the objective function;     see \ngrad\n.\n\n\n\n\ngx = g(x)\ngx = g!(x, gx)\n\n\n\n\n\n\n\nH\n: The lower triangle of the Hessian of the objective function or of the     Lagrangian;     see \nhess\n.\n\n\n\n\nHx = H(x; obj_weight=1.0) # if the problem is unconstrained\nHx = H(x; obj_weight=1.0, y=zeros) # if the problem is constrained\n\n\n\n\n\n\n\nHcoord\n - The lower triangle of the Hessian of the objective function     or of the Lagrangian, in triplet format;     see \nhess_coord\n.\n\n\n\n\n(rows,cols,vals) = Hcoord(x; obj_weight=1.0) # if the problem is unconstrained\n(rows,cols,vals) = Hcoord(x; obj_weight=1.0, y=zeros) # if the problem is constrained\n\n\n\n\n\n\n\nHp\n and \nHp!\n - The product of the Hessian of the objective function or of     the Lagrangian by a vector;     see \nhprod\n.\n\n\n\n\nHv = Hp(x, v, obj_weight=1.0) # if the problem is unconstrained\nHv = Hp!(x, v, Hv, obj_weight=1.0) # if the problem is unconstrained\nHv = Hp(x, v, obj_weight=1.0, y=zeros) # if the problem is constrained\nHv = Hp!(x, v, Hv, obj_weight=1.0, y=zeros) # if the problem is constrained\n\n\n\n\n\n\n\nc\n and \nc!\n - $c(x)$, the constraints function;     see \ncons\n.\n\n\n\n\ncx = c(x)\ncx = c!(x, cx)\n\n\n\n\n\n\n\nJ\n - $J(x)$, the Jacobian of the constraints;     see \njac\n.\n\n\n\n\nJx = J(x)\n\n\n\n\n\n\n\nJcoord\n - $J(x)$, the Jacobian of the constraints, in triplet format;     see \njac_coord\n.\n\n\n\n\n(rows,cols,vals) = Jcoord(x)\n\n\n\n\n\n\n\nJp\n and \nJp!\n - The Jacobian-vector product;     see \njprod\n.\n\n\n\n\nJv = Jp(x, v)\nJv = Jp!(x, v, Jv)\n\n\n\n\n\n\n\nJtp\n and \nJtp!\n - The Jacobian-transposed-vector product;     see \njtprod\n.\n\n\n\n\nJtv = Jtp(x, v)\nJtv = Jtp!(x, v, Jtv)\n\n\n\n\n\nFor contrained problems, the function $c$ is required, and it must return an array even when m = 1, and $c_L$ and $c_U$ should be passed, otherwise the problem is ill-formed. For equality constraints, the corresponding index of $c_L$ and $c_U$ should be the same.\n\n\n\n\nExample\n\n\nusing\n \nNLPModels\n\n\nf\n(\nx\n)\n \n=\n \nsum\n(\nx\n.^\n4\n)\n\n\ng\n(\nx\n)\n \n=\n \n4\n*\nx\n.^\n3\n\n\nx\n \n=\n \n[\n1.0\n;\n \n0.5\n;\n \n0.25\n;\n \n0.125\n]\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx\n,\n \ng\n=\ng\n)\n\n\ngrad\n(\nnlp\n,\n \nx\n)\n\n\n\n\n\n\n4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125\n\n\n\n\n\n\n\nList of implemented functions\n\n\ncons\n, \ncons!\n, \ngrad\n, \ngrad!\n, \nhess\n, \nhess_coord\n, \nhprod\n, \nhprod!\n, \njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n, \nobj\n\n\n\n\nSlackModel\n\n\n#\n\n\nNLPModels.SlackModel\n \n \nType\n.\n\n\nA model whose only inequality constraints are bounds.\n\n\nGiven a model, this type represents a second model in which slack variables are introduced so as to convert linear and nonlinear inequality constraints to equality constraints and bounds. More precisely, if the original model has the form\n\n\n\n\n \\min f(x)  \\mbox{ s. t. }  c_L \\leq c(x) \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u, \n\n\n\n\nthe new model appears to the user as\n\n\n\n\n \\min f(X)  \\mbox{ s. t. }  g(X) = 0 \\mbox{ and } L \\leq X \\leq U. \n\n\n\n\nThe unknowns $X = (x, s)$ contain the original variables and slack variables $s$. The latter are such that the new model has the general form\n\n\n\n\n \\min f(x)  \\mbox{ s. t. }  c(x) - s = 0, c_L \\leq s \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u, \n\n\n\n\nalthough no slack variables are introduced for equality constraints.\n\n\nThe slack variables are implicitly ordered as [s(low), s(upp), s(rng)], where \nlow\n, \nupp\n and \nrng\n represent the indices of the constraints of the form $c_L \\leq c(x) \n \\infty$, $-\\infty \n c(x) \\leq c_U$ and $c_L \\leq c(x) \\leq c_U$, respectively.\n\n\n\n\nExample\n\n\n@example\nusing NLPModels\nf(x) = x[1]^2 + 4x[2]^2\nc(x) = [x[1]*x[2] - 1]\nx = [2.0; 2.0]\nnlp = ADNLPModel(f, x, c=c, lcon=[0.0])\nnlp_slack = SlackModel(nlp)\nnlp_slack.meta.lvar\n\n\n\n\nList of implemented functions\n\n\ncons\n, \ncons!\n, \ngrad\n, \ngrad!\n, \nhess\n, \nhess_coord\n, \nhprod\n, \nhprod!\n, \njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n, \nobj\n, \nreset!", 
            "title": "Models"
        }, 
        {
            "location": "/models/#models", 
            "text": "There are currently three models implemented in this package, besides the external ones.", 
            "title": "Models"
        }, 
        {
            "location": "/models/#adnlpmodel", 
            "text": "NLPModels.ADNLPModel", 
            "title": "ADNLPModel"
        }, 
        {
            "location": "/models/#example", 
            "text": "```@example\nusing NLPModels\nf(x) = sum(x.^4)\nx = [1.0; 0.5; 0.25; 0.125]\nnlp = ADNLPModel(f, x)\ngrad(nlp, x)  a id= List-of-implemented-functions-1 /a \n\n### List of implemented functions\n\n[cons](/api/#NLPModels.cons), [cons!](/api/#NLPModels.cons!), [grad](/api/#NLPModels.grad), [grad!](/api/#NLPModels.grad!), [hess](/api/#NLPModels.hess), [hess_coord](/api/#NLPModels.hess_coord), [hprod](/api/#NLPModels.hprod), [hprod!](/api/#NLPModels.hprod!), [jac](/api/#NLPModels.jac), [jac_coord](/api/#NLPModels.jac_coord), [jprod](/api/#NLPModels.jprod), [jprod!](/api/#NLPModels.jprod!), [jtprod](/api/#NLPModels.jtprod), [jtprod!](/api/#NLPModels.jtprod!), [obj](/api/#NLPModels.obj) a id= MathProgNLPModel-1 /a \n\n## MathProgNLPModel  NLPModels.MathProgNLPModel  a id= Example-2 /a \n\n### Example\n\n\n```@example\nusing NLPModels, MathProgBase, JuMP\nm = Model()\n@variable(m, x[1:4])\n@NLobjective(m, Min, sum{x[i]^4, i=1:4})\nnlp = MathProgNLPModel(m)\nx0 = [1.0; 0.5; 0.25; 0.125]\ngrad(nlp, x0)", 
            "title": "Example"
        }, 
        {
            "location": "/models/#list-of-implemented-functions", 
            "text": "cons ,  cons! ,  grad ,  grad! ,  hess ,  hess_coord ,  hprod ,  hprod! ,  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod! ,  obj", 
            "title": "List of implemented functions"
        }, 
        {
            "location": "/models/#simplenlpmodel", 
            "text": "#  NLPModels.SimpleNLPModel     Type .  SimpleNLPModel is an AbstractNLPModel that uses only user-defined functions. In this interface, the objective function $f$ and an initial estimate are required. If the user wants to use derivatives, they need to be passed. The same goes for the Hessian and Hessian-Vector product. For constraints, $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$  and the vectors $c_L$ and $c_U$ also need to be passed. Bounds on the variables and an inital estimate to the Lagrangian multipliers can also be provided. The user can also pass the Jacobian and the Lagrangian Hessian and Hessian-Vector product.  SimpleNLPModel(f, x0; lvar = [-\u221e,\u2026,-\u221e], uvar = [\u221e,\u2026,\u221e], y0=zeros,\n  lcon = [-\u221e,\u2026,-\u221e], ucon = [\u221e,\u2026,\u221e], name =  Generic ,\n  [list of functions])   f :: Function  - The objective function $f$;  x0 :: Vector  - The initial point of the problem;  lvar :: Vector  - $\\ell$, the lower bound of the variables;  uvar :: Vector  - $u$, the upper bound of the variables;  y0 :: Vector  - The initial value of the Lagrangian estimates;  lcon :: Vector  - $c_L$, the lower bounds of the constraints function;  ucon :: Vector  - $c_U$, the upper bounds of the constraints function;  name :: AbstractString  - A name for the model.   All functions passed have a direct correlation with a NLP function. You don't have to define any more than you need, but calling an undefined function will throw a  NotImplementedError . The list is   g  and  g! : $\\nabla f(x)$, the gradient of the objective function;     see  grad .   gx = g(x)\ngx = g!(x, gx)   H : The lower triangle of the Hessian of the objective function or of the     Lagrangian;     see  hess .   Hx = H(x; obj_weight=1.0) # if the problem is unconstrained\nHx = H(x; obj_weight=1.0, y=zeros) # if the problem is constrained   Hcoord  - The lower triangle of the Hessian of the objective function     or of the Lagrangian, in triplet format;     see  hess_coord .   (rows,cols,vals) = Hcoord(x; obj_weight=1.0) # if the problem is unconstrained\n(rows,cols,vals) = Hcoord(x; obj_weight=1.0, y=zeros) # if the problem is constrained   Hp  and  Hp!  - The product of the Hessian of the objective function or of     the Lagrangian by a vector;     see  hprod .   Hv = Hp(x, v, obj_weight=1.0) # if the problem is unconstrained\nHv = Hp!(x, v, Hv, obj_weight=1.0) # if the problem is unconstrained\nHv = Hp(x, v, obj_weight=1.0, y=zeros) # if the problem is constrained\nHv = Hp!(x, v, Hv, obj_weight=1.0, y=zeros) # if the problem is constrained   c  and  c!  - $c(x)$, the constraints function;     see  cons .   cx = c(x)\ncx = c!(x, cx)   J  - $J(x)$, the Jacobian of the constraints;     see  jac .   Jx = J(x)   Jcoord  - $J(x)$, the Jacobian of the constraints, in triplet format;     see  jac_coord .   (rows,cols,vals) = Jcoord(x)   Jp  and  Jp!  - The Jacobian-vector product;     see  jprod .   Jv = Jp(x, v)\nJv = Jp!(x, v, Jv)   Jtp  and  Jtp!  - The Jacobian-transposed-vector product;     see  jtprod .   Jtv = Jtp(x, v)\nJtv = Jtp!(x, v, Jtv)  For contrained problems, the function $c$ is required, and it must return an array even when m = 1, and $c_L$ and $c_U$ should be passed, otherwise the problem is ill-formed. For equality constraints, the corresponding index of $c_L$ and $c_U$ should be the same.", 
            "title": "SimpleNLPModel"
        }, 
        {
            "location": "/models/#example_1", 
            "text": "using   NLPModels  f ( x )   =   sum ( x .^ 4 )  g ( x )   =   4 * x .^ 3  x   =   [ 1.0 ;   0.5 ;   0.25 ;   0.125 ]  nlp   =   SimpleNLPModel ( f ,   x ,   g = g )  grad ( nlp ,   x )   4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125", 
            "title": "Example"
        }, 
        {
            "location": "/models/#list-of-implemented-functions_1", 
            "text": "cons ,  cons! ,  grad ,  grad! ,  hess ,  hess_coord ,  hprod ,  hprod! ,  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod! ,  obj", 
            "title": "List of implemented functions"
        }, 
        {
            "location": "/models/#slackmodel", 
            "text": "#  NLPModels.SlackModel     Type .  A model whose only inequality constraints are bounds.  Given a model, this type represents a second model in which slack variables are introduced so as to convert linear and nonlinear inequality constraints to equality constraints and bounds. More precisely, if the original model has the form    \\min f(x)  \\mbox{ s. t. }  c_L \\leq c(x) \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u,    the new model appears to the user as    \\min f(X)  \\mbox{ s. t. }  g(X) = 0 \\mbox{ and } L \\leq X \\leq U.    The unknowns $X = (x, s)$ contain the original variables and slack variables $s$. The latter are such that the new model has the general form    \\min f(x)  \\mbox{ s. t. }  c(x) - s = 0, c_L \\leq s \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u,    although no slack variables are introduced for equality constraints.  The slack variables are implicitly ordered as [s(low), s(upp), s(rng)], where  low ,  upp  and  rng  represent the indices of the constraints of the form $c_L \\leq c(x)   \\infty$, $-\\infty   c(x) \\leq c_U$ and $c_L \\leq c(x) \\leq c_U$, respectively.", 
            "title": "SlackModel"
        }, 
        {
            "location": "/models/#example_2", 
            "text": "@example\nusing NLPModels\nf(x) = x[1]^2 + 4x[2]^2\nc(x) = [x[1]*x[2] - 1]\nx = [2.0; 2.0]\nnlp = ADNLPModel(f, x, c=c, lcon=[0.0])\nnlp_slack = SlackModel(nlp)\nnlp_slack.meta.lvar", 
            "title": "Example"
        }, 
        {
            "location": "/models/#list-of-implemented-functions_2", 
            "text": "cons ,  cons! ,  grad ,  grad! ,  hess ,  hess_coord ,  hprod ,  hprod! ,  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod! ,  obj ,  reset!", 
            "title": "List of implemented functions"
        }, 
        {
            "location": "/tutorial/", 
            "text": "Tutorial\n\n\nNLPModels.jl was created for two purposes:\n\n\n\n\nAllow users to access problem databases in an unified way.  Mainly, this means  \nCUTEst.jl\n,  but it also gives access to \nAMPL  problems\n,  as well as JuMP defined problems (e.g. as in  \nOptimizationProblems.jl\n).\n\n\nAllow users to create their own problems in the same way.  As a consequence, optimization methods designed according to the NLPModels API  will accept NLPModels of any provenance.  See, for instance,  \nOptimize.jl\n.\n\n\n\n\nThe main interfaces for user defined problems are\n\n\n\n\nADNLPModel\n, which defines a model easily, using automatic   differentiation.\n\n\nSimpleNLPModel\n, which allows users to handle all functions themselves,   giving\n\n\n\n\n\n\nADNLPModel Tutorial\n\n\nADNLPModel is simple to use and is useful for classrooms. It only needs the objective function $f$ and a starting point $x^0$ to be well-defined. For constrained problems, you'll also need the constraints function $c$, and the constraints vectors $c_L$ and $c_U$, such that $c_L \\leq c(x) \\leq c_U$. Equality constraints will be automatically identified as those indices $i$ for which $c_{L_i} = c_{U_i}$.\n\n\nLet's define the famous Rosenbrock function \n\\begin{align*} f(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2, \\end{align*}\n with starting point $x^0 = (-1.2,1.0)$.\n\n\n```@example adnlp\nusing NLPModels\n\n\nnlp = ADNLPModel(x-\n(x[1] - 1.0)^2 + 100*(x[2] - x[1]^2)^2 , [-1.2; 1.0])\n\n\nThis is enough to define the model. Let\ns get the objective function value at $x^0$, using only `nlp`.\n\n\n```@example adnlp\nfx = obj(nlp, nlp.meta.x0)\nprintln(\nfx = $fx\n)\n\n\n\n\n\nDone. Let's try the gradient and Hessian.\n\n\n```@example adnlp\ngx = grad(nlp, nlp.meta.x0)\nHx = hess(nlp, nlp.meta.x0)\nprintln(\"gx = $gx\")\nprintln(\"Hx = $Hx\")\n\n\nNotice how only the lower triangle of the Hessian is stored. Also notice that it is *dense*. This is a current limitation of this model. It doesn\nt return sparse matrices, so use it with care.\n\n\nLet\ns do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method\n\n\n1. Given $x^0$, $\\varepsilon \n 0$, and $\\eta \\in (0,1)$. Set $k = 0$;\n2. If $\\Vert \\nabla f(x^k) \\Vert \n \\varepsilon$ STOP with $x^* = x^k$;\n3. Compute $d^k = -\\nabla f(x^k)$;\n4. Compute $\\alpha_k \\in (0,1]$ such that $ f(x^k + \\alpha_kd^k) \n f(x^k) + \\alpha_k\\eta \\nabla f(x^k)^Td^k $\n5. Define $x^{k+1} = x^k + \\alpha_kx^k$\n6. Update $k = k + 1$ and go to step 2.\n\n\n```@example adnlp\nfunction steepest(nlp; itmax=100000, eta=1e-4, eps=1e-6, sigma=0.66)\n  x = nlp.meta.x0\n  fx = obj(nlp, x)\n  \u2207fx = grad(nlp, x)\n  slope = dot(\u2207fx, \u2207fx)\n  \u2207f_norm = sqrt(slope)\n  iter = 0\n  while \u2207f_norm \n eps \n iter \n itmax\n    t = 1.0\n    x_trial = x - t * \u2207fx\n    f_trial = obj(nlp, x_trial)\n    while obj(nlp, x - t*\u2207fx) \n fx - eta * t * slope\n      t *= sigma\n      x_trial = x - t * \u2207fx\n      f_trial = obj(nlp, x_trial)\n    end\n    x = x_trial\n    fx = f_trial\n    \u2207fx = grad(nlp, x)\n    slope = dot(\u2207fx, \u2207fx)\n    \u2207f_norm = sqrt(slope)\n    iter += 1\n  end\n  optimal = \u2207f_norm \n= eps\n  return x, fx, \u2207f_norm, optimal, iter\nend\n\nx, fx, ngx, optimal, iter = steepest(nlp)\nprintln(\nx = $x\n)\nprintln(\nfx = $fx\n)\nprintln(\nngx = $ngx\n)\nprintln(\noptimal = $optimal\n)\nprintln(\niter = $iter\n)\n\n\n\n\n\nMaybe this code is too complicated? If you're in a class you just want to show a Newton step.\n\n\n```@example adnlp\ng(x) = grad(nlp, x)\nH(x) = hess(nlp, x) + triu(hess(nlp, x)', 1)\nx = nlp.meta.x0\nd = -H(x)\\g(x)\n\n\nor a few\n\n\n```@example adnlp\nfor i = 1:5\n  x = x - H(x)\\g(x)\n  println(\nx = $x\n)\nend\n\n\n\n\n\nAlso, notice how we can reuse the method.\n\n\n```@example adnlp\nf(x) = (x[1]^2 + x[2]^2 - 4)^2 + (x[1]*x[2] - 1)^2\nx0 = [2.0; 1.0]\nnlp = ADNLPModel(f, x0)\n\n\nx, fx, ngx, optimal, iter = steepest(nlp)\n\n\nEven using a different model.\n\n\n```@example adnlp\nusing OptimizationProblems # Defines a lot of JuMP models\n\nnlp = MathProgNLPModel(woods())\nx, fx, ngx, optimal, iter = steepest(nlp)\nprintln(\nfx = $fx\n)\nprintln(\nngx = $ngx\n)\nprintln(\noptimal = $optimal\n)\nprintln(\niter = $iter\n)\n\n\n\n\n\nFor constrained minimization, you need the constraints vector and bounds too. Bounds on the variables can be passed through a new vector.\n\n\n```@example adnlp2\nusing NLPModels # hide\nf(x) = (x[1] - 1.0)^2 + 100*(x[2] - x[1]^2)^2\nx0 = [-1.2; 1.0]\nlvar = [-Inf; 0.1]\nuvar = [0.5; 0.5]\nc(x) = [x[1] + x[2] - 2; x[1]^2 + x[2]^2]\nlcon = [0.0; -Inf]\nucon = [Inf; 1.0]\nnlp = ADNLPModel(f, x0, c=c, lvar=lvar, uvar=uvar, lcon=lcon, ucon=ucon)\n\n\nprintln(\"cx = $(cons(nlp, nlp.meta.x0))\")\nprintln(\"Jx = $(jac(nlp, nlp.meta.x0))\")\n\n\na id=\nSimpleNLPModel-Tutorial-1\n/a\n\n\n## SimpleNLPModel Tutorial\n\n\nSimpleNLPModel allows you to pass every single function of the model. On the other hand, it doesn\nt handle anything else. Calling an undefined function will throw a `NotImplementedError`. Only the objective function is mandatory (if you don\nt need it, pass `x-\n0`).\n\n\n```julia\nusing NLPModels\n\nf(x) = (x[1] - 1.0)^2 + 4*(x[2] - 1.0)^2\nx0 = zeros(2)\nnlp = SimpleNLPModel(f, x0)\n\nfx = obj(nlp, nlp.meta.x0)\nprintln(\nfx = $fx\n)\n\n# grad(nlp, nlp.meta.x0) # This is undefined\n\n\n\n\n\nfx = 5.0\n\n\n\n\n\ng\n(\nx\n)\n \n=\n \n[\n2\n*\n(\nx\n[\n1\n]\n \n-\n \n1.0\n);\n \n8\n*\n(\nx\n[\n2\n]\n \n-\n \n1.0\n)]\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx0\n,\n \ng\n=\ng\n)\n\n\n\ngrad\n(\nnlp\n,\n \nnlp\n.\nmeta\n.\nx0\n)\n\n\n\n\n\n\n2-element Array{Float64,1}:\n -2.0\n -8.0\n\n\n\n\n\n\"But what's to stop me from defining \ng\n however I want?\" Nothing. So you have to be careful on how you're defining it. You should probably check your derivatives. If the function is simply defined, you can try using automatic differentiation. Alternatively, you can use the \nderivative checker\n.\n\n\ngradient_check\n(\nnlp\n)\n\n\n\n\n\n\nDict{Int64,Float64} with 0 entries\n\n\n\n\n\ngwrong\n(\nx\n)\n \n=\n \n[\n2\n*\n(\nx\n[\n1\n]\n \n-\n \n1.0\n);\n \n8\n*\nx\n[\n2\n]\n \n-\n \n1.0\n]\n \n# Find the error\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx0\n,\n \ng\n=\ngwrong\n)\n\n\ngradient_check\n(\nnlp\n)\n\n\n\n\n\n\nDict{Int64,Float64} with 1 entry:\n  2 =\n 6.999999999951384\n\n\n\n\n\nFor constrained problems, we still need the constraints function, \nlcon\n and \nucon\n. Also, let's pass the Jacobian-vector product.\n\n\nc\n(\nx\n)\n \n=\n \n[\nx\n[\n1\n]\n^\n2\n \n+\n \nx\n[\n2\n]\n^\n2\n;\n \nx\n[\n1\n]\n*\nx\n[\n2\n]\n \n-\n \n1\n]\n\n\nlcon\n \n=\n \n[\n1.0\n;\n \n0.0\n]\n\n\nucon\n \n=\n \n[\n4.0\n;\n \n0.0\n]\n\n\nJacprod\n(\nx\n,\n \nv\n)\n \n=\n \n[\n2\n*\nx\n[\n1\n]\n*\nv\n[\n1\n]\n \n+\n \n2\n*\nx\n[\n2\n]\n*\nv\n[\n2\n];\n \nx\n[\n2\n]\n*\nv\n[\n1\n]\n \n+\n \nx\n[\n1\n]\n*\nv\n[\n2\n]]\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx0\n,\n \nc\n=\nc\n,\n \nlcon\n=\nlcon\n,\n \nucon\n=\nucon\n,\n \ng\n=\ng\n,\n \nJp\n=\nJacprod\n)\n\n\njprod\n(\nnlp\n,\n \nones\n(\n2\n),\n \nones\n(\n2\n))\n\n\n\n\n\n\n2-element Array{Float64,1}:\n 4.0\n 2.0\n\n\n\n\n\nFurthermore, NLPModels also works with inplace operations. Since some models do not take full advantage of this (like ADNLPModel), a user might want to define his/her own functions that do.\n\n\nf\n(\nx\n)\n \n=\n \n(\nx\n[\n1\n]\n \n-\n \n1.0\n)\n^\n2\n \n+\n \n4\n*\n(\nx\n[\n2\n]\n \n-\n \n1.0\n)\n^\n2\n\n\nx0\n \n=\n \nzeros\n(\n2\n)\n\n\ng!\n(\nx\n,\n \ngx\n)\n \n=\n \nbegin\n\n  \ngx\n[\n1\n]\n \n=\n \n2\n*\n(\nx\n[\n1\n]\n \n-\n \n1.0\n)\n\n  \ngx\n[\n2\n]\n \n=\n \n8\n*\n(\nx\n[\n2\n]\n \n=\n \n1.0\n)\n\n  \nreturn\n \ngx\n\n\nend\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx0\n,\n \ng!\n \n=\ng!\n)\n \n# Watchout, g!=g! is interpreted as g != g!\n\n\ngx\n \n=\n \nzeros\n(\n2\n)\n\n\ngrad!\n(\nnlp\n,\n \nnlp\n.\nmeta\n.\nx0\n,\n \ngx\n)\n\n\n\n\n\n\n2-element Array{Float64,1}:\n -2.0\n  8.0", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorial/#tutorial", 
            "text": "NLPModels.jl was created for two purposes:   Allow users to access problem databases in an unified way.  Mainly, this means   CUTEst.jl ,  but it also gives access to  AMPL  problems ,  as well as JuMP defined problems (e.g. as in   OptimizationProblems.jl ).  Allow users to create their own problems in the same way.  As a consequence, optimization methods designed according to the NLPModels API  will accept NLPModels of any provenance.  See, for instance,   Optimize.jl .   The main interfaces for user defined problems are   ADNLPModel , which defines a model easily, using automatic   differentiation.  SimpleNLPModel , which allows users to handle all functions themselves,   giving", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorial/#adnlpmodel-tutorial", 
            "text": "ADNLPModel is simple to use and is useful for classrooms. It only needs the objective function $f$ and a starting point $x^0$ to be well-defined. For constrained problems, you'll also need the constraints function $c$, and the constraints vectors $c_L$ and $c_U$, such that $c_L \\leq c(x) \\leq c_U$. Equality constraints will be automatically identified as those indices $i$ for which $c_{L_i} = c_{U_i}$.  Let's define the famous Rosenbrock function  \\begin{align*} f(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2, \\end{align*}  with starting point $x^0 = (-1.2,1.0)$.  ```@example adnlp\nusing NLPModels  nlp = ADNLPModel(x- (x[1] - 1.0)^2 + 100*(x[2] - x[1]^2)^2 , [-1.2; 1.0])  This is enough to define the model. Let s get the objective function value at $x^0$, using only `nlp`.\n\n\n```@example adnlp\nfx = obj(nlp, nlp.meta.x0)\nprintln( fx = $fx )  Done. Let's try the gradient and Hessian.  ```@example adnlp\ngx = grad(nlp, nlp.meta.x0)\nHx = hess(nlp, nlp.meta.x0)\nprintln(\"gx = $gx\")\nprintln(\"Hx = $Hx\")  Notice how only the lower triangle of the Hessian is stored. Also notice that it is *dense*. This is a current limitation of this model. It doesn t return sparse matrices, so use it with care.\n\n\nLet s do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method\n\n\n1. Given $x^0$, $\\varepsilon   0$, and $\\eta \\in (0,1)$. Set $k = 0$;\n2. If $\\Vert \\nabla f(x^k) \\Vert   \\varepsilon$ STOP with $x^* = x^k$;\n3. Compute $d^k = -\\nabla f(x^k)$;\n4. Compute $\\alpha_k \\in (0,1]$ such that $ f(x^k + \\alpha_kd^k)   f(x^k) + \\alpha_k\\eta \\nabla f(x^k)^Td^k $\n5. Define $x^{k+1} = x^k + \\alpha_kx^k$\n6. Update $k = k + 1$ and go to step 2.\n\n\n```@example adnlp\nfunction steepest(nlp; itmax=100000, eta=1e-4, eps=1e-6, sigma=0.66)\n  x = nlp.meta.x0\n  fx = obj(nlp, x)\n  \u2207fx = grad(nlp, x)\n  slope = dot(\u2207fx, \u2207fx)\n  \u2207f_norm = sqrt(slope)\n  iter = 0\n  while \u2207f_norm   eps   iter   itmax\n    t = 1.0\n    x_trial = x - t * \u2207fx\n    f_trial = obj(nlp, x_trial)\n    while obj(nlp, x - t*\u2207fx)   fx - eta * t * slope\n      t *= sigma\n      x_trial = x - t * \u2207fx\n      f_trial = obj(nlp, x_trial)\n    end\n    x = x_trial\n    fx = f_trial\n    \u2207fx = grad(nlp, x)\n    slope = dot(\u2207fx, \u2207fx)\n    \u2207f_norm = sqrt(slope)\n    iter += 1\n  end\n  optimal = \u2207f_norm  = eps\n  return x, fx, \u2207f_norm, optimal, iter\nend\n\nx, fx, ngx, optimal, iter = steepest(nlp)\nprintln( x = $x )\nprintln( fx = $fx )\nprintln( ngx = $ngx )\nprintln( optimal = $optimal )\nprintln( iter = $iter )  Maybe this code is too complicated? If you're in a class you just want to show a Newton step.  ```@example adnlp\ng(x) = grad(nlp, x)\nH(x) = hess(nlp, x) + triu(hess(nlp, x)', 1)\nx = nlp.meta.x0\nd = -H(x)\\g(x)  or a few\n\n\n```@example adnlp\nfor i = 1:5\n  x = x - H(x)\\g(x)\n  println( x = $x )\nend  Also, notice how we can reuse the method.  ```@example adnlp\nf(x) = (x[1]^2 + x[2]^2 - 4)^2 + (x[1]*x[2] - 1)^2\nx0 = [2.0; 1.0]\nnlp = ADNLPModel(f, x0)  x, fx, ngx, optimal, iter = steepest(nlp)  Even using a different model.\n\n\n```@example adnlp\nusing OptimizationProblems # Defines a lot of JuMP models\n\nnlp = MathProgNLPModel(woods())\nx, fx, ngx, optimal, iter = steepest(nlp)\nprintln( fx = $fx )\nprintln( ngx = $ngx )\nprintln( optimal = $optimal )\nprintln( iter = $iter )  For constrained minimization, you need the constraints vector and bounds too. Bounds on the variables can be passed through a new vector.  ```@example adnlp2\nusing NLPModels # hide\nf(x) = (x[1] - 1.0)^2 + 100*(x[2] - x[1]^2)^2\nx0 = [-1.2; 1.0]\nlvar = [-Inf; 0.1]\nuvar = [0.5; 0.5]\nc(x) = [x[1] + x[2] - 2; x[1]^2 + x[2]^2]\nlcon = [0.0; -Inf]\nucon = [Inf; 1.0]\nnlp = ADNLPModel(f, x0, c=c, lvar=lvar, uvar=uvar, lcon=lcon, ucon=ucon)  println(\"cx = $(cons(nlp, nlp.meta.x0))\")\nprintln(\"Jx = $(jac(nlp, nlp.meta.x0))\")  a id= SimpleNLPModel-Tutorial-1 /a \n\n## SimpleNLPModel Tutorial\n\n\nSimpleNLPModel allows you to pass every single function of the model. On the other hand, it doesn t handle anything else. Calling an undefined function will throw a `NotImplementedError`. Only the objective function is mandatory (if you don t need it, pass `x- 0`).\n\n\n```julia\nusing NLPModels\n\nf(x) = (x[1] - 1.0)^2 + 4*(x[2] - 1.0)^2\nx0 = zeros(2)\nnlp = SimpleNLPModel(f, x0)\n\nfx = obj(nlp, nlp.meta.x0)\nprintln( fx = $fx )\n\n# grad(nlp, nlp.meta.x0) # This is undefined  fx = 5.0  g ( x )   =   [ 2 * ( x [ 1 ]   -   1.0 );   8 * ( x [ 2 ]   -   1.0 )]  nlp   =   SimpleNLPModel ( f ,   x0 ,   g = g )  grad ( nlp ,   nlp . meta . x0 )   2-element Array{Float64,1}:\n -2.0\n -8.0  \"But what's to stop me from defining  g  however I want?\" Nothing. So you have to be careful on how you're defining it. You should probably check your derivatives. If the function is simply defined, you can try using automatic differentiation. Alternatively, you can use the  derivative checker .  gradient_check ( nlp )   Dict{Int64,Float64} with 0 entries  gwrong ( x )   =   [ 2 * ( x [ 1 ]   -   1.0 );   8 * x [ 2 ]   -   1.0 ]   # Find the error  nlp   =   SimpleNLPModel ( f ,   x0 ,   g = gwrong )  gradient_check ( nlp )   Dict{Int64,Float64} with 1 entry:\n  2 =  6.999999999951384  For constrained problems, we still need the constraints function,  lcon  and  ucon . Also, let's pass the Jacobian-vector product.  c ( x )   =   [ x [ 1 ] ^ 2   +   x [ 2 ] ^ 2 ;   x [ 1 ] * x [ 2 ]   -   1 ]  lcon   =   [ 1.0 ;   0.0 ]  ucon   =   [ 4.0 ;   0.0 ]  Jacprod ( x ,   v )   =   [ 2 * x [ 1 ] * v [ 1 ]   +   2 * x [ 2 ] * v [ 2 ];   x [ 2 ] * v [ 1 ]   +   x [ 1 ] * v [ 2 ]]  nlp   =   SimpleNLPModel ( f ,   x0 ,   c = c ,   lcon = lcon ,   ucon = ucon ,   g = g ,   Jp = Jacprod )  jprod ( nlp ,   ones ( 2 ),   ones ( 2 ))   2-element Array{Float64,1}:\n 4.0\n 2.0  Furthermore, NLPModels also works with inplace operations. Since some models do not take full advantage of this (like ADNLPModel), a user might want to define his/her own functions that do.  f ( x )   =   ( x [ 1 ]   -   1.0 ) ^ 2   +   4 * ( x [ 2 ]   -   1.0 ) ^ 2  x0   =   zeros ( 2 )  g! ( x ,   gx )   =   begin \n   gx [ 1 ]   =   2 * ( x [ 1 ]   -   1.0 ) \n   gx [ 2 ]   =   8 * ( x [ 2 ]   =   1.0 ) \n   return   gx  end  nlp   =   SimpleNLPModel ( f ,   x0 ,   g!   = g! )   # Watchout, g!=g! is interpreted as g != g!  gx   =   zeros ( 2 )  grad! ( nlp ,   nlp . meta . x0 ,   gx )   2-element Array{Float64,1}:\n -2.0\n  8.0", 
            "title": "ADNLPModel Tutorial"
        }, 
        {
            "location": "/api/", 
            "text": "API\n\n\nAs stated in the \nhome\n page, we consider the nonlinear optimization problem in the following format: \n\\begin{align*} \\min \\quad & f(x) \\\\\n& c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u. \\end{align*}\n To develop an optimization algorithm, we are usually worried not only with $f(x)$ and $c(x)$, but also with their derivatives. Namely,\n\n\n\n\n$\\nabla f(x)$, the gradient of $f$ at the point $x$;\n\n\n$\\nabla^2 f(x)$, the Hessian of $f$ at the point $x$;\n\n\n$J(x) = \\nabla c(x)$, the Jacobian of $c$ at the point $x$;\n\n\n$\\nabla^2 f(x) + \\sum_{i=1}^m \\lambda_i \\nabla^2 c_i(x)$,   the Hessian of the Lagrangian function at the point $(x,\\lambda)$.\n\n\n\n\nThere are many ways to access some of these values, so here is a little reference guide.\n\n\n\n\nReference guide\n\n\nThe following naming should be easy enough to follow. If not, click on the link and go to the description.\n\n\n\n\n!\n means inplace;\n\n\n_coord\n means coordinate format;\n\n\nprod\n means matrix-vector product;\n\n\n_op\n means operator (as in \nLinearOperators.jl\n).\n\n\n\n\nFeel free to open an issue to suggest other methods that should apply to all NLPModels instances.\n\n\n\n\n\n\n\n\nFunction\n\n\nNLPModels function\n\n\n\n\n\n\n\n\n\n\n$f(x)$\n\n\nobj\n\n\n\n\n\n\n$\\nabla f(x)$\n\n\ngrad\n, \ngrad!\n\n\n\n\n\n\n$\\nabla^2 f(x)$\n\n\nhess\n, \nhess_op\n, \nhess_coord\n, \nhprod\n, \nhprod!\n\n\n\n\n\n\n$c(x)$\n\n\ncons\n, \ncons!\n\n\n\n\n\n\n$J(x)$\n\n\njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n\n\n\n\n\n\n$\\nabla^2 L(x,y)$\n\n\nhess\n, \nhess_op\n, \nhess_coord\n, \nhprod\n, \nhprod!\n\n\n\n\n\n\n\n\n\n\nAbstractNLPModel functions\n\n\nobj\ngrad\ngrad!\ncons\ncons!\njac_coord\njac\njprod\njprod!\njtprod\njtprod!\nhess_coord\nhess\nhess_op\nhprod\nhprod!\nNLPtoMPB\nreset!\n\n\n\n\n\n\n\nDerivative check\n\n\n#\n\n\nNLPModels.gradient_check\n \n \nFunction\n.\n\n\nCheck the first derivatives of the objective at \nx\n against centered finite differences.\n\n\nThis function returns a dictionary indexed by components of the gradient for which the relative error exceeds \nrtol\n.\n\n\n#\n\n\nNLPModels.jacobian_check\n \n \nFunction\n.\n\n\nCheck the first derivatives of the constraints at \nx\n against centered finite differences.\n\n\nThis function returns a dictionary indexed by (j, i) tuples such that the relative error in the \ni\n-th partial derivative of the \nj\n-th constraint exceeds \nrtol\n.\n\n\n#\n\n\nNLPModels.hessian_check\n \n \nFunction\n.\n\n\nCheck the second derivatives of the objective and each constraints at \nx\n against centered finite differences. This check does not rely on exactness of the first derivatives, only on objective and constraint values.\n\n\nThe \nsgn\n arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as\n\n\nL(x,y) = f(x) + \u2211 y\u2c7c c\u2c7c(x)\n\n\n\n\n\ne.g., as in \nJuMPNLPModel\ns, and a negative value if the Lagrangian is formulated as\n\n\nL(x,y) = f(x) - \u2211 y\u2c7c c\u2c7c(x)\n\n\n\n\n\ne.g., as in \nAmplModel\ns. Only the sign of \nsgn\n is important.\n\n\nThis function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k \n 0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative \u2202\u00b2f\u2096/\u2202x\u1d62\u2202x\u2c7c exceeds \nrtol\n.\n\n\n#\n\n\nNLPModels.hessian_check_from_grad\n \n \nFunction\n.\n\n\nCheck the second derivatives of the objective and each constraints at \nx\n against centered finite differences. This check assumes exactness of the first derivatives.\n\n\nThe \nsgn\n arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as\n\n\nL(x,y) = f(x) + \u2211 y\u2c7c c\u2c7c(x)\n\n\n\n\n\ne.g., as in \nJuMPNLPModel\ns, and a negative value if the Lagrangian is formulated as\n\n\nL(x,y) = f(x) - \u2211 y\u2c7c c\u2c7c(x)\n\n\n\n\n\ne.g., as in \nAmplModel\ns. Only the sign of \nsgn\n is important.\n\n\nThis function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k \n 0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative \u2202\u00b2f\u2096/\u2202x\u1d62\u2202x\u2c7c exceeds \nrtol\n.", 
            "title": "API"
        }, 
        {
            "location": "/api/#api", 
            "text": "As stated in the  home  page, we consider the nonlinear optimization problem in the following format:  \\begin{align*} \\min \\quad & f(x) \\\\\n& c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u. \\end{align*}  To develop an optimization algorithm, we are usually worried not only with $f(x)$ and $c(x)$, but also with their derivatives. Namely,   $\\nabla f(x)$, the gradient of $f$ at the point $x$;  $\\nabla^2 f(x)$, the Hessian of $f$ at the point $x$;  $J(x) = \\nabla c(x)$, the Jacobian of $c$ at the point $x$;  $\\nabla^2 f(x) + \\sum_{i=1}^m \\lambda_i \\nabla^2 c_i(x)$,   the Hessian of the Lagrangian function at the point $(x,\\lambda)$.   There are many ways to access some of these values, so here is a little reference guide.", 
            "title": "API"
        }, 
        {
            "location": "/api/#reference-guide", 
            "text": "The following naming should be easy enough to follow. If not, click on the link and go to the description.   !  means inplace;  _coord  means coordinate format;  prod  means matrix-vector product;  _op  means operator (as in  LinearOperators.jl ).   Feel free to open an issue to suggest other methods that should apply to all NLPModels instances.     Function  NLPModels function      $f(x)$  obj    $\\nabla f(x)$  grad ,  grad!    $\\nabla^2 f(x)$  hess ,  hess_op ,  hess_coord ,  hprod ,  hprod!    $c(x)$  cons ,  cons!    $J(x)$  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod!    $\\nabla^2 L(x,y)$  hess ,  hess_op ,  hess_coord ,  hprod ,  hprod!", 
            "title": "Reference guide"
        }, 
        {
            "location": "/api/#abstractnlpmodel-functions", 
            "text": "obj\ngrad\ngrad!\ncons\ncons!\njac_coord\njac\njprod\njprod!\njtprod\njtprod!\nhess_coord\nhess\nhess_op\nhprod\nhprod!\nNLPtoMPB\nreset!", 
            "title": "AbstractNLPModel functions"
        }, 
        {
            "location": "/api/#derivative-check", 
            "text": "#  NLPModels.gradient_check     Function .  Check the first derivatives of the objective at  x  against centered finite differences.  This function returns a dictionary indexed by components of the gradient for which the relative error exceeds  rtol .  #  NLPModels.jacobian_check     Function .  Check the first derivatives of the constraints at  x  against centered finite differences.  This function returns a dictionary indexed by (j, i) tuples such that the relative error in the  i -th partial derivative of the  j -th constraint exceeds  rtol .  #  NLPModels.hessian_check     Function .  Check the second derivatives of the objective and each constraints at  x  against centered finite differences. This check does not rely on exactness of the first derivatives, only on objective and constraint values.  The  sgn  arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as  L(x,y) = f(x) + \u2211 y\u2c7c c\u2c7c(x)  e.g., as in  JuMPNLPModel s, and a negative value if the Lagrangian is formulated as  L(x,y) = f(x) - \u2211 y\u2c7c c\u2c7c(x)  e.g., as in  AmplModel s. Only the sign of  sgn  is important.  This function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k   0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative \u2202\u00b2f\u2096/\u2202x\u1d62\u2202x\u2c7c exceeds  rtol .  #  NLPModels.hessian_check_from_grad     Function .  Check the second derivatives of the objective and each constraints at  x  against centered finite differences. This check assumes exactness of the first derivatives.  The  sgn  arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as  L(x,y) = f(x) + \u2211 y\u2c7c c\u2c7c(x)  e.g., as in  JuMPNLPModel s, and a negative value if the Lagrangian is formulated as  L(x,y) = f(x) - \u2211 y\u2c7c c\u2c7c(x)  e.g., as in  AmplModel s. Only the sign of  sgn  is important.  This function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k   0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative \u2202\u00b2f\u2096/\u2202x\u1d62\u2202x\u2c7c exceeds  rtol .", 
            "title": "Derivative check"
        }, 
        {
            "location": "/remissive-index/", 
            "text": "Index\n\n\n\n\nNLPModels.SimpleNLPModel\n\n\nNLPModels.SlackModel\n\n\nNLPModels.cons\n\n\nNLPModels.cons!\n\n\nNLPModels.grad\n\n\nNLPModels.grad!\n\n\nNLPModels.gradient_check\n\n\nNLPModels.hess\n\n\nNLPModels.hess_coord\n\n\nNLPModels.hess_op\n\n\nNLPModels.hessian_check\n\n\nNLPModels.hessian_check_from_grad\n\n\nNLPModels.hprod\n\n\nNLPModels.hprod!\n\n\nNLPModels.jac\n\n\nNLPModels.jac_coord\n\n\nNLPModels.jacobian_check\n\n\nNLPModels.jprod\n\n\nNLPModels.jprod!\n\n\nNLPModels.jtprod\n\n\nNLPModels.jtprod!\n\n\nNLPModels.obj\n\n\nNLPModels.reset!", 
            "title": "Remissive Index"
        }, 
        {
            "location": "/remissive-index/#index", 
            "text": "NLPModels.SimpleNLPModel  NLPModels.SlackModel  NLPModels.cons  NLPModels.cons!  NLPModels.grad  NLPModels.grad!  NLPModels.gradient_check  NLPModels.hess  NLPModels.hess_coord  NLPModels.hess_op  NLPModels.hessian_check  NLPModels.hessian_check_from_grad  NLPModels.hprod  NLPModels.hprod!  NLPModels.jac  NLPModels.jac_coord  NLPModels.jacobian_check  NLPModels.jprod  NLPModels.jprod!  NLPModels.jtprod  NLPModels.jtprod!  NLPModels.obj  NLPModels.reset!", 
            "title": "Index"
        }
    ]
}