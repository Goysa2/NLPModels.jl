{
    "docs": [
        {
            "location": "/", 
            "text": "NLPModels.jl documentation\n\n\nThis package provides general guidelines to represent optimization problems in Julia and a standardized API to evaluate the functions and their derivatives. The main objective is to be able to rely on that \nAPI\n when designing optimization solvers in Julia.\n\n\n\n\nIntroduction\n\n\nThe general form of the optimization problem is \n\\begin{align*} \\min \\quad & f(x) \\\\\n& c_i(x) = 0, \\quad i \\in E, \\\\\n& c_{L_i} \\leq c_i(x) \\leq c_{U_i}, \\quad i \\in I, \\\\\n& \\ell \\leq x \\leq u, \\end{align*}\n where $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$, $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$, $E\\cup I = \\{1,2,\\dots,m\\}$, $E\\cap I = \\emptyset$, and $c_{L_i}, c_{U_i}, \\ell_j, u_j \\in \\mathbb{R}\\cup\\{\\pm\\infty\\}$ for $i = 1,\\dots,m$ and $j = 1,\\dots,n$.\n\n\nFor computational reasons, we write \n\\begin{align*} \\min \\quad & f(x) \\\\\n& c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u, \\end{align*}\n defining $c_{L_i} = c_{U_i}$ for all $i \\in E$. The Lagrangian of this problem is defined as \n\\begin{align*} L(x,\\lambda,z^L,z^U;\\sigma) = \\sigma f(x) + c(x)^T\\lambda  + \\sum_{i=1}^n z_i^L(x_i-l_i) + \\sum_{i=1}^nz_i^U(u_i-x_i), \\end{align*}\n where $\\sigma$ is a scaling parameter included for computational reasons. Notice that, for the Hessian, the variables $z^L$ and $z^U$ are not used.\n\n\nOptimization problems are represented by an instance/subtype of \nAbstractNLPModel\n. Such instances are composed of\n\n\n\n\nan instance of \nNLPModelMeta\n, which provides information about the problem,   including the number of variables, constraints, bounds on the variables, etc.\n\n\nother data specific to the provenance of the problem.\n\n\n\n\n\n\nInstall\n\n\nInstall NLPModels.jl with the following commands.\n\n\nPkg\n.\nadd\n(\nNLPModels\n)\n\n\n\n\n\n\nIf you want the \nADNLPModel\n or the \nMathProgNLPModel\n, you also need the\n\n\nPkg\n.\nadd\n(\nForwardDiff\n)\n\n\nPkg\n.\nadd\n(\nMathProgBase\n)\n\n\n\n\n\n\nrespectively. In addition, if you want to create a \nMathProgNLPModel\n from a \nJuMP\n model, you'll need\n\n\nPkg\n.\nadd\n(\nJuMP\n)\n\n\n\n\n\n\n\n\nUsage\n\n\nSee the \nModels\n, or the \nTutorial\n, or the \nAPI\n.\n\n\n\n\nInternal Interfaces\n\n\n\n\nADNLPModel\n: Uses    \nForwardDiff\n to compute the    derivatives. It has a very simple interface, though it isn't very efficient    for larger problems.\n\n\nMathProgNLPModel\n: Uses a \nMathProgModel\n, derived from a    \nAbstractMathProgModel\n model.    For instance, \nJuMP.jl\n models can be    used.\n\n\nSimpleNLPModel\n: Only uses user defined functions.\n\n\nSlackModel\n: Creates an equality constrained problem with bounds     on the variables using an existing NLPModel.\n\n\n\n\n\n\nExternal Interfaces\n\n\n\n\nAmplModel\n: Defined in    \nAmplNLReader.jl\n    for problems modeled using \nAMPL\n\n\nCUTEstModel\n: Defined in    \nCUTEst.jl\n for    problems from \nCUTEst\n.\n\n\n\n\nIf you want your interface here, open a PR.\n\n\n\n\nAttributes\n\n\nNLPModelMeta\n objects have the following attributes:\n\n\n\n\n\n\n\n\nAttribute\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nnvar\n\n\nInt\n\n\nnumber of variables\n\n\n\n\n\n\nx0\n\n\nArray{Float64,1}\n\n\ninitial guess\n\n\n\n\n\n\nlvar\n\n\nArray{Float64,1}\n\n\nvector of lower bounds\n\n\n\n\n\n\nuvar\n\n\nArray{Float64,1}\n\n\nvector of upper bounds\n\n\n\n\n\n\nifix\n\n\nArray{Int64,1}\n\n\nindices of fixed variables\n\n\n\n\n\n\nilow\n\n\nArray{Int64,1}\n\n\nindices of variables with lower bound only\n\n\n\n\n\n\niupp\n\n\nArray{Int64,1}\n\n\nindices of variables with upper bound only\n\n\n\n\n\n\nirng\n\n\nArray{Int64,1}\n\n\nindices of variables with lower and upper bound (range)\n\n\n\n\n\n\nifree\n\n\nArray{Int64,1}\n\n\nindices of free variables\n\n\n\n\n\n\niinf\n\n\nArray{Int64,1}\n\n\nindices of visibly infeasible bounds\n\n\n\n\n\n\nncon\n\n\nInt\n\n\ntotal number of general constraints\n\n\n\n\n\n\nnlin\n\n\nInt\n\n\nnumber of linear constraints\n\n\n\n\n\n\nnnln\n\n\nInt\n\n\nnumber of nonlinear general constraints\n\n\n\n\n\n\nnnet\n\n\nInt\n\n\nnumber of nonlinear network constraints\n\n\n\n\n\n\ny0\n\n\nArray{Float64,1}\n\n\ninitial Lagrange multipliers\n\n\n\n\n\n\nlcon\n\n\nArray{Float64,1}\n\n\nvector of constraint lower bounds\n\n\n\n\n\n\nucon\n\n\nArray{Float64,1}\n\n\nvector of constraint upper bounds\n\n\n\n\n\n\nlin\n\n\nRange1{Int64}\n\n\nindices of linear constraints\n\n\n\n\n\n\nnln\n\n\nRange1{Int64}\n\n\nindices of nonlinear constraints (not network)\n\n\n\n\n\n\nnnet\n\n\nRange1{Int64}\n\n\nindices of nonlinear network constraints\n\n\n\n\n\n\njfix\n\n\nArray{Int64,1}\n\n\nindices of equality constraints\n\n\n\n\n\n\njlow\n\n\nArray{Int64,1}\n\n\nindices of constraints of the form c(x) \u2265 cl\n\n\n\n\n\n\njupp\n\n\nArray{Int64,1}\n\n\nindices of constraints of the form c(x) \u2264 cu\n\n\n\n\n\n\njrng\n\n\nArray{Int64,1}\n\n\nindices of constraints of the form cl \u2264 c(x) \u2264 cu\n\n\n\n\n\n\njfree\n\n\nArray{Int64,1}\n\n\nindices of \"free\" constraints (there shouldn't be any)\n\n\n\n\n\n\njinf\n\n\nArray{Int64,1}\n\n\nindices of the visibly infeasible constraints\n\n\n\n\n\n\nnnzj\n\n\nInt\n\n\nnumber of nonzeros in the sparse Jacobian\n\n\n\n\n\n\nnnzh\n\n\nInt\n\n\nnumber of nonzeros in the sparse Hessian\n\n\n\n\n\n\nminimize\n\n\nBool\n\n\ntrue if \noptimize == minimize\n\n\n\n\n\n\nislp\n\n\nBool\n\n\ntrue if the problem is a linear program\n\n\n\n\n\n\nname\n\n\nString\n\n\nproblem name\n\n\n\n\n\n\n\n\n\n\nLicense\n\n\nThis content is released under the \nMIT\n License. \n \n\n\n\n\nContents\n\n\n\n\nAPI\n\n\nReference guide\n\n\nAbstractNLPModel functions\n\n\nDerivative check\n\n\n\n\n\n\nModels\n\n\nADNLPModel\n\n\nMathProgNLPModel\n\n\nSimpleNLPModel\n\n\nSlackModel\n\n\n\n\n\n\nReference\n\n\nNLPModels.jl documentation\n\n\nIntroduction\n\n\nInstall\n\n\nUsage\n\n\nInternal Interfaces\n\n\nExternal Interfaces\n\n\nAttributes\n\n\nLicense\n\n\nContents\n\n\n\n\n\n\nTutorial\n\n\nADNLPModel Tutorial\n\n\nSimpleNLPModel Tutorial", 
            "title": "Home"
        }, 
        {
            "location": "/#nlpmodelsjl-documentation", 
            "text": "This package provides general guidelines to represent optimization problems in Julia and a standardized API to evaluate the functions and their derivatives. The main objective is to be able to rely on that  API  when designing optimization solvers in Julia.", 
            "title": "NLPModels.jl documentation"
        }, 
        {
            "location": "/#introduction", 
            "text": "The general form of the optimization problem is  \\begin{align*} \\min \\quad & f(x) \\\\\n& c_i(x) = 0, \\quad i \\in E, \\\\\n& c_{L_i} \\leq c_i(x) \\leq c_{U_i}, \\quad i \\in I, \\\\\n& \\ell \\leq x \\leq u, \\end{align*}  where $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$, $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$, $E\\cup I = \\{1,2,\\dots,m\\}$, $E\\cap I = \\emptyset$, and $c_{L_i}, c_{U_i}, \\ell_j, u_j \\in \\mathbb{R}\\cup\\{\\pm\\infty\\}$ for $i = 1,\\dots,m$ and $j = 1,\\dots,n$.  For computational reasons, we write  \\begin{align*} \\min \\quad & f(x) \\\\\n& c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u, \\end{align*}  defining $c_{L_i} = c_{U_i}$ for all $i \\in E$. The Lagrangian of this problem is defined as  \\begin{align*} L(x,\\lambda,z^L,z^U;\\sigma) = \\sigma f(x) + c(x)^T\\lambda  + \\sum_{i=1}^n z_i^L(x_i-l_i) + \\sum_{i=1}^nz_i^U(u_i-x_i), \\end{align*}  where $\\sigma$ is a scaling parameter included for computational reasons. Notice that, for the Hessian, the variables $z^L$ and $z^U$ are not used.  Optimization problems are represented by an instance/subtype of  AbstractNLPModel . Such instances are composed of   an instance of  NLPModelMeta , which provides information about the problem,   including the number of variables, constraints, bounds on the variables, etc.  other data specific to the provenance of the problem.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#install", 
            "text": "Install NLPModels.jl with the following commands.  Pkg . add ( NLPModels )   If you want the  ADNLPModel  or the  MathProgNLPModel , you also need the  Pkg . add ( ForwardDiff )  Pkg . add ( MathProgBase )   respectively. In addition, if you want to create a  MathProgNLPModel  from a  JuMP  model, you'll need  Pkg . add ( JuMP )", 
            "title": "Install"
        }, 
        {
            "location": "/#usage", 
            "text": "See the  Models , or the  Tutorial , or the  API .", 
            "title": "Usage"
        }, 
        {
            "location": "/#internal-interfaces", 
            "text": "ADNLPModel : Uses     ForwardDiff  to compute the    derivatives. It has a very simple interface, though it isn't very efficient    for larger problems.  MathProgNLPModel : Uses a  MathProgModel , derived from a     AbstractMathProgModel  model.    For instance,  JuMP.jl  models can be    used.  SimpleNLPModel : Only uses user defined functions.  SlackModel : Creates an equality constrained problem with bounds     on the variables using an existing NLPModel.", 
            "title": "Internal Interfaces"
        }, 
        {
            "location": "/#external-interfaces", 
            "text": "AmplModel : Defined in     AmplNLReader.jl     for problems modeled using  AMPL  CUTEstModel : Defined in     CUTEst.jl  for    problems from  CUTEst .   If you want your interface here, open a PR.", 
            "title": "External Interfaces"
        }, 
        {
            "location": "/#attributes", 
            "text": "NLPModelMeta  objects have the following attributes:     Attribute  Type  Notes      nvar  Int  number of variables    x0  Array{Float64,1}  initial guess    lvar  Array{Float64,1}  vector of lower bounds    uvar  Array{Float64,1}  vector of upper bounds    ifix  Array{Int64,1}  indices of fixed variables    ilow  Array{Int64,1}  indices of variables with lower bound only    iupp  Array{Int64,1}  indices of variables with upper bound only    irng  Array{Int64,1}  indices of variables with lower and upper bound (range)    ifree  Array{Int64,1}  indices of free variables    iinf  Array{Int64,1}  indices of visibly infeasible bounds    ncon  Int  total number of general constraints    nlin  Int  number of linear constraints    nnln  Int  number of nonlinear general constraints    nnet  Int  number of nonlinear network constraints    y0  Array{Float64,1}  initial Lagrange multipliers    lcon  Array{Float64,1}  vector of constraint lower bounds    ucon  Array{Float64,1}  vector of constraint upper bounds    lin  Range1{Int64}  indices of linear constraints    nln  Range1{Int64}  indices of nonlinear constraints (not network)    nnet  Range1{Int64}  indices of nonlinear network constraints    jfix  Array{Int64,1}  indices of equality constraints    jlow  Array{Int64,1}  indices of constraints of the form c(x) \u2265 cl    jupp  Array{Int64,1}  indices of constraints of the form c(x) \u2264 cu    jrng  Array{Int64,1}  indices of constraints of the form cl \u2264 c(x) \u2264 cu    jfree  Array{Int64,1}  indices of \"free\" constraints (there shouldn't be any)    jinf  Array{Int64,1}  indices of the visibly infeasible constraints    nnzj  Int  number of nonzeros in the sparse Jacobian    nnzh  Int  number of nonzeros in the sparse Hessian    minimize  Bool  true if  optimize == minimize    islp  Bool  true if the problem is a linear program    name  String  problem name", 
            "title": "Attributes"
        }, 
        {
            "location": "/#license", 
            "text": "This content is released under the  MIT  License.", 
            "title": "License"
        }, 
        {
            "location": "/#contents", 
            "text": "API  Reference guide  AbstractNLPModel functions  Derivative check    Models  ADNLPModel  MathProgNLPModel  SimpleNLPModel  SlackModel    Reference  NLPModels.jl documentation  Introduction  Install  Usage  Internal Interfaces  External Interfaces  Attributes  License  Contents    Tutorial  ADNLPModel Tutorial  SimpleNLPModel Tutorial", 
            "title": "Contents"
        }, 
        {
            "location": "/models/", 
            "text": "Models\n\n\nThere are currently three models implemented in this package, besides the external ones.\n\n\n\n\nADNLPModel\n\n\n#\n\n\nNLPModels.ADNLPModel\n \n \nType\n.\n\n\nADNLPModel is an AbstractNLPModel using ForwardDiff to computer the derivatives. In this interface, the objective function $f$ and an initial estimate are required. If there are constraints, the function $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$  and the vectors $c_L$ and $c_U$ also need to be passed. Bounds on the variables and an inital estimate to the Lagrangian multipliers can also be provided.\n\n\nADNLPModel(f, x0; lvar = [-\u221e,\u2026,-\u221e], uvar = [\u221e,\u2026,\u221e], y0=zeros,\n  c = NotImplemented, lcon = [-\u221e,\u2026,-\u221e], ucon = [\u221e,\u2026,\u221e], name = \nGeneric\n)\n\n\n\n\n\n\n\nf :: Function\n - The objective function $f$;\n\n\nx0 :: Vector\n - The initial point of the problem;\n\n\nlvar :: Vector\n - $\\ell$, the lower bound of the variables;\n\n\nuvar :: Vector\n - $u$, the upper bound of the variables;\n\n\nc :: Function\n - The constraints function $c$;\n\n\ny0 :: Vector\n - The initial value of the Lagrangian estimates;\n\n\nlcon :: Vector\n - $c_L$, the lower bounds of the constraints function;\n\n\nucon :: Vector\n - $c_U$, the upper bounds of the constraints function;\n\n\nname :: String\n - A name for the model.\n\n\n\n\nThe functions follow the same restrictions of ForwardDiff functions, summarised here:\n\n\n\n\nThe function can only be composed of generic Julia functions;\n\n\nThe function must accept only one argument;\n\n\nThe function's argument must accept a subtype of Vector;\n\n\nThe function should be type-stable.\n\n\n\n\nFor contrained problems, the function $c$ is required, and it must return an array even when m = 1, and $c_L$ and $c_U$ should be passed, otherwise the problem is ill-formed. For equality constraints, the corresponding index of $c_L$ and $c_U$ should be the same.\n\n\n\n\nExample\n\n\nusing\n \nNLPModels\n\n\nf\n(\nx\n)\n \n=\n \nsum\n(\nx\n.^\n4\n)\n\n\nx\n \n=\n \n[\n1.0\n;\n \n0.5\n;\n \n0.25\n;\n \n0.125\n]\n\n\nnlp\n \n=\n \nADNLPModel\n(\nf\n,\n \nx\n)\n\n\ngrad\n(\nnlp\n,\n \nx\n)\n\n\n\n\n\n\n4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125\n\n\n\n\n\n\n\nList of implemented functions\n\n\ncons\n, \ncons!\n, \ngrad\n, \ngrad!\n, \nhess\n, \nhess_coord\n, \nhprod\n, \nhprod!\n, \njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n, \nobj\n\n\n\n\nMathProgNLPModel\n\n\n#\n\n\nNLPModels.MathProgNLPModel\n \n \nType\n.\n\n\nConstruct a \nMathProgNLPModel\n from a \nMathProgModel\n.\n\n\nConstruct a \nMathProgNLPModel\n from a JuMP \nModel\n.\n\n\n\n\nExample\n\n\nusing\n \nNLPModels\n,\n \nMathProgBase\n,\n \nJuMP\n\n\nm\n \n=\n \nModel\n()\n\n\n@\nvariable\n(\nm\n,\n \nx\n[\n1\n:\n4\n])\n\n\n@\nNLobjective\n(\nm\n,\n \nMin\n,\n \nsum\n{\nx\n[\ni\n]\n^\n4\n,\n \ni\n=\n1\n:\n4\n})\n\n\nnlp\n \n=\n \nMathProgNLPModel\n(\nm\n)\n\n\nx0\n \n=\n \n[\n1.0\n;\n \n0.5\n;\n \n0.25\n;\n \n0.125\n]\n\n\ngrad\n(\nnlp\n,\n \nx0\n)\n\n\n\n\n\n\n4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125\n\n\n\n\n\n\n\nList of implemented functions\n\n\ncons\n, \ncons!\n, \ngrad\n, \ngrad!\n, \nhess\n, \nhess_coord\n, \nhprod\n, \nhprod!\n, \njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n, \nobj\n\n\n\n\nSimpleNLPModel\n\n\n#\n\n\nNLPModels.SimpleNLPModel\n \n \nType\n.\n\n\nSimpleNLPModel is an AbstractNLPModel that uses only user-defined functions. In this interface, the objective function $f$ and an initial estimate are required. If the user wants to use derivatives, they need to be passed. The same goes for the Hessian and Hessian-Vector product. For constraints, $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$  and the vectors $c_L$ and $c_U$ also need to be passed. Bounds on the variables and an inital estimate to the Lagrangian multipliers can also be provided. The user can also pass the Jacobian and the Lagrangian Hessian and Hessian-Vector product.\n\n\nSimpleNLPModel(f, x0; lvar = [-\u221e,\u2026,-\u221e], uvar = [\u221e,\u2026,\u221e], y0=zeros,\n  lcon = [-\u221e,\u2026,-\u221e], ucon = [\u221e,\u2026,\u221e], name = \nGeneric\n,\n  [list of functions])\n\n\n\n\n\n\n\nf :: Function\n - The objective function $f$;\n\n\nx0 :: Vector\n - The initial point of the problem;\n\n\nlvar :: Vector\n - $\\ell$, the lower bound of the variables;\n\n\nuvar :: Vector\n - $u$, the upper bound of the variables;\n\n\ny0 :: Vector\n - The initial value of the Lagrangian estimates;\n\n\nlcon :: Vector\n - $c_L$, the lower bounds of the constraints function;\n\n\nucon :: Vector\n - $c_U$, the upper bounds of the constraints function;\n\n\nname :: String\n - A name for the model.\n\n\n\n\nAll functions passed have a direct correlation with a NLP function. You don't have to define any more than you need, but calling an undefined function will throw a \nNotImplementedError\n. The list is\n\n\n\n\ng\n and \ng!\n: $\\nabla f(x)$, the gradient of the objective function;     see \ngrad\n.\n\n\n\n\ngx = g(x)\ngx = g!(x, gx)\n\n\n\n\n\n\n\nH\n: The lower triangle of the Hessian of the objective function or of the     Lagrangian;     see \nhess\n.\n\n\n\n\nHx = H(x; obj_weight=1.0) # if the problem is unconstrained\nHx = H(x; obj_weight=1.0, y=zeros) # if the problem is constrained\n\n\n\n\n\n\n\nHcoord\n - The lower triangle of the Hessian of the objective function     or of the Lagrangian, in triplet format;     see \nhess_coord\n.\n\n\n\n\n(rows,cols,vals) = Hcoord(x; obj_weight=1.0) # if the problem is unconstrained\n(rows,cols,vals) = Hcoord(x; obj_weight=1.0, y=zeros) # if the problem is constrained\n\n\n\n\n\n\n\nHp\n and \nHp!\n - The product of the Hessian of the objective function or of     the Lagrangian by a vector;     see \nhprod\n.\n\n\n\n\nHv = Hp(x, v, obj_weight=1.0) # if the problem is unconstrained\nHv = Hp!(x, v, Hv, obj_weight=1.0) # if the problem is unconstrained\nHv = Hp(x, v, obj_weight=1.0, y=zeros) # if the problem is constrained\nHv = Hp!(x, v, Hv, obj_weight=1.0, y=zeros) # if the problem is constrained\n\n\n\n\n\n\n\nc\n and \nc!\n - $c(x)$, the constraints function;     see \ncons\n.\n\n\n\n\ncx = c(x)\ncx = c!(x, cx)\n\n\n\n\n\n\n\nJ\n - $J(x)$, the Jacobian of the constraints;     see \njac\n.\n\n\n\n\nJx = J(x)\n\n\n\n\n\n\n\nJcoord\n - $J(x)$, the Jacobian of the constraints, in triplet format;     see \njac_coord\n.\n\n\n\n\n(rows,cols,vals) = Jcoord(x)\n\n\n\n\n\n\n\nJp\n and \nJp!\n - The Jacobian-vector product;     see \njprod\n.\n\n\n\n\nJv = Jp(x, v)\nJv = Jp!(x, v, Jv)\n\n\n\n\n\n\n\nJtp\n and \nJtp!\n - The Jacobian-transposed-vector product;     see \njtprod\n.\n\n\n\n\nJtv = Jtp(x, v)\nJtv = Jtp!(x, v, Jtv)\n\n\n\n\n\nFor contrained problems, the function $c$ is required, and it must return an array even when m = 1, and $c_L$ and $c_U$ should be passed, otherwise the problem is ill-formed. For equality constraints, the corresponding index of $c_L$ and $c_U$ should be the same.\n\n\n\n\nExample\n\n\nusing\n \nNLPModels\n\n\nf\n(\nx\n)\n \n=\n \nsum\n(\nx\n.^\n4\n)\n\n\ng\n(\nx\n)\n \n=\n \n4\n*\nx\n.^\n3\n\n\nx\n \n=\n \n[\n1.0\n;\n \n0.5\n;\n \n0.25\n;\n \n0.125\n]\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx\n,\n \ng\n=\ng\n)\n\n\ngrad\n(\nnlp\n,\n \nx\n)\n\n\n\n\n\n\n4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125\n\n\n\n\n\n\n\nList of implemented functions\n\n\ncons\n, \ncons!\n, \ngrad\n, \ngrad!\n, \nhess\n, \nhess_coord\n, \nhprod\n, \nhprod!\n, \njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n, \nobj\n\n\n\n\nSlackModel\n\n\n#\n\n\nNLPModels.SlackModel\n \n \nType\n.\n\n\nA model whose only inequality constraints are bounds.\n\n\nGiven a model, this type represents a second model in which slack variables are introduced so as to convert linear and nonlinear inequality constraints to equality constraints and bounds. More precisely, if the original model has the form\n\n\n\n\n \\min f(x)  \\mbox{ s. t. }  c_L \\leq c(x) \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u, \n\n\n\n\nthe new model appears to the user as\n\n\n\n\n \\min f(X)  \\mbox{ s. t. }  g(X) = 0 \\mbox{ and } L \\leq X \\leq U. \n\n\n\n\nThe unknowns $X = (x, s)$ contain the original variables and slack variables $s$. The latter are such that the new model has the general form\n\n\n\n\n \\min f(x)  \\mbox{ s. t. }  c(x) - s = 0, c_L \\leq s \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u, \n\n\n\n\nalthough no slack variables are introduced for equality constraints.\n\n\nThe slack variables are implicitly ordered as [s(low), s(upp), s(rng)], where \nlow\n, \nupp\n and \nrng\n represent the indices of the constraints of the form $c_L \\leq c(x) \n \\infty$, $-\\infty \n c(x) \\leq c_U$ and $c_L \\leq c(x) \\leq c_U$, respectively.\n\n\n\n\nExample\n\n\nusing\n \nNLPModels\n\n\nf\n(\nx\n)\n \n=\n \nx\n[\n1\n]\n^\n2\n \n+\n \n4\nx\n[\n2\n]\n^\n2\n\n\nc\n(\nx\n)\n \n=\n \n[\nx\n[\n1\n]\n*\nx\n[\n2\n]\n \n-\n \n1\n]\n\n\nx\n \n=\n \n[\n2.0\n;\n \n2.0\n]\n\n\nnlp\n \n=\n \nADNLPModel\n(\nf\n,\n \nx\n,\n \nc\n=\nc\n,\n \nlcon\n=\n[\n0.0\n])\n\n\nnlp_slack\n \n=\n \nSlackModel\n(\nnlp\n)\n\n\nnlp_slack\n.\nmeta\n.\nlvar\n\n\n\n\n\n\n3-element Array{Float64,1}:\n -Inf\n -Inf\n    0.0\n\n\n\n\n\n\n\nList of implemented functions\n\n\ncons\n, \ncons!\n, \ngrad\n, \ngrad!\n, \nhess\n, \nhess_coord\n, \nhprod\n, \nhprod!\n, \njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n, \nobj\n, \nreset!", 
            "title": "Models"
        }, 
        {
            "location": "/models/#models", 
            "text": "There are currently three models implemented in this package, besides the external ones.", 
            "title": "Models"
        }, 
        {
            "location": "/models/#adnlpmodel", 
            "text": "#  NLPModels.ADNLPModel     Type .  ADNLPModel is an AbstractNLPModel using ForwardDiff to computer the derivatives. In this interface, the objective function $f$ and an initial estimate are required. If there are constraints, the function $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$  and the vectors $c_L$ and $c_U$ also need to be passed. Bounds on the variables and an inital estimate to the Lagrangian multipliers can also be provided.  ADNLPModel(f, x0; lvar = [-\u221e,\u2026,-\u221e], uvar = [\u221e,\u2026,\u221e], y0=zeros,\n  c = NotImplemented, lcon = [-\u221e,\u2026,-\u221e], ucon = [\u221e,\u2026,\u221e], name =  Generic )   f :: Function  - The objective function $f$;  x0 :: Vector  - The initial point of the problem;  lvar :: Vector  - $\\ell$, the lower bound of the variables;  uvar :: Vector  - $u$, the upper bound of the variables;  c :: Function  - The constraints function $c$;  y0 :: Vector  - The initial value of the Lagrangian estimates;  lcon :: Vector  - $c_L$, the lower bounds of the constraints function;  ucon :: Vector  - $c_U$, the upper bounds of the constraints function;  name :: String  - A name for the model.   The functions follow the same restrictions of ForwardDiff functions, summarised here:   The function can only be composed of generic Julia functions;  The function must accept only one argument;  The function's argument must accept a subtype of Vector;  The function should be type-stable.   For contrained problems, the function $c$ is required, and it must return an array even when m = 1, and $c_L$ and $c_U$ should be passed, otherwise the problem is ill-formed. For equality constraints, the corresponding index of $c_L$ and $c_U$ should be the same.", 
            "title": "ADNLPModel"
        }, 
        {
            "location": "/models/#example", 
            "text": "using   NLPModels  f ( x )   =   sum ( x .^ 4 )  x   =   [ 1.0 ;   0.5 ;   0.25 ;   0.125 ]  nlp   =   ADNLPModel ( f ,   x )  grad ( nlp ,   x )   4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125", 
            "title": "Example"
        }, 
        {
            "location": "/models/#list-of-implemented-functions", 
            "text": "cons ,  cons! ,  grad ,  grad! ,  hess ,  hess_coord ,  hprod ,  hprod! ,  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod! ,  obj", 
            "title": "List of implemented functions"
        }, 
        {
            "location": "/models/#mathprognlpmodel", 
            "text": "#  NLPModels.MathProgNLPModel     Type .  Construct a  MathProgNLPModel  from a  MathProgModel .  Construct a  MathProgNLPModel  from a JuMP  Model .", 
            "title": "MathProgNLPModel"
        }, 
        {
            "location": "/models/#example_1", 
            "text": "using   NLPModels ,   MathProgBase ,   JuMP  m   =   Model ()  @ variable ( m ,   x [ 1 : 4 ])  @ NLobjective ( m ,   Min ,   sum { x [ i ] ^ 4 ,   i = 1 : 4 })  nlp   =   MathProgNLPModel ( m )  x0   =   [ 1.0 ;   0.5 ;   0.25 ;   0.125 ]  grad ( nlp ,   x0 )   4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125", 
            "title": "Example"
        }, 
        {
            "location": "/models/#list-of-implemented-functions_1", 
            "text": "cons ,  cons! ,  grad ,  grad! ,  hess ,  hess_coord ,  hprod ,  hprod! ,  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod! ,  obj", 
            "title": "List of implemented functions"
        }, 
        {
            "location": "/models/#simplenlpmodel", 
            "text": "#  NLPModels.SimpleNLPModel     Type .  SimpleNLPModel is an AbstractNLPModel that uses only user-defined functions. In this interface, the objective function $f$ and an initial estimate are required. If the user wants to use derivatives, they need to be passed. The same goes for the Hessian and Hessian-Vector product. For constraints, $c:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$  and the vectors $c_L$ and $c_U$ also need to be passed. Bounds on the variables and an inital estimate to the Lagrangian multipliers can also be provided. The user can also pass the Jacobian and the Lagrangian Hessian and Hessian-Vector product.  SimpleNLPModel(f, x0; lvar = [-\u221e,\u2026,-\u221e], uvar = [\u221e,\u2026,\u221e], y0=zeros,\n  lcon = [-\u221e,\u2026,-\u221e], ucon = [\u221e,\u2026,\u221e], name =  Generic ,\n  [list of functions])   f :: Function  - The objective function $f$;  x0 :: Vector  - The initial point of the problem;  lvar :: Vector  - $\\ell$, the lower bound of the variables;  uvar :: Vector  - $u$, the upper bound of the variables;  y0 :: Vector  - The initial value of the Lagrangian estimates;  lcon :: Vector  - $c_L$, the lower bounds of the constraints function;  ucon :: Vector  - $c_U$, the upper bounds of the constraints function;  name :: String  - A name for the model.   All functions passed have a direct correlation with a NLP function. You don't have to define any more than you need, but calling an undefined function will throw a  NotImplementedError . The list is   g  and  g! : $\\nabla f(x)$, the gradient of the objective function;     see  grad .   gx = g(x)\ngx = g!(x, gx)   H : The lower triangle of the Hessian of the objective function or of the     Lagrangian;     see  hess .   Hx = H(x; obj_weight=1.0) # if the problem is unconstrained\nHx = H(x; obj_weight=1.0, y=zeros) # if the problem is constrained   Hcoord  - The lower triangle of the Hessian of the objective function     or of the Lagrangian, in triplet format;     see  hess_coord .   (rows,cols,vals) = Hcoord(x; obj_weight=1.0) # if the problem is unconstrained\n(rows,cols,vals) = Hcoord(x; obj_weight=1.0, y=zeros) # if the problem is constrained   Hp  and  Hp!  - The product of the Hessian of the objective function or of     the Lagrangian by a vector;     see  hprod .   Hv = Hp(x, v, obj_weight=1.0) # if the problem is unconstrained\nHv = Hp!(x, v, Hv, obj_weight=1.0) # if the problem is unconstrained\nHv = Hp(x, v, obj_weight=1.0, y=zeros) # if the problem is constrained\nHv = Hp!(x, v, Hv, obj_weight=1.0, y=zeros) # if the problem is constrained   c  and  c!  - $c(x)$, the constraints function;     see  cons .   cx = c(x)\ncx = c!(x, cx)   J  - $J(x)$, the Jacobian of the constraints;     see  jac .   Jx = J(x)   Jcoord  - $J(x)$, the Jacobian of the constraints, in triplet format;     see  jac_coord .   (rows,cols,vals) = Jcoord(x)   Jp  and  Jp!  - The Jacobian-vector product;     see  jprod .   Jv = Jp(x, v)\nJv = Jp!(x, v, Jv)   Jtp  and  Jtp!  - The Jacobian-transposed-vector product;     see  jtprod .   Jtv = Jtp(x, v)\nJtv = Jtp!(x, v, Jtv)  For contrained problems, the function $c$ is required, and it must return an array even when m = 1, and $c_L$ and $c_U$ should be passed, otherwise the problem is ill-formed. For equality constraints, the corresponding index of $c_L$ and $c_U$ should be the same.", 
            "title": "SimpleNLPModel"
        }, 
        {
            "location": "/models/#example_2", 
            "text": "using   NLPModels  f ( x )   =   sum ( x .^ 4 )  g ( x )   =   4 * x .^ 3  x   =   [ 1.0 ;   0.5 ;   0.25 ;   0.125 ]  nlp   =   SimpleNLPModel ( f ,   x ,   g = g )  grad ( nlp ,   x )   4-element Array{Float64,1}:\n 4.0\n 0.5\n 0.0625\n 0.0078125", 
            "title": "Example"
        }, 
        {
            "location": "/models/#list-of-implemented-functions_2", 
            "text": "cons ,  cons! ,  grad ,  grad! ,  hess ,  hess_coord ,  hprod ,  hprod! ,  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod! ,  obj", 
            "title": "List of implemented functions"
        }, 
        {
            "location": "/models/#slackmodel", 
            "text": "#  NLPModels.SlackModel     Type .  A model whose only inequality constraints are bounds.  Given a model, this type represents a second model in which slack variables are introduced so as to convert linear and nonlinear inequality constraints to equality constraints and bounds. More precisely, if the original model has the form    \\min f(x)  \\mbox{ s. t. }  c_L \\leq c(x) \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u,    the new model appears to the user as    \\min f(X)  \\mbox{ s. t. }  g(X) = 0 \\mbox{ and } L \\leq X \\leq U.    The unknowns $X = (x, s)$ contain the original variables and slack variables $s$. The latter are such that the new model has the general form    \\min f(x)  \\mbox{ s. t. }  c(x) - s = 0, c_L \\leq s \\leq c_U \\mbox{ and } \\ell \\leq x \\leq u,    although no slack variables are introduced for equality constraints.  The slack variables are implicitly ordered as [s(low), s(upp), s(rng)], where  low ,  upp  and  rng  represent the indices of the constraints of the form $c_L \\leq c(x)   \\infty$, $-\\infty   c(x) \\leq c_U$ and $c_L \\leq c(x) \\leq c_U$, respectively.", 
            "title": "SlackModel"
        }, 
        {
            "location": "/models/#example_3", 
            "text": "using   NLPModels  f ( x )   =   x [ 1 ] ^ 2   +   4 x [ 2 ] ^ 2  c ( x )   =   [ x [ 1 ] * x [ 2 ]   -   1 ]  x   =   [ 2.0 ;   2.0 ]  nlp   =   ADNLPModel ( f ,   x ,   c = c ,   lcon = [ 0.0 ])  nlp_slack   =   SlackModel ( nlp )  nlp_slack . meta . lvar   3-element Array{Float64,1}:\n -Inf\n -Inf\n    0.0", 
            "title": "Example"
        }, 
        {
            "location": "/models/#list-of-implemented-functions_3", 
            "text": "cons ,  cons! ,  grad ,  grad! ,  hess ,  hess_coord ,  hprod ,  hprod! ,  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod! ,  obj ,  reset!", 
            "title": "List of implemented functions"
        }, 
        {
            "location": "/tutorial/", 
            "text": "Tutorial\n\n\nNLPModels.jl was created for two purposes:\n\n\n\n\nAllow users to access problem databases in an unified way.  Mainly, this means  \nCUTEst.jl\n,  but it also gives access to \nAMPL  problems\n,  as well as JuMP defined problems (e.g. as in  \nOptimizationProblems.jl\n).\n\n\nAllow users to create their own problems in the same way.  As a consequence, optimization methods designed according to the NLPModels API  will accept NLPModels of any provenance.  See, for instance,  \nOptimize.jl\n.\n\n\n\n\nThe main interfaces for user defined problems are\n\n\n\n\nADNLPModel\n, which defines a model easily, using automatic   differentiation.\n\n\nSimpleNLPModel\n, which allows users to handle all functions themselves,   giving\n\n\n\n\n\n\nADNLPModel Tutorial\n\n\nADNLPModel is simple to use and is useful for classrooms. It only needs the objective function $f$ and a starting point $x^0$ to be well-defined. For constrained problems, you'll also need the constraints function $c$, and the constraints vectors $c_L$ and $c_U$, such that $c_L \\leq c(x) \\leq c_U$. Equality constraints will be automatically identified as those indices $i$ for which $c_{L_i} = c_{U_i}$.\n\n\nLet's define the famous Rosenbrock function \n\\begin{align*} f(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2, \\end{align*}\n with starting point $x^0 = (-1.2,1.0)$.\n\n\nusing\n \nNLPModels\n\n\n\nnlp\n \n=\n \nADNLPModel\n(\nx\n-\n(\nx\n[\n1\n]\n \n-\n \n1.0\n)\n^\n2\n \n+\n \n100\n*\n(\nx\n[\n2\n]\n \n-\n \nx\n[\n1\n]\n^\n2\n)\n^\n2\n \n,\n \n[\n-\n1.2\n;\n \n1.0\n])\n\n\n\n\n\n\nNLPModels.ADNLPModel(Minimization problem Generic\nnvar = 2, ncon = 0 (0 linear)\n,NLPModels.Counters(0,0,0,0,0,0,0,0,0,0,0),(anonymous function),(anonymous function))\n\n\n\n\n\nThis is enough to define the model. Let's get the objective function value at $x^0$, using only \nnlp\n.\n\n\nfx\n \n=\n \nobj\n(\nnlp\n,\n \nnlp\n.\nmeta\n.\nx0\n)\n\n\nprintln\n(\nfx = \n$fx\n)\n\n\n\n\n\n\nfx = 24.199999999999996\n\n\n\n\n\nDone. Let's try the gradient and Hessian.\n\n\ngx\n \n=\n \ngrad\n(\nnlp\n,\n \nnlp\n.\nmeta\n.\nx0\n)\n\n\nHx\n \n=\n \nhess\n(\nnlp\n,\n \nnlp\n.\nmeta\n.\nx0\n)\n\n\nprintln\n(\ngx = \n$gx\n)\n\n\nprintln\n(\nHx = \n$Hx\n)\n\n\n\n\n\n\ngx = [-215.59999999999997,-87.99999999999999]\nHx = [1330.0 0.0\n 480.0 200.0]\n\n\n\n\n\nNotice how only the lower triangle of the Hessian is stored. Also notice that it is \ndense\n. This is a current limitation of this model. It doesn't return sparse matrices, so use it with care.\n\n\nLet's do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method\n\n\n\n\nGiven $x^0$, $\\varepsilon \n 0$, and $\\eta \\in (0,1)$. Set $k = 0$;\n\n\nIf $\\Vert \\nabla f(x^k) \\Vert \n \\varepsilon$ STOP with $x^* = x^k$;\n\n\nCompute $d^k = -\\nabla f(x^k)$;\n\n\nCompute $\\alpha_k \\in (0,1]$ such that $ f(x^k + \\alpha_kd^k) \n f(x^k) + \\alpha_k\\eta \\nabla f(x^k)^Td^k $\n\n\nDefine $x^{k+1} = x^k + \\alpha_kx^k$\n\n\nUpdate $k = k + 1$ and go to step 2.\n\n\n\n\nfunction\n steepest\n(\nnlp\n;\n \nitmax\n=\n100000\n,\n \neta\n=\n1e-4\n,\n \neps\n=\n1e-6\n,\n \nsigma\n=\n0.66\n)\n\n  \nx\n \n=\n \nnlp\n.\nmeta\n.\nx0\n\n  \nfx\n \n=\n \nobj\n(\nnlp\n,\n \nx\n)\n\n  \n\u2207fx\n \n=\n \ngrad\n(\nnlp\n,\n \nx\n)\n\n  \nslope\n \n=\n \ndot\n(\n\u2207fx\n,\n \n\u2207fx\n)\n\n  \n\u2207f_norm\n \n=\n \nsqrt\n(\nslope\n)\n\n  \niter\n \n=\n \n0\n\n  \nwhile\n \n\u2207f_norm\n \n \neps\n \n \niter\n \n \nitmax\n\n    \nt\n \n=\n \n1.0\n\n    \nx_trial\n \n=\n \nx\n \n-\n \nt\n \n*\n \n\u2207fx\n\n    \nf_trial\n \n=\n \nobj\n(\nnlp\n,\n \nx_trial\n)\n\n    \nwhile\n \nobj\n(\nnlp\n,\n \nx\n \n-\n \nt\n*\n\u2207fx\n)\n \n \nfx\n \n-\n \neta\n \n*\n \nt\n \n*\n \nslope\n\n      \nt\n \n*=\n \nsigma\n\n      \nx_trial\n \n=\n \nx\n \n-\n \nt\n \n*\n \n\u2207fx\n\n      \nf_trial\n \n=\n \nobj\n(\nnlp\n,\n \nx_trial\n)\n\n    \nend\n\n    \nx\n \n=\n \nx_trial\n\n    \nfx\n \n=\n \nf_trial\n\n    \n\u2207fx\n \n=\n \ngrad\n(\nnlp\n,\n \nx\n)\n\n    \nslope\n \n=\n \ndot\n(\n\u2207fx\n,\n \n\u2207fx\n)\n\n    \n\u2207f_norm\n \n=\n \nsqrt\n(\nslope\n)\n\n    \niter\n \n+=\n \n1\n\n  \nend\n\n  \noptimal\n \n=\n \n\u2207f_norm\n \n=\n \neps\n\n  \nreturn\n \nx\n,\n \nfx\n,\n \n\u2207f_norm\n,\n \noptimal\n,\n \niter\n\n\nend\n\n\n\nx\n,\n \nfx\n,\n \nngx\n,\n \noptimal\n,\n \niter\n \n=\n \nsteepest\n(\nnlp\n)\n\n\nprintln\n(\nx = \n$x\n)\n\n\nprintln\n(\nfx = \n$fx\n)\n\n\nprintln\n(\nngx = \n$ngx\n)\n\n\nprintln\n(\noptimal = \n$optimal\n)\n\n\nprintln\n(\niter = \n$iter\n)\n\n\n\n\n\n\nx = [1.0000006499501406,1.0000013043156974]\nfx = 4.2438440239813445e-13\nngx = 9.984661274466946e-7\noptimal = true\n\n\n\n\n\nMaybe this code is too complicated? If you're in a class you just want to show a Newton step.\n\n\ng\n(\nx\n)\n \n=\n \ngrad\n(\nnlp\n,\n \nx\n)\n\n\nH\n(\nx\n)\n \n=\n \nhess\n(\nnlp\n,\n \nx\n)\n \n+\n \ntriu\n(\nhess\n(\nnlp\n,\n \nx\n)\n,\n \n1\n)\n\n\nx\n \n=\n \nnlp\n.\nmeta\n.\nx0\n\n\nd\n \n=\n \n-\nH\n(\nx\n)\n\\\ng\n(\nx\n)\n\n\n\n\n\n\n2-element Array{Float64,1}:\n 0.0247191\n 0.380674\n\n\n\n\n\nor a few\n\n\nfor\n \ni\n \n=\n \n1\n:\n5\n\n  \nx\n \n=\n \nx\n \n-\n \nH\n(\nx\n)\n\\\ng\n(\nx\n)\n\n  \nprintln\n(\nx = \n$x\n)\n\n\nend\n\n\n\n\n\n\nx = [-1.1752808988764043,1.3806741573033705]\nx = [0.763114871176745,-3.1750338547488415]\nx = [0.7634296788843487,0.5828247754975662]\nx = [0.9999953110849514,0.9440273238534179]\nx = [0.9999956956536327,0.999991391325645]\n\n\n\n\n\nAlso, notice how we can reuse the method.\n\n\nf\n(\nx\n)\n \n=\n \n(\nx\n[\n1\n]\n^\n2\n \n+\n \nx\n[\n2\n]\n^\n2\n \n-\n \n4\n)\n^\n2\n \n+\n \n(\nx\n[\n1\n]\n*\nx\n[\n2\n]\n \n-\n \n1\n)\n^\n2\n\n\nx0\n \n=\n \n[\n2.0\n;\n \n1.0\n]\n\n\nnlp\n \n=\n \nADNLPModel\n(\nf\n,\n \nx0\n)\n\n\n\nx\n,\n \nfx\n,\n \nngx\n,\n \noptimal\n,\n \niter\n \n=\n \nsteepest\n(\nnlp\n)\n\n\n\n\n\n\n([1.9318516779143746,0.5176381004152917],1.2842480268876117e-14,9.402834701778815e-7,true,120)\n\n\n\n\n\nEven using a different model.\n\n\nusing\n \nOptimizationProblems\n \n# Defines a lot of JuMP models\n\n\n\nnlp\n \n=\n \nMathProgNLPModel\n(\nwoods\n())\n\n\nx\n,\n \nfx\n,\n \nngx\n,\n \noptimal\n,\n \niter\n \n=\n \nsteepest\n(\nnlp\n)\n\n\nprintln\n(\nfx = \n$fx\n)\n\n\nprintln\n(\nngx = \n$ngx\n)\n\n\nprintln\n(\noptimal = \n$optimal\n)\n\n\nprintln\n(\niter = \n$iter\n)\n\n\n\n\n\n\nfx = 1.0000000000002167\nngx = 9.893253859340887e-7\noptimal = true\niter = 12016\n\n\n\n\n\nFor constrained minimization, you need the constraints vector and bounds too. Bounds on the variables can be passed through a new vector.\n\n\nf\n(\nx\n)\n \n=\n \n(\nx\n[\n1\n]\n \n-\n \n1.0\n)\n^\n2\n \n+\n \n100\n*\n(\nx\n[\n2\n]\n \n-\n \nx\n[\n1\n]\n^\n2\n)\n^\n2\n\n\nx0\n \n=\n \n[\n-\n1.2\n;\n \n1.0\n]\n\n\nlvar\n \n=\n \n[\n-\nInf\n;\n \n0.1\n]\n\n\nuvar\n \n=\n \n[\n0.5\n;\n \n0.5\n]\n\n\nc\n(\nx\n)\n \n=\n \n[\nx\n[\n1\n]\n \n+\n \nx\n[\n2\n]\n \n-\n \n2\n;\n \nx\n[\n1\n]\n^\n2\n \n+\n \nx\n[\n2\n]\n^\n2\n]\n\n\nlcon\n \n=\n \n[\n0.0\n;\n \n-\nInf\n]\n\n\nucon\n \n=\n \n[\nInf\n;\n \n1.0\n]\n\n\nnlp\n \n=\n \nADNLPModel\n(\nf\n,\n \nx0\n,\n \nc\n=\nc\n,\n \nlvar\n=\nlvar\n,\n \nuvar\n=\nuvar\n,\n \nlcon\n=\nlcon\n,\n \nucon\n=\nucon\n)\n\n\n\nprintln\n(\ncx = \n$(cons(nlp, nlp.meta.x0))\n)\n\n\nprintln\n(\nJx = \n$(jac(nlp, nlp.meta.x0))\n)\n\n\n\n\n\n\ncx = [-2.2,2.44]\nJx = [1.0 1.0\n -2.4 2.0]\n\n\n\n\n\n\n\nSimpleNLPModel Tutorial\n\n\nSimpleNLPModel allows you to pass every single function of the model. On the other hand, it doesn't handle anything else. Calling an undefined function will throw a \nNotImplementedError\n. Only the objective function is mandatory (if you don't need it, pass \nx-\n0\n).\n\n\nusing\n \nNLPModels\n\n\n\nf\n(\nx\n)\n \n=\n \n(\nx\n[\n1\n]\n \n-\n \n1.0\n)\n^\n2\n \n+\n \n4\n*\n(\nx\n[\n2\n]\n \n-\n \n1.0\n)\n^\n2\n\n\nx0\n \n=\n \nzeros\n(\n2\n)\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx0\n)\n\n\n\nfx\n \n=\n \nobj\n(\nnlp\n,\n \nnlp\n.\nmeta\n.\nx0\n)\n\n\nprintln\n(\nfx = \n$fx\n)\n\n\n\n# grad(nlp, nlp.meta.x0) # This is undefined\n\n\n\n\n\n\nfx = 5.0\n\n\n\n\n\ng\n(\nx\n)\n \n=\n \n[\n2\n*\n(\nx\n[\n1\n]\n \n-\n \n1.0\n);\n \n8\n*\n(\nx\n[\n2\n]\n \n-\n \n1.0\n)]\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx0\n,\n \ng\n=\ng\n)\n\n\n\ngrad\n(\nnlp\n,\n \nnlp\n.\nmeta\n.\nx0\n)\n\n\n\n\n\n\n2-element Array{Float64,1}:\n -2.0\n -8.0\n\n\n\n\n\n\"But what's to stop me from defining \ng\n however I want?\" Nothing. So you have to be careful on how you're defining it. You should probably check your derivatives. If the function is simply defined, you can try using automatic differentiation. Alternatively, you can use the \nderivative checker\n.\n\n\ngradient_check\n(\nnlp\n)\n\n\n\n\n\n\nDict{Int64,Float64} with 0 entries\n\n\n\n\n\ngwrong\n(\nx\n)\n \n=\n \n[\n2\n*\n(\nx\n[\n1\n]\n \n-\n \n1.0\n);\n \n8\n*\nx\n[\n2\n]\n \n-\n \n1.0\n]\n \n# Find the error\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx0\n,\n \ng\n=\ngwrong\n)\n\n\ngradient_check\n(\nnlp\n)\n\n\n\n\n\n\nDict{Int64,Float64} with 1 entry:\n  2 =\n 6.999999999951384\n\n\n\n\n\nFor constrained problems, we still need the constraints function, \nlcon\n and \nucon\n. Also, let's pass the Jacobian-vector product.\n\n\nc\n(\nx\n)\n \n=\n \n[\nx\n[\n1\n]\n^\n2\n \n+\n \nx\n[\n2\n]\n^\n2\n;\n \nx\n[\n1\n]\n*\nx\n[\n2\n]\n \n-\n \n1\n]\n\n\nlcon\n \n=\n \n[\n1.0\n;\n \n0.0\n]\n\n\nucon\n \n=\n \n[\n4.0\n;\n \n0.0\n]\n\n\nJacprod\n(\nx\n,\n \nv\n)\n \n=\n \n[\n2\n*\nx\n[\n1\n]\n*\nv\n[\n1\n]\n \n+\n \n2\n*\nx\n[\n2\n]\n*\nv\n[\n2\n];\n \nx\n[\n2\n]\n*\nv\n[\n1\n]\n \n+\n \nx\n[\n1\n]\n*\nv\n[\n2\n]]\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx0\n,\n \nc\n=\nc\n,\n \nlcon\n=\nlcon\n,\n \nucon\n=\nucon\n,\n \ng\n=\ng\n,\n \nJp\n=\nJacprod\n)\n\n\njprod\n(\nnlp\n,\n \nones\n(\n2\n),\n \nones\n(\n2\n))\n\n\n\n\n\n\n2-element Array{Float64,1}:\n 4.0\n 2.0\n\n\n\n\n\nFurthermore, NLPModels also works with inplace operations. Since some models do not take full advantage of this (like ADNLPModel), a user might want to define his/her own functions that do.\n\n\nf\n(\nx\n)\n \n=\n \n(\nx\n[\n1\n]\n \n-\n \n1.0\n)\n^\n2\n \n+\n \n4\n*\n(\nx\n[\n2\n]\n \n-\n \n1.0\n)\n^\n2\n\n\nx0\n \n=\n \nzeros\n(\n2\n)\n\n\ng!\n(\nx\n,\n \ngx\n)\n \n=\n \nbegin\n\n  \ngx\n[\n1\n]\n \n=\n \n2\n*\n(\nx\n[\n1\n]\n \n-\n \n1.0\n)\n\n  \ngx\n[\n2\n]\n \n=\n \n8\n*\n(\nx\n[\n2\n]\n \n=\n \n1.0\n)\n\n  \nreturn\n \ngx\n\n\nend\n\n\nnlp\n \n=\n \nSimpleNLPModel\n(\nf\n,\n \nx0\n,\n \ng!\n \n=\ng!\n)\n \n# Watchout, g!=g! is interpreted as g != g!\n\n\ngx\n \n=\n \nzeros\n(\n2\n)\n\n\ngrad!\n(\nnlp\n,\n \nnlp\n.\nmeta\n.\nx0\n,\n \ngx\n)\n\n\n\n\n\n\n2-element Array{Float64,1}:\n -2.0\n  8.0", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorial/#tutorial", 
            "text": "NLPModels.jl was created for two purposes:   Allow users to access problem databases in an unified way.  Mainly, this means   CUTEst.jl ,  but it also gives access to  AMPL  problems ,  as well as JuMP defined problems (e.g. as in   OptimizationProblems.jl ).  Allow users to create their own problems in the same way.  As a consequence, optimization methods designed according to the NLPModels API  will accept NLPModels of any provenance.  See, for instance,   Optimize.jl .   The main interfaces for user defined problems are   ADNLPModel , which defines a model easily, using automatic   differentiation.  SimpleNLPModel , which allows users to handle all functions themselves,   giving", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorial/#adnlpmodel-tutorial", 
            "text": "ADNLPModel is simple to use and is useful for classrooms. It only needs the objective function $f$ and a starting point $x^0$ to be well-defined. For constrained problems, you'll also need the constraints function $c$, and the constraints vectors $c_L$ and $c_U$, such that $c_L \\leq c(x) \\leq c_U$. Equality constraints will be automatically identified as those indices $i$ for which $c_{L_i} = c_{U_i}$.  Let's define the famous Rosenbrock function  \\begin{align*} f(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2, \\end{align*}  with starting point $x^0 = (-1.2,1.0)$.  using   NLPModels  nlp   =   ADNLPModel ( x - ( x [ 1 ]   -   1.0 ) ^ 2   +   100 * ( x [ 2 ]   -   x [ 1 ] ^ 2 ) ^ 2   ,   [ - 1.2 ;   1.0 ])   NLPModels.ADNLPModel(Minimization problem Generic\nnvar = 2, ncon = 0 (0 linear)\n,NLPModels.Counters(0,0,0,0,0,0,0,0,0,0,0),(anonymous function),(anonymous function))  This is enough to define the model. Let's get the objective function value at $x^0$, using only  nlp .  fx   =   obj ( nlp ,   nlp . meta . x0 )  println ( fx =  $fx )   fx = 24.199999999999996  Done. Let's try the gradient and Hessian.  gx   =   grad ( nlp ,   nlp . meta . x0 )  Hx   =   hess ( nlp ,   nlp . meta . x0 )  println ( gx =  $gx )  println ( Hx =  $Hx )   gx = [-215.59999999999997,-87.99999999999999]\nHx = [1330.0 0.0\n 480.0 200.0]  Notice how only the lower triangle of the Hessian is stored. Also notice that it is  dense . This is a current limitation of this model. It doesn't return sparse matrices, so use it with care.  Let's do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method   Given $x^0$, $\\varepsilon   0$, and $\\eta \\in (0,1)$. Set $k = 0$;  If $\\Vert \\nabla f(x^k) \\Vert   \\varepsilon$ STOP with $x^* = x^k$;  Compute $d^k = -\\nabla f(x^k)$;  Compute $\\alpha_k \\in (0,1]$ such that $ f(x^k + \\alpha_kd^k)   f(x^k) + \\alpha_k\\eta \\nabla f(x^k)^Td^k $  Define $x^{k+1} = x^k + \\alpha_kx^k$  Update $k = k + 1$ and go to step 2.   function  steepest ( nlp ;   itmax = 100000 ,   eta = 1e-4 ,   eps = 1e-6 ,   sigma = 0.66 ) \n   x   =   nlp . meta . x0 \n   fx   =   obj ( nlp ,   x ) \n   \u2207fx   =   grad ( nlp ,   x ) \n   slope   =   dot ( \u2207fx ,   \u2207fx ) \n   \u2207f_norm   =   sqrt ( slope ) \n   iter   =   0 \n   while   \u2207f_norm     eps     iter     itmax \n     t   =   1.0 \n     x_trial   =   x   -   t   *   \u2207fx \n     f_trial   =   obj ( nlp ,   x_trial ) \n     while   obj ( nlp ,   x   -   t * \u2207fx )     fx   -   eta   *   t   *   slope \n       t   *=   sigma \n       x_trial   =   x   -   t   *   \u2207fx \n       f_trial   =   obj ( nlp ,   x_trial ) \n     end \n     x   =   x_trial \n     fx   =   f_trial \n     \u2207fx   =   grad ( nlp ,   x ) \n     slope   =   dot ( \u2207fx ,   \u2207fx ) \n     \u2207f_norm   =   sqrt ( slope ) \n     iter   +=   1 \n   end \n   optimal   =   \u2207f_norm   =   eps \n   return   x ,   fx ,   \u2207f_norm ,   optimal ,   iter  end  x ,   fx ,   ngx ,   optimal ,   iter   =   steepest ( nlp )  println ( x =  $x )  println ( fx =  $fx )  println ( ngx =  $ngx )  println ( optimal =  $optimal )  println ( iter =  $iter )   x = [1.0000006499501406,1.0000013043156974]\nfx = 4.2438440239813445e-13\nngx = 9.984661274466946e-7\noptimal = true  Maybe this code is too complicated? If you're in a class you just want to show a Newton step.  g ( x )   =   grad ( nlp ,   x )  H ( x )   =   hess ( nlp ,   x )   +   triu ( hess ( nlp ,   x ) ,   1 )  x   =   nlp . meta . x0  d   =   - H ( x ) \\ g ( x )   2-element Array{Float64,1}:\n 0.0247191\n 0.380674  or a few  for   i   =   1 : 5 \n   x   =   x   -   H ( x ) \\ g ( x ) \n   println ( x =  $x )  end   x = [-1.1752808988764043,1.3806741573033705]\nx = [0.763114871176745,-3.1750338547488415]\nx = [0.7634296788843487,0.5828247754975662]\nx = [0.9999953110849514,0.9440273238534179]\nx = [0.9999956956536327,0.999991391325645]  Also, notice how we can reuse the method.  f ( x )   =   ( x [ 1 ] ^ 2   +   x [ 2 ] ^ 2   -   4 ) ^ 2   +   ( x [ 1 ] * x [ 2 ]   -   1 ) ^ 2  x0   =   [ 2.0 ;   1.0 ]  nlp   =   ADNLPModel ( f ,   x0 )  x ,   fx ,   ngx ,   optimal ,   iter   =   steepest ( nlp )   ([1.9318516779143746,0.5176381004152917],1.2842480268876117e-14,9.402834701778815e-7,true,120)  Even using a different model.  using   OptimizationProblems   # Defines a lot of JuMP models  nlp   =   MathProgNLPModel ( woods ())  x ,   fx ,   ngx ,   optimal ,   iter   =   steepest ( nlp )  println ( fx =  $fx )  println ( ngx =  $ngx )  println ( optimal =  $optimal )  println ( iter =  $iter )   fx = 1.0000000000002167\nngx = 9.893253859340887e-7\noptimal = true\niter = 12016  For constrained minimization, you need the constraints vector and bounds too. Bounds on the variables can be passed through a new vector.  f ( x )   =   ( x [ 1 ]   -   1.0 ) ^ 2   +   100 * ( x [ 2 ]   -   x [ 1 ] ^ 2 ) ^ 2  x0   =   [ - 1.2 ;   1.0 ]  lvar   =   [ - Inf ;   0.1 ]  uvar   =   [ 0.5 ;   0.5 ]  c ( x )   =   [ x [ 1 ]   +   x [ 2 ]   -   2 ;   x [ 1 ] ^ 2   +   x [ 2 ] ^ 2 ]  lcon   =   [ 0.0 ;   - Inf ]  ucon   =   [ Inf ;   1.0 ]  nlp   =   ADNLPModel ( f ,   x0 ,   c = c ,   lvar = lvar ,   uvar = uvar ,   lcon = lcon ,   ucon = ucon )  println ( cx =  $(cons(nlp, nlp.meta.x0)) )  println ( Jx =  $(jac(nlp, nlp.meta.x0)) )   cx = [-2.2,2.44]\nJx = [1.0 1.0\n -2.4 2.0]", 
            "title": "ADNLPModel Tutorial"
        }, 
        {
            "location": "/tutorial/#simplenlpmodel-tutorial", 
            "text": "SimpleNLPModel allows you to pass every single function of the model. On the other hand, it doesn't handle anything else. Calling an undefined function will throw a  NotImplementedError . Only the objective function is mandatory (if you don't need it, pass  x- 0 ).  using   NLPModels  f ( x )   =   ( x [ 1 ]   -   1.0 ) ^ 2   +   4 * ( x [ 2 ]   -   1.0 ) ^ 2  x0   =   zeros ( 2 )  nlp   =   SimpleNLPModel ( f ,   x0 )  fx   =   obj ( nlp ,   nlp . meta . x0 )  println ( fx =  $fx )  # grad(nlp, nlp.meta.x0) # This is undefined   fx = 5.0  g ( x )   =   [ 2 * ( x [ 1 ]   -   1.0 );   8 * ( x [ 2 ]   -   1.0 )]  nlp   =   SimpleNLPModel ( f ,   x0 ,   g = g )  grad ( nlp ,   nlp . meta . x0 )   2-element Array{Float64,1}:\n -2.0\n -8.0  \"But what's to stop me from defining  g  however I want?\" Nothing. So you have to be careful on how you're defining it. You should probably check your derivatives. If the function is simply defined, you can try using automatic differentiation. Alternatively, you can use the  derivative checker .  gradient_check ( nlp )   Dict{Int64,Float64} with 0 entries  gwrong ( x )   =   [ 2 * ( x [ 1 ]   -   1.0 );   8 * x [ 2 ]   -   1.0 ]   # Find the error  nlp   =   SimpleNLPModel ( f ,   x0 ,   g = gwrong )  gradient_check ( nlp )   Dict{Int64,Float64} with 1 entry:\n  2 =  6.999999999951384  For constrained problems, we still need the constraints function,  lcon  and  ucon . Also, let's pass the Jacobian-vector product.  c ( x )   =   [ x [ 1 ] ^ 2   +   x [ 2 ] ^ 2 ;   x [ 1 ] * x [ 2 ]   -   1 ]  lcon   =   [ 1.0 ;   0.0 ]  ucon   =   [ 4.0 ;   0.0 ]  Jacprod ( x ,   v )   =   [ 2 * x [ 1 ] * v [ 1 ]   +   2 * x [ 2 ] * v [ 2 ];   x [ 2 ] * v [ 1 ]   +   x [ 1 ] * v [ 2 ]]  nlp   =   SimpleNLPModel ( f ,   x0 ,   c = c ,   lcon = lcon ,   ucon = ucon ,   g = g ,   Jp = Jacprod )  jprod ( nlp ,   ones ( 2 ),   ones ( 2 ))   2-element Array{Float64,1}:\n 4.0\n 2.0  Furthermore, NLPModels also works with inplace operations. Since some models do not take full advantage of this (like ADNLPModel), a user might want to define his/her own functions that do.  f ( x )   =   ( x [ 1 ]   -   1.0 ) ^ 2   +   4 * ( x [ 2 ]   -   1.0 ) ^ 2  x0   =   zeros ( 2 )  g! ( x ,   gx )   =   begin \n   gx [ 1 ]   =   2 * ( x [ 1 ]   -   1.0 ) \n   gx [ 2 ]   =   8 * ( x [ 2 ]   =   1.0 ) \n   return   gx  end  nlp   =   SimpleNLPModel ( f ,   x0 ,   g!   = g! )   # Watchout, g!=g! is interpreted as g != g!  gx   =   zeros ( 2 )  grad! ( nlp ,   nlp . meta . x0 ,   gx )   2-element Array{Float64,1}:\n -2.0\n  8.0", 
            "title": "SimpleNLPModel Tutorial"
        }, 
        {
            "location": "/api/", 
            "text": "API\n\n\nAs stated in the \nhome\n page, we consider the nonlinear optimization problem in the following format: \n\\begin{align*} \\min \\quad & f(x) \\\\\n& c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u. \\end{align*}\n To develop an optimization algorithm, we are usually worried not only with $f(x)$ and $c(x)$, but also with their derivatives. Namely,\n\n\n\n\n$\\nabla f(x)$, the gradient of $f$ at the point $x$;\n\n\n$\\nabla^2 f(x)$, the Hessian of $f$ at the point $x$;\n\n\n$J(x) = \\nabla c(x)$, the Jacobian of $c$ at the point $x$;\n\n\n$\\nabla^2 f(x) + \\sum_{i=1}^m \\lambda_i \\nabla^2 c_i(x)$,   the Hessian of the Lagrangian function at the point $(x,\\lambda)$.\n\n\n\n\nThere are many ways to access some of these values, so here is a little reference guide.\n\n\n\n\nReference guide\n\n\nThe following naming should be easy enough to follow. If not, click on the link and go to the description.\n\n\n\n\n!\n means inplace;\n\n\n_coord\n means coordinate format;\n\n\nprod\n means matrix-vector product;\n\n\n_op\n means operator (as in \nLinearOperators.jl\n).\n\n\n\n\nFeel free to open an issue to suggest other methods that should apply to all NLPModels instances.\n\n\n\n\n\n\n\n\nFunction\n\n\nNLPModels function\n\n\n\n\n\n\n\n\n\n\n$f(x)$\n\n\nobj\n\n\n\n\n\n\n$\\nabla f(x)$\n\n\ngrad\n, \ngrad!\n\n\n\n\n\n\n$\\nabla^2 f(x)$\n\n\nhess\n, \nhess_op\n, \nhess_coord\n, \nhprod\n, \nhprod!\n\n\n\n\n\n\n$c(x)$\n\n\ncons\n, \ncons!\n\n\n\n\n\n\n$J(x)$\n\n\njac\n, \njac_coord\n, \njprod\n, \njprod!\n, \njtprod\n, \njtprod!\n\n\n\n\n\n\n$\\nabla^2 L(x,y)$\n\n\nhess\n, \nhess_op\n, \nhess_coord\n, \nhprod\n, \nhprod!\n\n\n\n\n\n\n\n\n\n\nAbstractNLPModel functions\n\n\n#\n\n\nNLPModels.obj\n \n \nFunction\n.\n\n\nobj(nlp, x)\n\n\nEvaluate $f(x)$, the objective function of \nnlp\n at \nx\n.\n\n\n#\n\n\nNLPModels.grad\n \n \nFunction\n.\n\n\ngrad(nlp, x)\n\n\nEvaluate $\\nabla f(x)$, the gradient of the objective function at \nx\n.\n\n\n#\n\n\nNLPModels.grad!\n \n \nFunction\n.\n\n\ngrad!(nlp, x, g)\n\n\nEvaluate $\\nabla f(x)$, the gradient of the objective function at \nx\n in place.\n\n\n#\n\n\nNLPModels.cons\n \n \nFunction\n.\n\n\ncons(nlp, x)\n\n\nEvaluate $c(x)$, the constraints at \nx\n.\n\n\n#\n\n\nNLPModels.cons!\n \n \nFunction\n.\n\n\ncons!(nlp, x, c)\n\n\nEvaluate $c(x)$, the constraints at \nx\n in place.\n\n\n#\n\n\nNLPModels.jac_coord\n \n \nFunction\n.\n\n\n(rows,cols,vals) = jac_coord(nlp, x)\n\n\nEvaluate $\\nabla c(x)$, the constraint's Jacobian at \nx\n in sparse coordinate format.\n\n\n#\n\n\nNLPModels.jac\n \n \nFunction\n.\n\n\nJx = jac(nlp, x)\n\n\nEvaluate $\\nabla c(x)$, the constraint's Jacobian at \nx\n as a sparse matrix.\n\n\n#\n\n\nNLPModels.jprod\n \n \nFunction\n.\n\n\nJv = jprod(nlp, x, v)\n\n\nEvaluate $\\nabla c(x)v$, the Jacobian-vector product at \nx\n.\n\n\n#\n\n\nNLPModels.jprod!\n \n \nFunction\n.\n\n\nJv = jprod!(nlp, x, v, Jv)\n\n\nEvaluate $\\nabla c(x)v$, the Jacobian-vector product at \nx\n in place.\n\n\n#\n\n\nNLPModels.jtprod\n \n \nFunction\n.\n\n\nJtv = jtprod(nlp, x, v, Jtv)\n\n\nEvaluate $\\nabla c(x)^Tv$, the transposed-Jacobian-vector product at \nx\n.\n\n\n#\n\n\nNLPModels.jtprod!\n \n \nFunction\n.\n\n\nJtv = jtprod!(nlp, x, v, Jtv)\n\n\nEvaluate $\\nabla c(x)^Tv$, the transposed-Jacobian-vector product at \nx\n in place.\n\n\n#\n\n\nNLPModels.hess_coord\n \n \nFunction\n.\n\n\n(rows,cols,vals) = hess_coord(nlp, x; obj_weight=1.0, y=zeros)\n\n\nEvaluate the Lagrangian Hessian at \n(x,y)\n in sparse coordinate format, with objective function scaled by \nobj_weight\n, i.e.,\n\n\n\n\n \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x), \n\n\n\n\nwith \u03c3 = obj_weight. Only the lower triangle is returned.\n\n\n#\n\n\nNLPModels.hess\n \n \nFunction\n.\n\n\nHx = hess(nlp, x; obj_weight=1.0, y=zeros)\n\n\nEvaluate the Lagrangian Hessian at \n(x,y)\n as a sparse matrix, with objective function scaled by \nobj_weight\n, i.e.,\n\n\n\n\n \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x), \n\n\n\n\nwith \u03c3 = obj_weight. Only the lower triangle is returned.\n\n\n#\n\n\nNLPModels.hess_op\n \n \nFunction\n.\n\n\nH = hess_op(nlp, x; obj_weight=1.0, y=zeros)\n\n\nReturn the Lagrangian Hessian at \n(x,y)\n with objective function scaled by \nobj_weight\n as a linear operator. The resulting object may be used as if it were a matrix, e.g., \nH * v\n. The linear operator H represents\n\n\n\n\n \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x), \n\n\n\n\nwith \u03c3 = obj_weight.\n\n\n#\n\n\nNLPModels.hprod\n \n \nFunction\n.\n\n\nHv = hprod(nlp, x, v; obj_weight=1.0, y=zeros)\n\n\nEvaluate the product of the Lagrangian Hessian at \n(x,y)\n with the vector \nv\n, with objective function scaled by \nobj_weight\n, i.e.,\n\n\n\n\n \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x), \n\n\n\n\nwith \u03c3 = obj_weight.\n\n\n#\n\n\nNLPModels.hprod!\n \n \nFunction\n.\n\n\nHv = hprod!(nlp, x, v, Hv; obj_weight=1.0, y=zeros)\n\n\nEvaluate the product of the Lagrangian Hessian at \n(x,y)\n with the vector \nv\n in place, with objective function scaled by \nobj_weight\n, i.e.,\n\n\n\n\n \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x), \n\n\n\n\nwith \u03c3 = obj_weight.\n\n\n#\n\n\nNLPModels.NLPtoMPB\n \n \nFunction\n.\n\n\nmp = NLPtoMPB(nlp, solver)\n\n\n\n\n\nReturn a \nMathProgBase\n model corresponding to an \nAbstractNLPModel\n.\n\n\nArguments\n\n\n\n\nnlp::AbstractNLPModel\n\n\nsolver::AbstractMathProgSolver\n a solver instance, e.g., \nIpoptSolver()\n\n\n\n\nCurrently, all models are treated as nonlinear models.\n\n\nReturn values\n\n\nThe function returns a \nMathProgBase\n model \nmpbmodel\n such that it should be possible to call\n\n\nMathProgBase.optimize!(mpbmodel)\n\n\n\n\n\n#\n\n\nLinearOperators.reset!\n \n \nFunction\n.\n\n\nreset!(counters)\n\n\nReset evaluation counters\n\n\n`reset!(nlp)\n\n\nReset evaluation count in \nnlp\n\n\n\n\nDerivative check\n\n\n#\n\n\nNLPModels.gradient_check\n \n \nFunction\n.\n\n\nCheck the first derivatives of the objective at \nx\n against centered finite differences.\n\n\nThis function returns a dictionary indexed by components of the gradient for which the relative error exceeds \nrtol\n.\n\n\n#\n\n\nNLPModels.jacobian_check\n \n \nFunction\n.\n\n\nCheck the first derivatives of the constraints at \nx\n against centered finite differences.\n\n\nThis function returns a dictionary indexed by (j, i) tuples such that the relative error in the \ni\n-th partial derivative of the \nj\n-th constraint exceeds \nrtol\n.\n\n\n#\n\n\nNLPModels.hessian_check\n \n \nFunction\n.\n\n\nCheck the second derivatives of the objective and each constraints at \nx\n against centered finite differences. This check does not rely on exactness of the first derivatives, only on objective and constraint values.\n\n\nThe \nsgn\n arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as\n\n\nL(x,y) = f(x) + \u2211 y\u2c7c c\u2c7c(x)\n\n\n\n\n\ne.g., as in \nJuMPNLPModel\ns, and a negative value if the Lagrangian is formulated as\n\n\nL(x,y) = f(x) - \u2211 y\u2c7c c\u2c7c(x)\n\n\n\n\n\ne.g., as in \nAmplModel\ns. Only the sign of \nsgn\n is important.\n\n\nThis function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k \n 0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative \u2202\u00b2f\u2096/\u2202x\u1d62\u2202x\u2c7c exceeds \nrtol\n.\n\n\n#\n\n\nNLPModels.hessian_check_from_grad\n \n \nFunction\n.\n\n\nCheck the second derivatives of the objective and each constraints at \nx\n against centered finite differences. This check assumes exactness of the first derivatives.\n\n\nThe \nsgn\n arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as\n\n\nL(x,y) = f(x) + \u2211 y\u2c7c c\u2c7c(x)\n\n\n\n\n\ne.g., as in \nJuMPNLPModel\ns, and a negative value if the Lagrangian is formulated as\n\n\nL(x,y) = f(x) - \u2211 y\u2c7c c\u2c7c(x)\n\n\n\n\n\ne.g., as in \nAmplModel\ns. Only the sign of \nsgn\n is important.\n\n\nThis function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k \n 0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative \u2202\u00b2f\u2096/\u2202x\u1d62\u2202x\u2c7c exceeds \nrtol\n.", 
            "title": "API"
        }, 
        {
            "location": "/api/#api", 
            "text": "As stated in the  home  page, we consider the nonlinear optimization problem in the following format:  \\begin{align*} \\min \\quad & f(x) \\\\\n& c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u. \\end{align*}  To develop an optimization algorithm, we are usually worried not only with $f(x)$ and $c(x)$, but also with their derivatives. Namely,   $\\nabla f(x)$, the gradient of $f$ at the point $x$;  $\\nabla^2 f(x)$, the Hessian of $f$ at the point $x$;  $J(x) = \\nabla c(x)$, the Jacobian of $c$ at the point $x$;  $\\nabla^2 f(x) + \\sum_{i=1}^m \\lambda_i \\nabla^2 c_i(x)$,   the Hessian of the Lagrangian function at the point $(x,\\lambda)$.   There are many ways to access some of these values, so here is a little reference guide.", 
            "title": "API"
        }, 
        {
            "location": "/api/#reference-guide", 
            "text": "The following naming should be easy enough to follow. If not, click on the link and go to the description.   !  means inplace;  _coord  means coordinate format;  prod  means matrix-vector product;  _op  means operator (as in  LinearOperators.jl ).   Feel free to open an issue to suggest other methods that should apply to all NLPModels instances.     Function  NLPModels function      $f(x)$  obj    $\\nabla f(x)$  grad ,  grad!    $\\nabla^2 f(x)$  hess ,  hess_op ,  hess_coord ,  hprod ,  hprod!    $c(x)$  cons ,  cons!    $J(x)$  jac ,  jac_coord ,  jprod ,  jprod! ,  jtprod ,  jtprod!    $\\nabla^2 L(x,y)$  hess ,  hess_op ,  hess_coord ,  hprod ,  hprod!", 
            "title": "Reference guide"
        }, 
        {
            "location": "/api/#abstractnlpmodel-functions", 
            "text": "#  NLPModels.obj     Function .  obj(nlp, x)  Evaluate $f(x)$, the objective function of  nlp  at  x .  #  NLPModels.grad     Function .  grad(nlp, x)  Evaluate $\\nabla f(x)$, the gradient of the objective function at  x .  #  NLPModels.grad!     Function .  grad!(nlp, x, g)  Evaluate $\\nabla f(x)$, the gradient of the objective function at  x  in place.  #  NLPModels.cons     Function .  cons(nlp, x)  Evaluate $c(x)$, the constraints at  x .  #  NLPModels.cons!     Function .  cons!(nlp, x, c)  Evaluate $c(x)$, the constraints at  x  in place.  #  NLPModels.jac_coord     Function .  (rows,cols,vals) = jac_coord(nlp, x)  Evaluate $\\nabla c(x)$, the constraint's Jacobian at  x  in sparse coordinate format.  #  NLPModels.jac     Function .  Jx = jac(nlp, x)  Evaluate $\\nabla c(x)$, the constraint's Jacobian at  x  as a sparse matrix.  #  NLPModels.jprod     Function .  Jv = jprod(nlp, x, v)  Evaluate $\\nabla c(x)v$, the Jacobian-vector product at  x .  #  NLPModels.jprod!     Function .  Jv = jprod!(nlp, x, v, Jv)  Evaluate $\\nabla c(x)v$, the Jacobian-vector product at  x  in place.  #  NLPModels.jtprod     Function .  Jtv = jtprod(nlp, x, v, Jtv)  Evaluate $\\nabla c(x)^Tv$, the transposed-Jacobian-vector product at  x .  #  NLPModels.jtprod!     Function .  Jtv = jtprod!(nlp, x, v, Jtv)  Evaluate $\\nabla c(x)^Tv$, the transposed-Jacobian-vector product at  x  in place.  #  NLPModels.hess_coord     Function .  (rows,cols,vals) = hess_coord(nlp, x; obj_weight=1.0, y=zeros)  Evaluate the Lagrangian Hessian at  (x,y)  in sparse coordinate format, with objective function scaled by  obj_weight , i.e.,    \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x),    with \u03c3 = obj_weight. Only the lower triangle is returned.  #  NLPModels.hess     Function .  Hx = hess(nlp, x; obj_weight=1.0, y=zeros)  Evaluate the Lagrangian Hessian at  (x,y)  as a sparse matrix, with objective function scaled by  obj_weight , i.e.,    \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x),    with \u03c3 = obj_weight. Only the lower triangle is returned.  #  NLPModels.hess_op     Function .  H = hess_op(nlp, x; obj_weight=1.0, y=zeros)  Return the Lagrangian Hessian at  (x,y)  with objective function scaled by  obj_weight  as a linear operator. The resulting object may be used as if it were a matrix, e.g.,  H * v . The linear operator H represents    \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x),    with \u03c3 = obj_weight.  #  NLPModels.hprod     Function .  Hv = hprod(nlp, x, v; obj_weight=1.0, y=zeros)  Evaluate the product of the Lagrangian Hessian at  (x,y)  with the vector  v , with objective function scaled by  obj_weight , i.e.,    \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x),    with \u03c3 = obj_weight.  #  NLPModels.hprod!     Function .  Hv = hprod!(nlp, x, v, Hv; obj_weight=1.0, y=zeros)  Evaluate the product of the Lagrangian Hessian at  (x,y)  with the vector  v  in place, with objective function scaled by  obj_weight , i.e.,    \\nabla^2L(x,y) = \\sigma * \\nabla^2 f(x) + \\sum_{i=1}^m y_i\\nabla^2 c_i(x),    with \u03c3 = obj_weight.  #  NLPModels.NLPtoMPB     Function .  mp = NLPtoMPB(nlp, solver)  Return a  MathProgBase  model corresponding to an  AbstractNLPModel .  Arguments   nlp::AbstractNLPModel  solver::AbstractMathProgSolver  a solver instance, e.g.,  IpoptSolver()   Currently, all models are treated as nonlinear models.  Return values  The function returns a  MathProgBase  model  mpbmodel  such that it should be possible to call  MathProgBase.optimize!(mpbmodel)  #  LinearOperators.reset!     Function .  reset!(counters)  Reset evaluation counters  `reset!(nlp)  Reset evaluation count in  nlp", 
            "title": "AbstractNLPModel functions"
        }, 
        {
            "location": "/api/#derivative-check", 
            "text": "#  NLPModels.gradient_check     Function .  Check the first derivatives of the objective at  x  against centered finite differences.  This function returns a dictionary indexed by components of the gradient for which the relative error exceeds  rtol .  #  NLPModels.jacobian_check     Function .  Check the first derivatives of the constraints at  x  against centered finite differences.  This function returns a dictionary indexed by (j, i) tuples such that the relative error in the  i -th partial derivative of the  j -th constraint exceeds  rtol .  #  NLPModels.hessian_check     Function .  Check the second derivatives of the objective and each constraints at  x  against centered finite differences. This check does not rely on exactness of the first derivatives, only on objective and constraint values.  The  sgn  arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as  L(x,y) = f(x) + \u2211 y\u2c7c c\u2c7c(x)  e.g., as in  JuMPNLPModel s, and a negative value if the Lagrangian is formulated as  L(x,y) = f(x) - \u2211 y\u2c7c c\u2c7c(x)  e.g., as in  AmplModel s. Only the sign of  sgn  is important.  This function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k   0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative \u2202\u00b2f\u2096/\u2202x\u1d62\u2202x\u2c7c exceeds  rtol .  #  NLPModels.hessian_check_from_grad     Function .  Check the second derivatives of the objective and each constraints at  x  against centered finite differences. This check assumes exactness of the first derivatives.  The  sgn  arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as  L(x,y) = f(x) + \u2211 y\u2c7c c\u2c7c(x)  e.g., as in  JuMPNLPModel s, and a negative value if the Lagrangian is formulated as  L(x,y) = f(x) - \u2211 y\u2c7c c\u2c7c(x)  e.g., as in  AmplModel s. Only the sign of  sgn  is important.  This function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k   0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative \u2202\u00b2f\u2096/\u2202x\u1d62\u2202x\u2c7c exceeds  rtol .", 
            "title": "Derivative check"
        }, 
        {
            "location": "/reference/", 
            "text": "Reference\n\n\n\n\nNLPModels.ADNLPModel\n\n\nNLPModels.MathProgNLPModel\n\n\nNLPModels.SimpleNLPModel\n\n\nNLPModels.SlackModel\n\n\nLinearOperators.reset!\n\n\nNLPModels.NLPtoMPB\n\n\nNLPModels.cons\n\n\nNLPModels.cons!\n\n\nNLPModels.grad\n\n\nNLPModels.grad!\n\n\nNLPModels.gradient_check\n\n\nNLPModels.hess\n\n\nNLPModels.hess_coord\n\n\nNLPModels.hess_op\n\n\nNLPModels.hessian_check\n\n\nNLPModels.hessian_check_from_grad\n\n\nNLPModels.hprod\n\n\nNLPModels.hprod!\n\n\nNLPModels.jac\n\n\nNLPModels.jac_coord\n\n\nNLPModels.jacobian_check\n\n\nNLPModels.jprod\n\n\nNLPModels.jprod!\n\n\nNLPModels.jtprod\n\n\nNLPModels.jtprod!\n\n\nNLPModels.obj", 
            "title": "Reference"
        }, 
        {
            "location": "/reference/#reference", 
            "text": "NLPModels.ADNLPModel  NLPModels.MathProgNLPModel  NLPModels.SimpleNLPModel  NLPModels.SlackModel  LinearOperators.reset!  NLPModels.NLPtoMPB  NLPModels.cons  NLPModels.cons!  NLPModels.grad  NLPModels.grad!  NLPModels.gradient_check  NLPModels.hess  NLPModels.hess_coord  NLPModels.hess_op  NLPModels.hessian_check  NLPModels.hessian_check_from_grad  NLPModels.hprod  NLPModels.hprod!  NLPModels.jac  NLPModels.jac_coord  NLPModels.jacobian_check  NLPModels.jprod  NLPModels.jprod!  NLPModels.jtprod  NLPModels.jtprod!  NLPModels.obj", 
            "title": "Reference"
        }
    ]
}