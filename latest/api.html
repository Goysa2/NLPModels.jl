<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · NLPModels.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/><link href="assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="index.html"><img class="logo" src="assets/logo.png" alt="NLPModels.jl logo"/></a><h1>NLPModels.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><a class="toctext" href="models.html">Models</a></li><li><a class="toctext" href="tools.html">Tools</a></li><li><a class="toctext" href="tutorial.html">Tutorial</a></li><li class="current"><a class="toctext" href="api.html">API</a><ul class="internal"><li><a class="toctext" href="#Reference-guide-1">Reference guide</a></li><li><a class="toctext" href="#API-for-NLSModels-1">API for NLSModels</a></li><li><a class="toctext" href="#AbstractNLPModel-functions-1">AbstractNLPModel functions</a></li><li><a class="toctext" href="#AbstractNLSModel-1">AbstractNLSModel</a></li><li><a class="toctext" href="#Derivative-check-1">Derivative check</a></li></ul></li><li><a class="toctext" href="reference.html">Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="api.html">API</a></li></ul><a class="edit-page" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/master/docs/src/api.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>API</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="API-1" href="#API-1">API</a></h1><p>As stated in the <a href="home">home</a> page, we consider the nonlinear optimization problem in the following format:</p><div>\[\begin{align*}
\min \quad &amp; f(x) \\
&amp; c_L \leq c(x) \leq c_U \\
&amp; \ell \leq x \leq u.
\end{align*}\]</div><p>To develop an optimization algorithm, we are usually worried not only with <span>$f(x)$</span> and <span>$c(x)$</span>, but also with their derivatives. Namely,</p><ul><li><p><span>$\nabla f(x)$</span>, the gradient of <span>$f$</span> at the point <span>$x$</span>;</p></li><li><p><span>$\nabla^2 f(x)$</span>, the Hessian of <span>$f$</span> at the point <span>$x$</span>;</p></li><li><p><span>$J(x) = \nabla c(x)$</span>, the Jacobian of <span>$c$</span> at the point <span>$x$</span>;</p></li><li><p><span>$\nabla^2 f(x) + \sum_{i=1}^m \lambda_i \nabla^2 c_i(x)$</span>, the Hessian of the Lagrangian function at the point <span>$(x,\lambda)$</span>.</p></li></ul><p>There are many ways to access some of these values, so here is a little reference guide.</p><h2><a class="nav-anchor" id="Reference-guide-1" href="#Reference-guide-1">Reference guide</a></h2><p>The following naming should be easy enough to follow. If not, click on the link and go to the description.</p><ul><li><p><code>!</code> means inplace;</p></li><li><p><code>_coord</code> means coordinate format;</p></li><li><p><code>prod</code> means matrix-vector product;</p></li><li><p><code>_op</code> means operator (as in <a href="https://github.com/JuliaSmoothOptimizers/LinearOperators.jl">LinearOperators.jl</a>).</p></li></ul><p>Feel free to open an issue to suggest other methods that should apply to all NLPModels instances.</p><table><tr><th>Function</th><th>NLPModels function</th></tr><tr><td><span>$f(x)$</span></td><td><a href="api.html#NLPModels.obj">obj</a>, <a href="api.html#NLPModels.objgrad">objgrad</a>, <a href="api.html#NLPModels.objgrad!">objgrad!</a>, <a href="api.html#NLPModels.objcons">objcons</a>, <a href="api.html#NLPModels.objcons!">objcons!</a></td></tr><tr><td><span>$\nabla f(x)$</span></td><td><a href="api.html#NLPModels.grad">grad</a>, <a href="api.html#NLPModels.grad!">grad!</a>, <a href="api.html#NLPModels.objgrad">objgrad</a>, <a href="api.html#NLPModels.objgrad!">objgrad!</a></td></tr><tr><td><span>$\nabla^2 f(x)$</span></td><td><a href="api.html#NLPModels.hess">hess</a>, <a href="api.html#NLPModels.hess_op">hess_op</a>, <a href="api.html#NLPModels.hess_op!">hess_op!</a>, <a href="api.html#NLPModels.hess_coord">hess_coord</a>, <a href="api.html#NLPModels.hprod">hprod</a>, <a href="api.html#NLPModels.hprod!">hprod!</a></td></tr><tr><td><span>$c(x)$</span></td><td><a href="api.html#NLPModels.cons">cons</a>, <a href="api.html#NLPModels.cons!">cons!</a>, <a href="api.html#NLPModels.objcons">objcons</a>, <a href="api.html#NLPModels.objcons!">objcons!</a></td></tr><tr><td><span>$J(x)$</span></td><td><a href="api.html#NLPModels.jac">jac</a>, <a href="api.html#NLPModels.jac_op">jac_op</a>, <a href="api.html#NLPModels.jac_op!">jac_op!</a>, <a href="api.html#NLPModels.jac_coord">jac_coord</a>, <a href="api.html#NLPModels.jprod">jprod</a>, <a href="api.html#NLPModels.jprod!">jprod!</a>, <a href="api.html#NLPModels.jtprod">jtprod</a>, <a href="api.html#NLPModels.jtprod!">jtprod!</a></td></tr><tr><td><span>$\nabla^2 L(x,y)$</span></td><td><a href="api.html#NLPModels.hess">hess</a>, <a href="api.html#NLPModels.hess_op">hess_op</a>, <a href="api.html#NLPModels.hess_coord">hess_coord</a>, <a href="api.html#NLPModels.hprod">hprod</a>, <a href="api.html#NLPModels.hprod!">hprod!</a></td></tr></table><h2><a class="nav-anchor" id="API-for-NLSModels-1" href="#API-for-NLSModels-1">API for NLSModels</a></h2><p>For the Nonlinear Least Squares models, <span>$f(x) = \Vert F(x)\Vert^2$</span>, and these models have additional function to access the residual value and its derivatives. Namely,</p><ul><li><p><span>$J_F(x) = \nabla F(x)$</span></p></li><li><p><span>$\nabla^2 F_i(x)$</span></p></li></ul><table><tr><th>Function</th><th>function</th></tr><tr><td><span>$F(x)$</span></td><td><a href="api.html#NLPModels.residual">residual</a>, <a href="api.html#NLPModels.residual!">residual!</a></td></tr><tr><td><span>$J_F(x)$</span></td><td><a href="api.html#NLPModels.jac_residual">jac_residual</a>, <a href="api.html#NLPModels.jprod_residual">jprod_residual</a>, <a href="api.html#NLPModels.jprod_residual!">jprod_residual!</a>, <a href="api.html#NLPModels.jtprod_residual">jtprod_residual</a>, <a href="api.html#NLPModels.jtprod_residual!">jtprod_residual!</a>, <a href="api.html#NLPModels.jac_op_residual">jac_op_residual</a>, <a href="api.html#NLPModels.jac_op_residual!">jac_op_residual!</a></td></tr><tr><td><span>$\nabla^2 F_i(x)$</span></td><td><a href="api.html#NLPModels.hess_residual">hess_residual</a>, <a href="api.html#NLPModels.hprod_residual">hprod_residual</a>, <a href="api.html#NLPModels.hprod_residual!">hprod_residual!</a>, <a href="api.html#NLPModels.hess_op_residual">hess_op_residual</a>, <a href="api.html#NLPModels.hess_op_residual!">hess_op_residual!</a></td></tr></table><h2><a class="nav-anchor" id="AbstractNLPModel-functions-1" href="#AbstractNLPModel-functions-1">AbstractNLPModel functions</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.obj" href="#NLPModels.obj"><code>NLPModels.obj</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>f = obj(nlp, x)</code></p><p>Evaluate <span>$f(x)$</span>, the objective function of <code>nlp</code> at <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L85-L89">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.grad" href="#NLPModels.grad"><code>NLPModels.grad</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>g = grad(nlp, x)</code></p><p>Evaluate <span>$\nabla f(x)$</span>, the gradient of the objective function at <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L92-L96">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.grad!" href="#NLPModels.grad!"><code>NLPModels.grad!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>g = grad!(nlp, x, g)</code></p><p>Evaluate <span>$\nabla f(x)$</span>, the gradient of the objective function at <code>x</code> in place.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L99-L103">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.objgrad" href="#NLPModels.objgrad"><code>NLPModels.objgrad</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>f, g = objgrad(nlp, x)</code></p><p>Evaluate <span>$f(x)$</span> and <span>$\nabla f(x)$</span> at <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L149-L153">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.objgrad!" href="#NLPModels.objgrad!"><code>NLPModels.objgrad!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>f, g = objgrad!(nlp, x, g)</code></p><p>Evaluate <span>$f(x)$</span> and <span>$\nabla f(x)$</span> at <code>x</code>. <code>g</code> is overwritten with the value of <span>$\nabla f(x)$</span>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L159-L164">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.cons" href="#NLPModels.cons"><code>NLPModels.cons</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>c = cons(nlp, x)</code></p><p>Evaluate <span>$c(x)$</span>, the constraints at <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L106-L110">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.cons!" href="#NLPModels.cons!"><code>NLPModels.cons!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>c = cons!(nlp, x, c)</code></p><p>Evaluate <span>$c(x)$</span>, the constraints at <code>x</code> in place.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L113-L117">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.objcons" href="#NLPModels.objcons"><code>NLPModels.objcons</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>f, c = objcons(nlp, x)</code></p><p>Evaluate <span>$f(x)$</span> and <span>$c(x)$</span> at <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L129-L133">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.objcons!" href="#NLPModels.objcons!"><code>NLPModels.objcons!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>f = objcons!(nlp, x, c)</code></p><p>Evaluate <span>$f(x)$</span> and <span>$c(x)$</span> at <code>x</code>. <code>c</code> is overwritten with the value of <span>$c(x)$</span>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L139-L143">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_coord" href="#NLPModels.jac_coord"><code>NLPModels.jac_coord</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>(rows,cols,vals) = jac_coord(nlp, x)</code></p><p>Evaluate <span>$\nabla c(x)$</span>, the constraint&#39;s Jacobian at <code>x</code> in sparse coordinate format.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L170-L174">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac" href="#NLPModels.jac"><code>NLPModels.jac</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>Jx = jac(nlp, x)</code></p><p>Evaluate <span>$\nabla c(x)$</span>, the constraint&#39;s Jacobian at <code>x</code> as a sparse matrix.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L177-L181">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_op" href="#NLPModels.jac_op"><code>NLPModels.jac_op</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>J = jac_op(nlp, x)</code></p><p>Return the Jacobian at <code>x</code> as a linear operator. The resulting object may be used as if it were a matrix, e.g., <code>J * v</code> or <code>J&#39; * v</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L211-L217">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_op!" href="#NLPModels.jac_op!"><code>NLPModels.jac_op!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>J = jac_op!(nlp, x, Jv, Jtv)</code></p><p>Return the Jacobian at <code>x</code> as a linear operator. The resulting object may be used as if it were a matrix, e.g., <code>J * v</code> or <code>J&#39; * v</code>. The values <code>Jv</code> and <code>Jtv</code> are used as preallocated storage for the operations.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L225-L232">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jprod" href="#NLPModels.jprod"><code>NLPModels.jprod</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>Jv = jprod(nlp, x, v)</code></p><p>Evaluate <span>$\nabla c(x)v$</span>, the Jacobian-vector product at <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L183-L187">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jprod!" href="#NLPModels.jprod!"><code>NLPModels.jprod!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>Jv = jprod!(nlp, x, v, Jv)</code></p><p>Evaluate <span>$\nabla c(x)v$</span>, the Jacobian-vector product at <code>x</code> in place.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L190-L194">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jtprod" href="#NLPModels.jtprod"><code>NLPModels.jtprod</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>Jtv = jtprod(nlp, x, v, Jtv)</code></p><p>Evaluate <span>$\nabla c(x)^Tv$</span>, the transposed-Jacobian-vector product at <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L197-L201">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jtprod!" href="#NLPModels.jtprod!"><code>NLPModels.jtprod!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>Jtv = jtprod!(nlp, x, v, Jtv)</code></p><p>Evaluate <span>$\nabla c(x)^Tv$</span>, the transposed-Jacobian-vector product at <code>x</code> in place.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L204-L208">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_coord" href="#NLPModels.hess_coord"><code>NLPModels.hess_coord</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>(rows,cols,vals) = hess_coord(nlp, x; obj_weight=1.0, y=zeros)</code></p><p>Evaluate the Lagrangian Hessian at <code>(x,y)</code> in sparse coordinate format, with objective function scaled by <code>obj_weight</code>, i.e.,</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum_{i=1}^m y_i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight. Only the lower triangle is returned.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L250-L260">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess" href="#NLPModels.hess"><code>NLPModels.hess</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>Hx = hess(nlp, x; obj_weight=1.0, y=zeros)</code></p><p>Evaluate the Lagrangian Hessian at <code>(x,y)</code> as a sparse matrix, with objective function scaled by <code>obj_weight</code>, i.e.,</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum_{i=1}^m y_i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight. Only the lower triangle is returned.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L263-L273">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_op" href="#NLPModels.hess_op"><code>NLPModels.hess_op</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>H = hess_op(nlp, x; obj_weight=1.0, y=zeros)</code></p><p>Return the Lagrangian Hessian at <code>(x,y)</code> with objective function scaled by <code>obj_weight</code> as a linear operator. The resulting object may be used as if it were a matrix, e.g., <code>H * v</code>. The linear operator H represents</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum_{i=1}^m y_i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L300-L310">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_op!" href="#NLPModels.hess_op!"><code>NLPModels.hess_op!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>H = hess_op!(nlp, x, Hv; obj_weight=1.0, y=zeros)</code></p><p>Return the Lagrangian Hessian at <code>(x,y)</code> with objective function scaled by <code>obj_weight</code> as a linear operator, and storing the result on <code>Hv</code>. The resulting object may be used as if it were a matrix, e.g., <code>w = H * v</code>. The vector <code>Hv</code> is used as preallocated storage for the operation.  The linear operator H represents</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum_{i=1}^m y_i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L317-L329">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hprod" href="#NLPModels.hprod"><code>NLPModels.hprod</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>Hv = hprod(nlp, x, v; obj_weight=1.0, y=zeros)</code></p><p>Evaluate the product of the Lagrangian Hessian at <code>(x,y)</code> with the vector <code>v</code>, with objective function scaled by <code>obj_weight</code>, i.e.,</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum_{i=1}^m y_i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L276-L285">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hprod!" href="#NLPModels.hprod!"><code>NLPModels.hprod!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>Hv = hprod!(nlp, x, v, Hv; obj_weight=1.0, y=zeros)</code></p><p>Evaluate the product of the Lagrangian Hessian at <code>(x,y)</code> with the vector <code>v</code> in place, with objective function scaled by <code>obj_weight</code>, i.e.,</p><p>\[ \nabla^2L(x,y) = \sigma * \nabla^2 f(x) + \sum_{i=1}^m y_i\nabla^2 c_i(x), \]</p><p>with σ = obj_weight.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L288-L297">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.NLPtoMPB" href="#NLPModels.NLPtoMPB"><code>NLPModels.NLPtoMPB</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">mp = NLPtoMPB(nlp, solver)</code></pre><p>Return a <code>MathProgBase</code> model corresponding to an <code>AbstractNLPModel</code>.</p><p><strong>Arguments</strong></p><ul><li><p><code>nlp::AbstractNLPModel</code></p></li><li><p><code>solver::AbstractMathProgSolver</code> a solver instance, e.g., <code>IpoptSolver()</code></p></li></ul><p>Currently, all models are treated as nonlinear models.</p><p><strong>Return values</strong></p><p>The function returns a <code>MathProgBase</code> model <code>mpbmodel</code> such that it should be possible to call</p><pre><code class="language-none">MathProgBase.optimize!(mpbmodel)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/nlp_to_mpb.jl#L124-L140">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="LinearOperators.reset!" href="#LinearOperators.reset!"><code>LinearOperators.reset!</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>reset!(counters)</code></p><p>Reset evaluation counters</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L64-L68">source</a><div><p>`reset!(nlp)</p><p>Reset evaluation count in <code>nlp</code></p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLPModels.jl#L75-L79">source</a></section><h2><a class="nav-anchor" id="AbstractNLSModel-1" href="#AbstractNLSModel-1">AbstractNLSModel</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.residual" href="#NLPModels.residual"><code>NLPModels.residual</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Fx = residual(nls, x)</code></pre><p>Computes F(x), the residual at x.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L91-L95">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.residual!" href="#NLPModels.residual!"><code>NLPModels.residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Fx = residual!(nls, x, Fx)</code></pre><p>Computes F(x), the residual at x.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L101-L105">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_residual" href="#NLPModels.jac_residual"><code>NLPModels.jac_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Jx = jac_residual(nls, x)</code></pre><p>Computes J(x), the Jacobian of the residual at x.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L110-L114">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jprod_residual" href="#NLPModels.jprod_residual"><code>NLPModels.jprod_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Jv = jprod_residual(nls, x, v)</code></pre><p>Computes the product of the Jacobian of the residual at x and a vector, i.e.,  J(x)*v.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L119-L123">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jprod_residual!" href="#NLPModels.jprod_residual!"><code>NLPModels.jprod_residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Jv = jprod_residual!(nls, x, v, Jv)</code></pre><p>Computes the product of the Jacobian of the residual at x and a vector, i.e.,  J(x)*v, storing it in <code>Jv</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L129-L133">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jtprod_residual" href="#NLPModels.jtprod_residual"><code>NLPModels.jtprod_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Jtv = jtprod_residual(nls, x, v)</code></pre><p>Computes the product of the transpose of the Jacobian of the residual at x and a vector, i.e.,  J(x)&#39;*v.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L138-L142">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jtprod_residual!" href="#NLPModels.jtprod_residual!"><code>NLPModels.jtprod_residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Jtv = jtprod_residual!(nls, x, v, Jtv)</code></pre><p>Computes the product of the transpose of the Jacobian of the residual at x and a vector, i.e.,  J(x)&#39;*v, storing it in <code>Jtv</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L148-L152">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_op_residual" href="#NLPModels.jac_op_residual"><code>NLPModels.jac_op_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Jx = jac_op_residual(nls, x)</code></pre><p>Computes J(x), the Jacobian of the residual at x, in linear operator form.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L157-L161">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jac_op_residual!" href="#NLPModels.jac_op_residual!"><code>NLPModels.jac_op_residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Jx = jac_op_residual!(nls, x, Jv, Jtv)</code></pre><p>Computes J(x), the Jacobian of the residual at x, in linear operator form. The vectors <code>Jv</code> and <code>Jtv</code> are used as preallocated storage for the operations.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L170-L175">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_residual" href="#NLPModels.hess_residual"><code>NLPModels.hess_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Hi = hess_residual(nls, x, i)</code></pre><p>Computes the Hessian of the i-th residual at x.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L185-L189">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hprod_residual" href="#NLPModels.hprod_residual"><code>NLPModels.hprod_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Hiv = hprod_residual(nls, x, i, v)</code></pre><p>Computes the product of the Hessian of the i-th residual at x, times the vector v.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L194-L198">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hprod_residual!" href="#NLPModels.hprod_residual!"><code>NLPModels.hprod_residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Hiv = hprod_residual!(nls, x, i, v, Hiv)</code></pre><p>Computes the product of the Hessian of the i-th residual at x, times the vector v, and stores it in vector Hiv.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L204-L208">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_op_residual" href="#NLPModels.hess_op_residual"><code>NLPModels.hess_op_residual</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Hop = hess_op_residual(nls, x, i)</code></pre><p>Computes the Hessian of the i-th residual at x, in linear operator form.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L213-L217">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hess_op_residual!" href="#NLPModels.hess_op_residual!"><code>NLPModels.hess_op_residual!</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Hop = hess_op_residual!(nls, x, i, Hiv)</code></pre><p>Computes the Hessian of the i-th residual at x, in linear operator form. The vector <code>Hiv</code> is used as preallocated storage for the operation.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/NLSModels.jl#L224-L228">source</a></section><h2><a class="nav-anchor" id="Derivative-check-1" href="#Derivative-check-1">Derivative check</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.gradient_check" href="#NLPModels.gradient_check"><code>NLPModels.gradient_check</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Check the first derivatives of the objective at <code>x</code> against centered finite differences.</p><p>This function returns a dictionary indexed by components of the gradient for which the relative error exceeds <code>rtol</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/dercheck.jl#L8-L14">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.jacobian_check" href="#NLPModels.jacobian_check"><code>NLPModels.jacobian_check</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Check the first derivatives of the constraints at <code>x</code> against centered finite differences.</p><p>This function returns a dictionary indexed by (j, i) tuples such that the relative error in the <code>i</code>-th partial derivative of the <code>j</code>-th constraint exceeds <code>rtol</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/dercheck.jl#L38-L45">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hessian_check" href="#NLPModels.hessian_check"><code>NLPModels.hessian_check</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Check the second derivatives of the objective and each constraints at <code>x</code> against centered finite differences. This check does not rely on exactness of the first derivatives, only on objective and constraint values.</p><p>The <code>sgn</code> arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as</p><pre><code class="language-none">L(x,y) = f(x) + ∑ yⱼ cⱼ(x)</code></pre><p>e.g., as in <code>JuMPNLPModel</code>s, and a negative value if the Lagrangian is formulated as</p><pre><code class="language-none">L(x,y) = f(x) - ∑ yⱼ cⱼ(x)</code></pre><p>e.g., as in <code>AmplModel</code>s. Only the sign of <code>sgn</code> is important.</p><p>This function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k &gt; 0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative ∂²fₖ/∂xᵢ∂xⱼ exceeds <code>rtol</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/dercheck.jl#L79-L100">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLPModels.hessian_check_from_grad" href="#NLPModels.hessian_check_from_grad"><code>NLPModels.hessian_check_from_grad</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Check the second derivatives of the objective and each constraints at <code>x</code> against centered finite differences. This check assumes exactness of the first derivatives.</p><p>The <code>sgn</code> arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as</p><pre><code class="language-none">L(x,y) = f(x) + ∑ yⱼ cⱼ(x)</code></pre><p>e.g., as in <code>JuMPNLPModel</code>s, and a negative value if the Lagrangian is formulated as</p><pre><code class="language-none">L(x,y) = f(x) - ∑ yⱼ cⱼ(x)</code></pre><p>e.g., as in <code>AmplModel</code>s. Only the sign of <code>sgn</code> is important.</p><p>This function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k &gt; 0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative ∂²fₖ/∂xᵢ∂xⱼ exceeds <code>rtol</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/585a5c728ba63930aea7c34e017e459eec607922/src/dercheck.jl#L165-L186">source</a></section><footer><hr/><a class="previous" href="tutorial.html"><span class="direction">Previous</span><span class="title">Tutorial</span></a><a class="next" href="reference.html"><span class="direction">Next</span><span class="title">Reference</span></a></footer></article></body></html>
